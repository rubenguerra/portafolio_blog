[
	{
		"id" : 1,
		"estado" : 1,
		"fecha_creacion" : "2022-08-25",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Una introducción al Machine Learning",
		"slug" : "introduccion-al-machine-learning",
		"descripcion" : "¿Qué es el Machine learning y cómo forma parte de nuestra vida cotidiana?",
		"contenido" : "<h2>Introducci&oacute;n</h2>\r\n\r\n<p>El machine learning, sin que nos demos cuenta, est&aacute; a nuestro alrededor. Tanto cuando compramos un boleto de avi&oacute;n para disfrutar de unas vacaciones, como cuando realizamos una b&uacute;squeda en nuestro celular para saber algo, un algoritmo realiza algunas tareas espec&iacute;ficas, como decidir el precio a pagar en aqu&eacute;l vuelo, como el mostrar la informaci&oacute;n m&aacute;s id&oacute;nea en nuestra b&uacute;squeda. Cuando alguien realiza una solicitud de pr&eacute;stamo a un banco, un algoritmo va a decidir si es elegible o no para obtenerlo. Cuando hacemos alguna compra, el machine learning nos mostrar&aacute; alternativas no s&oacute;lo compatibles con nuestros deseos, sino que har&aacute; sugerencias mostrando opciones parecidas a los deseos que hemos mostrado. Todo esto impacta en cada aspecto de nuestra vida y lo mejor, es que este desarrollo no hace m&aacute;s que incrementarse.</p>\r\n\r\n<p>El Machine learning est&aacute; tan enraizado ya en nuestro mundo diario, que la informaci&oacute;n que pueden manejar los sistemas a los que tenemos acceso nos dan la impresi&oacute;n de que somos vigilados por algo, o por alguien -una inteligencia inquietante-,&nbsp; a toda hora y eso nos cubre con un sentimiento de aprehensi&oacute;n, la mayoria de las veces, que nos deja abrumados y recelosos al mismo tiempo.</p>\r\n\r\n<p>Desde hace algunos a&ntilde;os ya, desde la &uacute;ltima d&eacute;cada, para ser m&aacute;s precisos, el machine learning ha estado construy&eacute;ndose a s&iacute; mismo y creciendo exponencialmente. Como n&uacute;cleo de las metas en el campo de la inteligencia artificial, es una de las m&aacute;s influyentes e importantes tecnolog&iacute;as de los d&iacute;as que corren. Sobre la base del manejo de una inmensa cantidad de datos, que se recopilan segundo a segundo durante todo el d&iacute;a, las computadoras, por medio de algoritmos especializados, y que vamos a conocer dentro de poco, buscan patrones para resumir, procesar, analizar y arrojar informaci&oacute;n que por medio de recursos naturales se nos hace imposible de notar.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Entendiendo el Machine learning</h2>\r\n\r\n<p>Todos nos sentimos maravillados de que por fin nos muestren que las m&aacute;quinas pueden aprender, pero vamos a desmitificar un poco sobre como funciona el machine learning, de esa manera podremos abordar su aprendizaje de manera sencila, desde sus bases y los procesos envueltos en este campo, como los modelos que hacen que funcione. El machine learning est&aacute; fundamentado, espec&iacute;ficamente, sobre programas computaciones que toman datos de variables y son ajustadas autom&aacute;ticamente. La meta es mejorar el desempe&ntilde;o de los modelos de machine learning por medio de los datos que ser&aacute;n procesados. Este procesamiento har&aacute; que la m&aacute;quina, por medio de patrones que descubre, pueda realizar tareas con datos ya desconocidos y lo mejor, hacer predicciones sobre estos datos de acuerdo a lo que aprendi&oacute; durante el proceso.</p>\r\n\r\n<p>El machine learning es un subcampo de la inteligencia artificial, que contiene adem&aacute;s &aacute;reas como el deep learning entre otros. Compuesto por algoritmos construidos como bloques que podemos usar para computarizar datos, aprender y actuar de manera inteligente por generalizaciones y patrones que obtiene de dichos datos, el machine learning puede predecir comportamientos o alimentar otras bases con datos arrojados por su procesamiento. A pesar de que este campo no ha sido explorado hasta los tiempos recientes, el t&eacute;rmino fue acu&ntilde;ado en el a&ntilde;o de 1959 por Arthur Samuel, pionero en el campo de la inteligencia artificial, mientras trabajaba en la IBM. Defini&oacute; el machine learning como un &quot;campo de estudio que da a las computadoras la capacidad de aprender sin estar expl&iacute;citamente programadas&quot;. (Gupta, 2021, p. 5).&nbsp; Desarrollos y descubrimientos que le dan sus fundamentos fueron hechos a trav&eacute;s de los setenta y ochenta pero su popularidad de hoy en d&iacute;a puede atribuirse, sin duda, a la disponibilidad presente de equipos m&aacute;s r&aacute;pidos y al alcance de cualquiera, de una ingente cantidad de datos, eficientes dispositivos de almacenamiento de esos datos y a la evoluci&oacute;n de nuevos algoritmos.</p>\r\n\r\n<p>Al m&aacute;s alto nivel, el machine learning es la habilidad de un sistema de adaptarse a nuevos datos y arrojar respuestas acordes. El proceso de aprendizaje se desarrolla a trav&eacute;s de iteraciones que arrojan una mejor calidad de respuesta. Las aplicaciones pueden aprender de transacciones y procesamientos previos usando patrones de reconocimiento para producir resultados &uacute;tiles y una mejor informaci&oacute;n. EL proceso b&aacute;sico comienza con la introducci&oacute;n al sistema de datos y entrenar la computadora, que por medio de algoritmos har&aacute; el procesamiento de esos datos. La escogencia del algoritmo a utilizar va a depender de la naturaleza de la tarea a realizar, y &eacute;stas pueden ser, en principio, tareas de clasificaci&oacute;n y de regresi&oacute;n. Los algoritmos del machine learning pueden identificar patrones de datos proporcionados y construir modelos que permiten captar relaciones entre los datos de entrada. Este procedimiento de crear patrones le ofrece la posibilidad a la m&aacute;quina (computadora) de predecir el comportamiento de datos desconocidos.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>El Machine Learning seg&uacute;n Mitchell</h2>\r\n\r\n<p>Una definici&oacute;n ampliamente aceptada del machine learning fue propuesta por el cient&iacute;fico de la computaci&oacute;n Tom M. Mitchell, quien defini&oacute;: una m&aacute;quina se dice que aprender si es capaz de adquirir experiencia y utilizarla para desempe&ntilde;arse en similares procesos futuros. Su definici&oacute;n dice en pocas palabras que las t&eacute;cnicas del machine learning aprender a transformar los datos en conocimientos de decisi&oacute;n.</p>\r\n\r\n<p>EL machine learning tambi&eacute;n implica estudios de algoritmos que mejoran una definida categoria de tareas mientras optimiza un criterio de desempe&ntilde;o basado en experiencias pasadas. El ML utiliza datos y experiencias para resolver una determinada disyuntiva o desempe&ntilde;o de criterio</p>\r\n\r\n<p>La propiedad m&aacute;s deseable de los algoritmos del ML es la generalizaci&oacute;n. Por ejemplo, un modelo deber&iacute;a desempe&ntilde;arse bien con nuevos datos, desconocidos. El objetivo real del aprendizaje es hacer certezas con datos de prueba que no fueron usados para el entrenamiento del modelo, que halle las regularidades, los patrones en los datos e ignore los aspectos sin importancia.&nbsp;&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algunos conceptos b&aacute;sicos</h2>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Dataset (Conjunto de datos): Piedra fundamental del proceso de aprendizaje, es el punto de inicio de cualquier proyecto de machine learning, pues au&iacute; se contienes medidas o valores de datos recolectadps, pueden ser n&uacute;meros o caracteres, que contienen importantes caracter&iacute;sticas y descripciones del comportamiento del problema a ser resuelto. Es importante tener en cuenta que si los datos son incompletos o contienen mucha desinformaci&oacute;n, ruido, ni siquiera el mejor algoritmo servir&aacute; para resolver el problema. Debemos hacer que nuestro dataset presente la mayor informaci&oacute;n posible, y mejorarla lo&nbsp;m&aacute;s que se pueda, en caso de no ser as&iacute;.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Caracter&iacute;sticas y atributos (Features/attributes): Referidas a los par&aacute;metros o variables del dataset, algunos ejemplos pueden ser: kilometraje de un auto, g&eacute;nero de los usuarios, frecuencia de palabras, son propiedas e informaci&oacute;n contenida en un dataset que ayuda a entender mejor el problema. Estas caracter&iacute;sticas son los factores que consideran los algoritmos del ML como entrada y les permite bsucar patrones, aprender e inferir, para tomar decisiones inteligentes. Cuando los datos est&aacute;n en forma de tabla son m&aacute;s simples de entender, con las caracteristicas como nombres de columna. Seleccionar las caracter&iacute;sticas importantes es necesario y muy importante, y usualmente es la parte del proceso que toma m&aacute;s tiempo.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Probar los datos (Testing data): Luego de que el modelo es entrenado, debe ser evaluado su niverl de certeza y esto lo hacemos con un conjunto de datos de prueba. Este procedimiento compara los resultados obtenidos con aquellos que se esperaban obtener; dependediendo de este procedimiento, se har&aacute;n los ajustes necesarios o no.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Modelo (Model): Existen muchas formas de solucionar un problema. La idea b&aacute;sica es construir una representaci&oacute;n matem&aacute;tica que logre capturar las relaciones entre las datos de entrada y de salida, en ingl&eacute;s se le dice una &quot;mapping function&quot;.&nbsp; Esto se logra mediante el proceso de entrenamiento.</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Por qu&eacute; usar machine learning?</span></h2>\r\n\r\n<p>Es importante recordar que el ML no ofrece soluciones a todo tipo de problemas a la mano. Existen muchos casos en los cuales las soluciones pueden encontrarse sin usar las t&eacute;cnicas del ML. La caracter&iacute;stica definitoria de la regla base de los algoritmos del machine learning es la identificaci&oacute;n y uso de un conjunto de reglas relacionales, que colectivamente representan el conocimiento adquirido por el sistema. Por ejemplo, no necesitaremos ML si podemos determinar nuestro valor objetivo usando reglas sencillas preestablecidas o predeterminados pasoa que pueden ser programados sin necesidad de datos que dirijan un aprendizaje, as&iacute;, una regla como {cebolla, lechuga} =&gt; {pan} que descubrimos en los datos de ventas de un supermercado indica que si los clientes compran cebolla y lechuga, es probable que compren tambi&eacute;n pan. Esta informaci&oacute;n puede ser usada como base para tomas decisiones de venta como que se hagan promociones de estos productos, distribuci&oacute;n de estos art&iacute;culos unos cerca de otros, etc.</p>\r\n\r\n<p>El ML es usado a menudo para tareas complejas o problemas que impliquen una gran cantidad de datos y muchas caracter&iacute;sticas involucradas. El ML es una buena opci&oacute;n si necesitamos:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Manejar reglas y ecuaciones muy complejas (como la utilizada en el reconocimientos de rostros).</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Realizar tareas sin necesidad de escribir programas.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">No podemos explicar el c&oacute;mo (el reconocimiento de voz)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Soluciones de clasificaci&oacute;n (spam o no).</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Reglas que constantemente est&aacute;n cambiando (detecci&oacute;n de fraude)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Algunas soluciones tradicionales que no pueden se escaladas.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Desarrollar sistemas que pueden autom&aacute;ticamente adaptarse y desenvolverse a s&iacute; mismos para usuarios individuales (noticias personalizadas o filtros de correo).</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Descubrir nuevos conocimientos de bases de datos muy grandes.</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos a finalizar nuestro recorrido por esta introducci&oacute;n no sin antes recomendarte que si te interesa este campo de desarrollo, sigas aprendiendo y buscando informaci&oacute;n al respecto. Que tu curiosidad se alimente cada d&iacute;a y que el constante aprendizaje sea la constante de tu vida. Espero nos volvamos a encontrar para seguir aprendiendo sobre el machine learning y de que forma est&aacute; cambiando nuestro mundo. &iexcl;Hasta luego!</p>\r\n\r\n<p><img alt=\"wink\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/wink_smile.png\" style=\"height:23px; width:23px\" title=\"wink\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5><span style=\"font-family:Arial,Helvetica,sans-serif\">(Versionado del libro: Introduction to Machine Learning in the Cloud with Python. Concepts and practice, de Gupta, Pramod y Sehgal, Naresh K. (2021). Springer)</span></h5>",
		"imagen_referencial" : "imagenes/intel-artifi.jpg",
		"publicado" : 1,
		"fecha_publicacion" : "2022-08-25 10:56:56.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 2,
		"estado" : 1,
		"fecha_creacion" : "2022-08-25",
		"fecha_modificacion" : "2023-04-09",
		"fecha_eliminacion" : "2023-04-09",
		"titulo" : "La odisea lingüística de Ulises",
		"slug" : "odisea-linguistica-ulises",
		"descripcion" : "Desde los inicios de la literatura de occidente, la figura de Ulises se yergue como el ideal del hombre auténtico, como el verdadero retrato del ser humano, varón incansable y lleno de ingenio, que sobrevive a fuerza de su capacidad imaginativa, astucia e inteligencia.",
		"contenido" : "<p>Recuerdo que la primera vez, y de las pocas veces, que tom&eacute; en mis manos la traducci&oacute;n hecha por Luis Segal&aacute; y Estalella de <strong>La Odisea</strong>, no pude soportar su escogencia de Odiseo para nombrar al protagonista de una de las obras cumbre de la literatura, y pens&eacute; que el autor habia escogido ese nombre arbitrariamente. Si siempre hab&iacute;a escuchado acerca de las aventuras de Ulises, de d&oacute;nde sal&iacute;a ese nombre de Odiseo; &iquest;era porque as&iacute; se llamaba la obra de Homero? De Eneas tambi&eacute;n hab&iacute;a escuchado, pero el nombre no se hab&iacute;a vuelto com&uacute;n a mis o&iacute;dos, no ten&iacute;a ning&uacute;n amigo, conocido o desconocido, que se llamara as&iacute;, de all&iacute; que las aventuras de este hombre se nombraran Eneadas; pero Odiseo no encajaba para m&iacute; en esos d&iacute;as de ni&ntilde;o; hasta que con el tiempo supe que as&iacute; se llamaba realmente el protagonista de las aventuras m&aacute;s fant&aacute;sticas e incre&iacute;bles de las que ten&iacute;a conocimiento. Mucho antes de Crusoe, de Dant&egrave;s, de Fogg y de tantos otros, un hombre que volv&iacute;a de la guerra hacia sus dominios se ve envuelto en un sinnumero de peripecias, desventuradas y venturosas, que lo marginan del resto de los mortales y que lo promueven al lugar de los h&eacute;roes eternos.</p>\r\n\r\n<p>Pero Ulises, como lo bautizaron los latinos cuando traducen la obra hom&eacute;rica, cambia sus ropajes ling&uuml;&iacute;sticos y fon&eacute;ticos desde el griego Oudises,</p>\r\n\r\n<p>&nbsp;</p>",
		"imagen_referencial" : "imagenes/francisco_1.jpg",
		"publicado" : 0,
		"fecha_publicacion" : "2022-08-25 11:07:14.000000",
		"autor_id" : 1,
		"categoria_id" : 4
	},
	{
		"id" : 5,
		"estado" : 1,
		"fecha_creacion" : "2022-08-25",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Construyamos modelos de redes neuronales con Tensorflow",
		"slug" : "modelos-redes_neuronales-tensorflow",
		"descripcion" : "En esta ocasión vamos a construir varios modelos de redes neuronales con el framework Tensorflow y Keras para comparar el desempeño de cada una de ellas y ver de cuántas maneras podemos programar una red neuronal.",
		"contenido" : "<h2>TensorFlow y Keras</h2>\r\n\r\n<p>Una de las m&aacute;s importantes caracter&iacute;sticas de TensorFlow 2.x es la incorporaci&oacute;n de Keras como una API de alto nivel. Esto es importante por varias razones, y la primera es que a pesar de que su compatibilidad era conocida desde hace tiempo, ambas conten&iacute;an bibliotecas separadas y de diferentes ciclos de desarrollo, lo que muchas veces causaba molestas brechas de compatibilidad. Esto ya no es ning&uacute;n problema, porque a partir de su versi&oacute;n 2.x Keras forma parte de TensorFlow y crecen en la misma direcci&oacute;n, haciendo de su interoperabilidad una caracter&iacute;stica poderosa y amable a la hora de hacer uso de estas herramientas. Podemos decir que TensorFlor es Keras y Keras es TensorFlow.</p>\r\n\r\n<p>Lo mejor es que la ventaja m&aacute;s grande de esta asociaci&oacute;n es que podemos usar las caracter&iacute;sticas de alto nivel de Keras, ganando con ello tiempo y un mejor desempe&ntilde;o a la hora de construir redes neuronales de forma m&aacute;s sencilla y poderosa.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Manos a la obra</h2>\r\n\r\n<p>Vamos a empezar a conocer las ventajas de esta plataforma para crear una red neuronal, pero en esta ocaci&oacute;n haremos cuatro modelos, y de esta forma aprenderemos y nos daremos cuenta de que no hay un solo camino hacia Roma.</p>\r\n\r\n<p>Usaremos la herramienta que nos brinda Google para programar conocida como Google Collaboraty, para ello debes tener una cuenta gmail y entrar a la direcci&oacute;n ***</p>\r\n\r\n<p>El primer paso ser&aacute; importar TensorFlow a nuestra &aacute;rea de trabajo, y no hace falta instalarlo porque ya viene preinstalado:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import tensorflow as tf\r\nprint(tf.__version__)</code></pre>\r\n\r\n<p>As&iacute; sabremos que n&uacute;mero de versi&oacute;n estamo utilizando</p>\r\n\r\n<p>Luego de importar TensorFlow debemos llamar las bibliotecas requeridas de la API Keras:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Importamos los métodos y modelos que vamos a utilizar para crear nuestra red neuronal\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelBinarizer\r\nfrom keras import Input\r\nfrom keras.datasets import mnist\r\nfrom keras.layers import Dense\r\nfrom keras.models import Model\r\nfrom keras.models import Sequential</code></pre>\r\n\r\n<p>Ahora vamos a comenzar dise&ntilde;ando nuestra primera red neuronal, para ello vamos hacer uso de la API Sequential y pasaremos una lista de capas al constructor Sequential. El n&uacute;mero en cada capa corresponde al n&uacute;mero de neuronas o unidades que contiene:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">capas = [Dense(256, input_shape=(28*28*1,),\r\n               activation='sigmoid'),\r\n         Dense(128, activation='sigmoid'),\r\n         Dense(10, activation='softmax')]\r\nmodelo_lista_secuencial = Sequential(capas)</code></pre>\r\n\r\n<p>&nbsp;Creamos otro modelo pero ahora usando el m&eacute;todo add() para agregar una capa a la vez. Como antes, el n&uacute;mero en cada capa es el n&uacute;mero de neuronas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_secuencial = Sequential()\r\nmodelo_secuencial.add(Dense(256, input_shape=(28*28*1,),\r\n                            activation='sigmoid'))\r\nmodelo_secuencial.add(Dense(128, activation='sigmoid'))\r\nmodelo_secuencial.add(Dense(10, activation='softmax'))</code></pre>\r\n\r\n<p>Este modelo a continuaci&oacute;n se construye por medio de la API Functional. Igual que en los anteriores, los n&uacute;meros en las capas son las neuronas:</p>\r\n\r\n<p>Por &uacute;ltimo, creamos un modelo usando el paradigma orientado a objeto con la sub-clase tensorflow.keras.models.Model:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">class ModeloClase(Model):\r\n    def __init__(self):\r\n      super(ModeloClase, self).__init__()\r\n      self.capa_1=Dense(256, activation='sigmoid')\r\n      self.capa_2=Dense(256, activation='sigmoid')\r\n      self.predicciones=Dense(10, activation='softmax')\r\n    def call(self, inputs, **kwargs):\r\n      x=self.capa_1(inputs)\r\n      x=self.capa_2(x)\r\n      return self.predicciones(x)\r\nmodelo_clase=ModeloClase()</code></pre>\r\n\r\n<p>Preparamos los datos para entrenamiento de los modelos que acabamos de crear. A veces debemos reformatear las im&aacute;genes, es decir, transformar las im&aacute;genes en vectores porque ese es el formato que pueden leer las redes neuronales:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">(X_train, y_train), (X_test, y_test)=mnist.load_data()\r\nX_train=X_train.reshape((X_train.shape[0], 28*28*1))\r\nX_test=X_test.reshape((X_test.shape[0], 28*28*1))\r\nX_train=X_train.astype('float32')/255.0\r\nX_test=X_test.astype('float32')/255.0</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz </span>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">11490434/11490434 [==============================] - 1s 0us/step</span></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora codificamos las etiquetas respuestas a fin de manipular solo n&uacute;meros:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">etiquetas_binarias=LabelBinarizer()\r\ny_train=etiquetas_binarias.fit_transform(y_train)\r\ny_test=etiquetas_binarias.fit_transform(y_test)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Tomamos el 20% de los datos para validaci&oacute;n:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Compilamos, entrenamos el modelo para 50 &eacute;pocas, epoch en ingl&eacute;s, y evaluamos por medio de los datos de prueba, X_test:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelos ={\r\n    'modelo_secuencial': modelo_secuencial,\r\n    'modelo_lista_secuencial': modelo_lista_secuencial,\r\n    'modelo_funcional': modelo_funcional,\r\n    'modelo_clase': modelo_clase\r\n}\r\n# Usamos un bucle para entrenar los modelos:\r\nfor nombre, modelo in modelos.items():\r\n  print(f'Compilando el modelo: {nombre}')\r\n  modelo.compile(loss='categorical_crossentropy',\r\n                 optimizer='adam',\r\n                 metrics=['accuracy'])\r\n  print(f'Entrenando el modelo: {nombre}')\r\n  modelo.fit(X_train, y_train,\r\n             validation_data=(X_valid, y_valid),\r\n             epochs=50,\r\n             batch_size=256,\r\n             verbose=0)\r\n  _, certeza = modelo.evaluate(X_test, y_test, verbose=0)\r\n  print(f'Probando el modelo: {nombre}. \\nCerteza: {certeza}')\r\n  print('---')</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nCompilando modelo: modelo_secuencial\r\nEntrenando modelo: modelo_secuencial\r\nProbando modelo: modelo_secuencial.\r\nCerteza: 0.9807999730110168</pre>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">--- </span></p>\r\n\r\n<pre>\r\nCompilando modelo: lista_modelo_secuencial\r\nEntrenando modelo: lista_modelo_secuencial\r\nProbando modelo: lista_modelo_secuencial.\r\nCerteza: 0.980400025844574</pre>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">--- </span></p>\r\n\r\n<pre>\r\nCompilando modelo: modelo_funcional\r\nEntrenando modelo: modelo_funcional\r\nProbando modelo: modelo_funcional.\r\nCerteza: 0.9800000190734863</pre>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">--- </span></p>\r\n\r\n<pre>\r\nCompilando modelo: modelo_clase\r\nEntrenando modelo: modelo_clase\r\nProbando modelo: modelo_clase.\r\nCerteza: 0.9801999926567078</pre>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">---</span></p>\r\n\r\n<p>Luego de 50 ciclos, epochs, todos los modelos muestran alrededor de un 98% de certeza al probar el conjunto de prueba, un resultado nada desde&ntilde;able.</p>\r\n\r\n<p>Para concluir, en este post pudimos conocer diferentes formas de construir una red neuronal usando TensorFlow y Keras, adem&aacute;s, aprendimos a entrenarla mediante un conjunto de datos muy famoso, aprendimos a cargar el dataset, dividirlo en paquetes para entrenar nuestros modelos y adem&aacute;s para probarlos.</p>\r\n\r\n<p>Esto es solo un abreboca que ojal&aacute; despierte tu inter&eacute;s en este campo tan fascinante como lo es la inteligencia artificial. &iexcl;Espero nos volvamos a encontrar pronto!</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5>(Versionado del cap. I del libro: TensorFlow 2.0. Computer Vision. Cookbook. Implement machine learning solutions to overcome various computer vision challenges, de Jes&uacute;s Mart&iacute;nez. (2021) Packt Publishing)</h5>\r\n\r\n<p>&nbsp;</p>",
		"imagen_referencial" : "imagenes/TF_logo.png",
		"publicado" : 1,
		"fecha_publicacion" : "2022-08-25 11:11:05.000000",
		"autor_id" : 1,
		"categoria_id" : 2
	},
	{
		"id" : 10,
		"estado" : 1,
		"fecha_creacion" : "2022-10-07",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "El árbol de decisiones y el aprendizaje supervisado",
		"slug" : "el-arbol-de-decisiones-y-el-aprendizaje-supervisado",
		"descripcion" : "Vamos a construir un árbol de decisiones y nos acercaremos a algunas nociones sobre el paradigma del aprendizaje supervisado en el machine learning con la ayuda del paquete scikit-learn.",
		"contenido" : "<h2>Entendiendo la noci&oacute;n de &aacute;rbol de decisiones</h2>\r\n\r\n<p>En el siguiente post vamos a echar un vistazo a la noci&oacute;n de aprendizaje supervisado, espec&iacute;ficamente haciendo uso del algoritmo del &aacute;rbol de decisiones -<strong>decision trees</strong> en ingl&eacute;s. &Eacute;ste es un algoritmo muy versatil y f&aacute;cil de entender, ampliamente utilizado y adem&aacute;s sirve como bloque formativo de numerosos algoritmos avanzados con los que tropezaremos pronto en nuestro camino del machine learning.</p>\r\n\r\n<p>Aprenderemos a entrenar un &aacute;rbol de decisiones, a usarlo para problemas de clasificaci&oacute;n o regresi&oacute;n, adem&aacute;s de manipular y seleccionar los hiperpar&aacute;metros necesarios para mejorar su desempe&ntilde;o y eficiencia. Para ello usaremos un conjunto de datos del mundo real y que est&aacute; incluido en el paquete de scikit-learn. Aprenderemos a cargar los datos necesarios, analizarlos, nos acercaremos a las nociones de validaci&oacute;n cruzada (cross-validation) y la evaluaci&oacute;n de m&eacute;tricas del modelo. En fin, aprenderemos los siguientes t&oacute;picos:</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>* &iquest;Qu&eacute; son los &aacute;rboles de decisiones?</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>* &iquest;C&oacute;mo aprenden los &aacute;rboles de decisiones?</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>* &iquest;C&oacute;mo obtener mejores resultados?</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>* Manejar hiperpar&aacute;metros para mayor certeza</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>* Visualizaci&oacute;n de decisiones</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>* Construcci&oacute;n de un &aacute;rbol de regresi&oacute;n</p>\r\n\t</li>\r\n</ul>\r\n\r\n<h4>&nbsp;</h4>\r\n\r\n<h2>&iquest;Qu&eacute; son los &aacute;rboles de decisiones?</h2>\r\n\r\n<p>Un &aacute;rbol de decisiones no es m&aacute;s que un modelo de predicci&oacute;n usado en un amplio campo de actividades profesionales, la inteligencia artificial es una de ellas, que est&aacute; compuesto por un grupo de condiciones que van dibujando un conjunto de nodos, vectores y decisiones para seleccionar la mejor respuesta a partir de las condicionales propuestas. Con una configuraci&oacute;n parecida a la de un &aacute;rbol geneal&oacute;gico, los &aacute;rboles de decisiones en inteligencia artificial tienen como particularidad principal la de que grafican una ingente cantidad de datos, y adem&aacute;s sirven de base para otros modelos.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/28/flor_iris.jpg\" style=\"height:200px; width:300px\" /></p>\r\n\r\n<h2>Clasificaci&oacute;n de la Iris</h2>\r\n\r\n<p>scikit-learn viene con un n&uacute;mero de grupos de datos que podemos usar para probar nuestros algoritmos. Uno de estos conjuntos de datos es el de la Iris, un g&eacute;nero de plantas rizomatosas de la familia Iridaceae, como leemos en Wikipedia. En el conjunto de datos que vamos a utilizar hay solo tres especies, Setosa, Versicolor, y Virg&iacute;nica. Cada ejemplo que conforma nuestro conjunto de datos o dataset tiene la longitud y el ancho del sepalo y del p&eacute;talo de cada planta muestra, &eacute;stos ser&aacute;n nuestros datos de entrada, features como se designan en ingl&eacute;s, o caracter&iacute;sticas. Dependiendo de estos datos el algoritmo determinar&aacute; de cu&aacute;l especie se trata.</p>\r\n\r\n<h2>Cargando los datos</h2>\r\n\r\n<p>Lo primero que vamos a hacer en este ejercicio ser&aacute; entonces cargar nuestro conjunto de datos o dataset desde el m&oacute;dulo de datos de scikit-learn a una variable que llamaremos iris:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn import datasets\r\nimport pandas as pd\r\niris = datasets.load_iris()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Usamos el comando dir para ver los m&eacute;todos y atributos tiene el dataset:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">dir(iris)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos a obtener una lista con los sigientes elementos: DESCR, data, feature_names, feature_names, filename, target y target_names. Algunas veces los conjuntos de datos (datasets) vienen con una descripci&oacute;n breve por parte de su creador, pero esto no es lo com&uacute;n. En la vida real casi siempre debemos preguntar a las personas que recopilan los datos en primer lugar, para comprender lo que siginifica cada valor, o al menos tener alguna noci&oacute;n estad&iacute;stica de los mismos para entenderlos antes de usarlos.</p>\r\n\r\n<p>Por ahora, miremos la descripci&oacute;n que nos ofrece esta data:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">print(iris.DESCR)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-size:10px\"><span style=\"font-family:Arial,Helvetica,sans-serif\">.. _iris_dataset:</span></span></p>\r\n\r\n<p><span style=\"font-size:10px\">Iris plants dataset<br />\r\n--------------------</span></p>\r\n\r\n<p><span style=\"font-size:10px\">**Data Set Characteristics:**</span></p>\r\n\r\n<p><span style=\"font-size:10px\">&nbsp;&nbsp;&nbsp; :Number of Instances: 150 (50 in each of three classes)<br />\r\n&nbsp;&nbsp;&nbsp; :Number of Attributes: 4 numeric, predictive attributes and the class<br />\r\n&nbsp;&nbsp;&nbsp; :Attribute Information:<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sepal length in cm<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sepal width in cm<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - petal length in cm<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - petal width in cm<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - class:<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Iris-Setosa<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Iris-Versicolour<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Iris-Virginica</span></p>\r\n\r\n<p><span style=\"font-size:10px\">&nbsp; &nbsp; ....</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">Esta descripci&oacute;n nos da alguna informaci&oacute;n &uacute;til, y las de m&aacute;s inter&eacute;s son:</span></p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">La data est&aacute; compuesta de 150 filas(o ejemplos). Razonablemente un dataset peque&ntilde;o y esto influye mucho en la evaluaci&oacute;n de nuestro modelo.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">La etiqueta objetivo (target) toma tres valores -Iris-Setosa, Iris-Versicolor e Iris-Virginica. Algunos algoritmos de clasificaci&oacute;n solo trabajan con dos clases de etiquetas, son los llamados clasificadores binarios. Por suerte, el algoritmo del &aacute;rbol de decisiones puede funcionar con m&aacute;s de dos clases, por lo que no habr&aacute; problemas esta vez.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">La data est&aacute; balanceada. Hay 50 muestras para cada clase de flor. Esto es algo que debemos tener en mente cuando entrenemos y evaluemos nuestro modelo.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Tenemos cuatro caracter&iacute;sticas (features): largo del s&eacute;palo (sepal length), ancho del s&eacute;palo (sepal width), largo del p&eacute;talo (petal length) y ancho del p&eacute;talo (petal width); y estas cuatro caracter&iacute;sticas son num&eacute;ricas.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">No hay valores perdidos. Es decir, ninguna de nuestras muestran tienen valores nulos. Luego vamos a aprender c&oacute;mo hacer con estos valores perdidos cuando aparezcan en nuestros datasets.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Las dimensiones de los p&eacute;talos est&aacute;n mejor relacionados correlativamente con los tipos de flores que las dimensiones de los s&eacute;palos. Entender los datos es muy &uacute;til, pero el problema aqu&iacute; es que esta correlaci&oacute;n est&aacute; calculada para todo el dataset. Idealmente, solo lo calculamos para nuestros datos de entrenamiento. Esto, de todos modos vamos a ignorarlo por el momento y s&oacute;lo lo usaremos para comprobarlo m&aacute;s tarde.</span></li>\r\n</ul>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">Vamos ahora a transformar nuestro conjunto de datos en un DataFrame, o tabla de datos. El m&eacute;todo <strong>feature_names</strong> nos regresa los nombres de las caracter&iacute;sticas del dataset, mientras que el m&eacute;todo <strong>data</strong> regresa los valores en la forma de un arreglo de numpy. As&iacute; mismo, la variable objetivo, target en ingl&eacute;s, nos da los valores en forma num&eacute;rica (0, 1 y 2) de los nombres de cada clase de flores; target_names relaciona (mapea) estos valores con: Iris-Setosa, Iris-Versicolor y la Iris-Virg&iacute;nica, respectivamente.</span></p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">Si queremos ver las primeras ocho columnas, escribimos:</span></p>\r\n\r\n<pre>\r\n<code class=\"language-python\">iris.data[:8]</code></pre>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">&nbsp;Salida:</span></p>\r\n\r\n<pre>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">array([[5.1, 3.5, 1.4, 0.2],\r\n       [4.9, 3. , 1.4, 0.2],\r\n       [4.7, 3.2, 1.3, 0.2],\r\n       [4.6, 3.1, 1.5, 0.2],\r\n       [5. , 3.6, 1.4, 0.2],\r\n       [5.4, 3.9, 1.7, 0.4],\r\n       [4.6, 3.4, 1.4, 0.3],\r\n       [5. , 3.4, 1.5, 0.2]])</span></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Recuerda que esos son los valores en cent&iacute;metros de s&eacute;palos y p&eacute;talos. El siguiente c&oacute;digo usa los m&eacute;todos data, feature_names y target para combinar toda la informaci&oacute;n de los datos en un DataFrame y asigna los nombres de las columnas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df = pd.DataFrame(iris.data, columns = iris.feature_names)\r\ndf['target'] = pd.Series(iris.target)\r\n</code></pre>\r\n\r\n<p>La columna con los valores a predecir es la llamada target; de todos modos, para m&aacute;s claridad, podemos crear una nueva columna (target_names) que relaciona nuestros valores en digitos con el nombre de las clases de flores:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df['target_name'] = df['target'].apply(lambda y: iris.target_names[y])</code></pre>\r\n\r\n<p>Vamos a imprimir una muestra con seis filas para ver nuestro dataframe (tabla):</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df.sample(n=6)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<table border=\"1\">\r\n\t<thead>\r\n\t\t<tr>\r\n\t\t\t<th>&nbsp;</th>\r\n\t\t\t<th>sepal length (cm)</th>\r\n\t\t\t<th>sepal width (cm)</th>\r\n\t\t\t<th>petal length (cm)</th>\r\n\t\t\t<th>petal width (cm)</th>\r\n\t\t\t<th>target</th>\r\n\t\t\t<th>target_names</th>\r\n\t\t</tr>\r\n\t</thead>\r\n\t<tbody>\r\n\t\t<tr>\r\n\t\t\t<th>73</th>\r\n\t\t\t<td>6.1</td>\r\n\t\t\t<td>2.8</td>\r\n\t\t\t<td>4.7</td>\r\n\t\t\t<td>1.2</td>\r\n\t\t\t<td>1</td>\r\n\t\t\t<td>versicolor</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<th>18</th>\r\n\t\t\t<td>5.7</td>\r\n\t\t\t<td>3.8</td>\r\n\t\t\t<td>1.7</td>\r\n\t\t\t<td>0.3</td>\r\n\t\t\t<td>0</td>\r\n\t\t\t<td>setosa</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<th>118</th>\r\n\t\t\t<td>7.7</td>\r\n\t\t\t<td>2.6</td>\r\n\t\t\t<td>6.9</td>\r\n\t\t\t<td>2.3</td>\r\n\t\t\t<td>2</td>\r\n\t\t\t<td>virginica</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<th>78</th>\r\n\t\t\t<td>6.0</td>\r\n\t\t\t<td>2.9</td>\r\n\t\t\t<td>4.5</td>\r\n\t\t\t<td>1.5</td>\r\n\t\t\t<td>1</td>\r\n\t\t\t<td>versicolor</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<th>76</th>\r\n\t\t\t<td>6.8</td>\r\n\t\t\t<td>2.8</td>\r\n\t\t\t<td>4.8</td>\r\n\t\t\t<td>1.4</td>\r\n\t\t\t<td>1</td>\r\n\t\t\t<td>versicolor</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<th>31</th>\r\n\t\t\t<td>5.4</td>\r\n\t\t\t<td>3.4</td>\r\n\t\t\t<td>1.5</td>\r\n\t\t\t<td>0.4</td>\r\n\t\t\t<td>0</td>\r\n\t\t\t<td>setosa</td>\r\n\t\t</tr>\r\n\t</tbody>\r\n</table>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Dividiendo los datos</h2>\r\n\r\n<p>Vamos a dividir nuestros datos para entrenamiento y prueba del modelo a construir. Tomaremos el 70% de los datos disponibles para el entrenamiento y el otro 30% ser&aacute; para realizar pruebas. Estos valores no necesariamente son exigidos, a veces podemos escoger un 80%-20%, pero hoy vamos a escoger aquella relaci&oacute;n. La divisi&oacute;n de los datos se hace de manera autom&aacute;tica mediante el siguiente c&oacute;digo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.model_selection import train_test_split\r\ndf_train, df_test = train_test_split(df, test_size = 0.3)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Usamos df_train.shape[0] y df_test.shape[0] para verificar cuantas filas hay en cada conjunto de datos de entrenamiento y de prueba. Tambi&eacute;n podemos listar las columnas de nuestros conjuntos de datos usando el comando: df_train.columns y df_test.columns; ambos tendr&aacute;n la misma salida de seis columnas. Te invito a probar.</p>\r\n\r\n<p>Las primeras cuatro columnas son nuestras caracter&iacute;sticas (features), mientras que la quinta es nuestro objetivo a determinar o etiqueta (target). La sexta columna no la necesitaremos por ahora. Vamos a dividir ahora los conjuntos de datos teniendo esto en consideraci&oacute;n. &iquest;Qu&eacute; hacemos? Para alimentar nuestro modelo le introduciremos dos tipos de datos, unos que llamaremos caracter&iacute;sticas (comunmente las identificamos con una x) y el otro tipo de datos ser&aacute; la etiqueta que est&aacute; relacionada con cada uno de los datos previos x; por eso la identificaremos con una y.</p>\r\n\r\n<p>Como sabemos, el m&eacute;todo feature_names contiene en este caso la lista de las columnas con las caracter&iacute;sticas de la data. Usaremos est&aacute; informaci&oacute;n para crear nuestros conjuntos x e y que nos servir&aacute;n para alimentar el algoritmo de ML.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">x_train = df_train[iris.feaure_names] # Toma de df_train las columnas con las medidas de las muestras. \r\nx_test = df_test[iris.feature_names] # Toma de df_test las columnas con las medidas de las muestras.\r\n\r\ny_train = df_train['target'] # Toma para entrenamiento sólo la columna de la etiqueta, con los nombres de las flores.\r\ny_test = df_train['target'] # Toma para las pruebas sólo la columna con los nombres de las flores.</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Entrenando el modelo y haciendo prediciones</h2>\r\n\r\n<p>Ahora vamos a entrenar nuestro modelo usando la configuraci&oacute;n por defecto, por ahora, luego veremos como configurar el algoritmo del &aacute;rbol de decisiones para mejorar los resultados.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.tree import DecisionTreeClassifier\r\n\r\n# Debemos instanciar nuestro clasificador\r\nclf = DecisionTreeClassifier()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos a entrenar el modelo, simplemente introduciendo los valores que hemos extra&iacute;do de nuestro conjunto de datos para tal fin. As&iacute; es como el algoritmo usa los datos para aprender. En los modelos de scikit_learn el m&eacute;todo fit(), en ingl&eacute;s fitting significa entrenando, para entrenar tomando los datos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">clf.fit(x_train, y_train)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Luego del entrenamiento, la instancia de nuestro modelo, clf en este caso, puede hacer las predicciones por medio de los datos de prueba (x_test):</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Llamaremos a nuestras predicciones y_test_pred\r\ny_test_pred = clf.predict(x_test)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Evaluando las predicciones</h2>\r\n\r\n<p>Como ya tenemos nuestras predicciones (y_test_pred), ahora necesitamos comparar esas predicciones con nuestro conjunto de prueba (y_test) y ver que tal le fue a nuestro modelo en su trabajo predictivo. Si bien es verdad que hay varias m&eacute;tricas de evaluaci&oacute;n de una clasificacio&oacute;n, como <em>precision</em>, <em>recall</em> y <em>accuracy</em>, este dataset tiene la caracter&iacute;stica de estar balanceado, es decir, tiene el mismo n&uacute;mero de muestras para cada clase. Por lo tanto, podemos unar la m&eacute;trica de accuracy (certeza) en este caso.</p>\r\n\r\n<p>Vamos a calcular la certeza:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.metrics import accuracy_score\r\naccuracy_score(y_test, y_test_pred)</code></pre>\r\n\r\n<p>Que nos da un valor de 0.933. Este valor puede variar en tu salida pero en realidad no importa, va a oscilar entre dicho valor.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si llegaste hasta ac&aacute;, pues, &iexcl;&iexcl;&iexcl;&iexcl;felicitaciones!!! Has entrenado tu algoritmo de aprendizaje supervisado y has aprendido algunas nociones sobre el &aacute;rbol de decisiones. Espero lo hayas disfrutado, y tambi&eacute;n espero que sigamos aprendiendo juntos.&nbsp;</p>\r\n\r\n<h5><span style=\"font-family:Arial,Helvetica,sans-serif\">(Versionado del libro: Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits, 2020, de Tarek Amr)</span></h5>\r\n\r\n<p>&nbsp;</p>",
		"imagen_referencial" : "imagenes/post-robot8_YBR6QTW.jpg",
		"publicado" : 1,
		"fecha_publicacion" : "2022-10-07 19:10:37.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 11,
		"estado" : 1,
		"fecha_creacion" : "2022-10-28",
		"fecha_modificacion" : "2023-05-07",
		"fecha_eliminacion" : "2023-05-07",
		"titulo" : "Una introducción a pandas I",
		"slug" : "una-introduccion-a-pandas-i",
		"descripcion" : "Pandas es una de las más famosas bibliotecas escritas en y para Python, y está esencialmente diseñada para replicar una hoja de cálculo en formato de este lenguaje. En esta primera parte de una serie que quiere acercarte a esta herramienta tan útil y de fácil uso aprenderemos sus comandos más básicos a fin de conocer su potencial y la forma en que nos puede ayudar en nuestro camino al análisis de datos.",
		"contenido" : "<h2><img alt=\"pandas\" src=\"https://media.istockphoto.com/photos/giant-panda-bear-picture-id475636556?b=1&amp;k=20&amp;m=475636556&amp;s=170667a&amp;w=0&amp;h=zPcCptPj5UVt5zXgPxBX9LqG6JjriVSCYWl2cIrE324=\" style=\"float:left; height:100px; margin-right:10px; width:200px\" />Conociendo pandas&nbsp;</h2>\r\n\r\n<p>La principal caracter&iacute;stica y por la cual ha sido desarrollado <strong>pandas</strong> es que permite la manipulaci&oacute;n de datos para su an&aacute;lisis, y es una herramienta muy poderosa para preprocesar datasets antes de implementar cualquier modelo de machine learning. Construir varios modelos de ML se hace mucho m&aacute;s f&aacute;cil despu&eacute;s de aprender los fundamentos de esta biblioteca, que junto a numpy, otra biblioteca basada en python, da excelente ayuda en el an&aacute;lisis de datos. Es una biblioteca open source, de libre uso, que provee un alto desempe&ntilde;o, adem&aacute;s de que brinda el uso f&aacute;cil de estructuras de datos. Se pueden procesar datos en diferentes formatos, como por ejemplo: CSV, JSON, Excel, HTML, PDF, entre otros.</p>\r\n\r\n<p>Para esta pr&aacute;ctica, vamos a usar el entorno de Anaconda, que ya viene preparada con la mayor&iacute;a de las librer&iacute;as que usaremos en el mundo del machine learning. S&oacute;lo necesitamos abrir un archivo nuevo en nuestro Jupyter notebook y comenzaremos a aprender.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Creando tablas</h2>\r\n\r\n<p>En pandas existen muchas formas para crear tablas; una de ellas, por ejemplo, es desde un diccionario. Primero importamos la biblioteca y le damos un alias, por convenci&oacute;n es pd:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import pandas as pd # Usamos el alias por conveniencia.\r\n\r\ndiccionario = {'Columna_1': [10,20,30], 'Columna_2': [40,50,60], 'Columna_3': [70,80,90]}\r\ndiccionario</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\n{&#39;Columna_1&#39;: [10, 20, 30],\r\n &#39;Columna_2&#39;: [40, 50, 60],\r\n &#39;Columna_3&#39;: [70, 80, 90]}</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para convertir este diccionario en una tabla de datos, usamos el comando pd.DataFrame.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df_diccionario = pd.DataFrame(diccionario)\r\ndf_diccionario</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/03/df_diccionario.png\" style=\"height:128px; width:274px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&iquest;Y qu&eacute; tal si ahora creamos un dataframe, o datos en formato de tabla, usando para ello una lista de nombres?, por ejemplo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">lista = ['Teresa', 'Lara', 'Marisol', 'Agustín', 'Brenda']</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Con esta lista vamos a crear nuestra tabla, s&oacute;lo pas&aacute;ndola como par&aacute;metro en nuestro m&eacute;todo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df = pd.DataFrame(lista)\r\ndf</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/03/lista.png\" style=\"height:176px; width:104px\" /></p>\r\n\r\n<p>Ahora bien, &iquest;qu&eacute; tal si tenemos dos listas y queremos que conformen nuestra tabla de datos?:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">nombres = ['Teresa', 'Lara', 'Marisol', 'Agustín', 'Brenda']\r\nedad = [30, 20, 25, 22, 20]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Lo que debemos hacer es pasar ciertos m&eacute;todos como valores a la funci&oacute;n de pandas. Pasamos las listas, dentro del m&eacute;todo zip que las toma y las une concatenando los valores respectivos; tambi&eacute;n le vamos a pasar los nombres de las columnas que queremos tenga nuestra tabla:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">df = pd.DataFrame(list(zip(nombres, edad)), columns = ['Nombre', 'Edad'])\r\ndf</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/03/nombres_edad.png\" style=\"height:166px; width:135px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Generando un data frame</h2>\r\n\r\n<p>Pandas nos puede ayudar en cualquier situaci&oacute;n en la que necesitemos visualizar y procesar datos; una muestra de esto ahora la vamos a aplicar en el &aacute;rea de la exploraci&oacute;n y explotaci&oacute;n del petr&oacute;leo y gas, por ejemplo. Vamos a crear un dataframe generando valores aleatorios, alimentado por una semilla igual a 100, para llenar las celdas de nuestra tabla, con ocho filas y cinco columnas. Esta vez usamos como par&aacute;metro para crear nuestros indices (index) una cadena de palabras a las que separamos por medio del comando <em>split()</em>. Este lo que hace es seleccionar cada &iacute;ndice despues del espacio en blanco; les asignamos el nombre a las columnas mediante una cadena de palabras, <em>columns</em>, e igual, las separamos mediante el mismo comando <em>split():</em></p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import numpy as np\r\nseed = 100\r\nnp.random.seed(seed)\r\n\r\nciclo_vida = pd.DataFrame(np.random.randn(8,5), index = 'Terreno Sísmica Geología Perforación Terminaciones Producción Facilidades Centro_de_corriente'.split(), columns= 'Ciclo_1 Ciclo_2 Ciclo_3 Ciclo_4 Ciclo_5'.split())\r\nciclo_vida</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/03/pandas_tabla.png\" style=\"height:236px; width:480px\" /></p>\r\n\r\n<p>Si queremos presentar las primeras filas o las &uacute;ltimas, usamos los comandos&nbsp;<em>head() y tail()</em>, respectivamente,&nbsp;pasando como par&aacute;metro el n&uacute;mero de l&iacute;neas que deseamos se muestren.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algunos estad&iacute;sticos b&aacute;sicos</h2>\r\n\r\n<p>Otra funci&oacute;n que usamos mucho para comprender los datos que estamos manejando es&nbsp;<em>.describe</em>&nbsp;y que nos provee de valores estad&iacute;sticos b&aacute;sicos (media, cantidad, desviaci&oacute;n est&aacute;ndar, valores m&aacute;ximos y m&iacute;nimos, los intercuartiles) de cada columna. Para esta tabla s&oacute;lo debemos escribir:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida.describe()</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/03/estadisticos_pandas.png\" style=\"height:243px; width:400px\" /></p>\r\n\r\n<p>Para seleccionar una columna del data frame es suficiente pasar el nombre de la columna dentro de corchetes, as&iacute;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida['Ciclo_2']</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si queremos seleccionar varias columnas a la vez, las encerramos en corchetes dobles:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida[['Ciclo_2', 'Ciclo_3', 'Ciclo_4']]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/ciclos_234.png\" style=\"height:237px; width:349px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>C&aacute;lculos entre columnas con pandas</h2>\r\n\r\n<p>Con pandas es muy f&aacute;cil hacer c&aacute;lculos r&aacute;pidos. Por ejemplo, vamos a sumar todas las columnas de la tabla y crearemos otra columna a la que llamaremos &quot;Total_ciclos&quot;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida['Total_ciclos'] = ciclo_vida['Ciclo_1']+ciclo_vida['Ciclo_2']+ciclo_vida['Ciclo_3']+ciclo_vida['Ciclo_4']+ciclo_vida['Ciclo_5']\r\nciclo_vida['Total_ciclos']</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/suma_total.png\" style=\"height:160px; width:283px\" /></p>\r\n\r\n<p>Ahora multiplicamos las dos primeras columnas de la tabla y llamaremos al resultado&quot;Ciclo_1_x_2&quot;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida['Ciclo_1_x_2']= ciclo_vida['Ciclo_1']*ciclo_vida['Ciclo_2']</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora si vemos el dataframe nos mostrar&aacute; los datos completos incluyendo las columnas que hemos creado:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/tabla_total.png\" style=\"height:245px; width:644px\" /></p>\r\n\r\n<p>Eliminar columnas o filas son tareas que muchas veces necesitaremos realizar cuando estemos frente a un dataframe. Aprendamos c&oacute;mo hacerlas.</p>\r\n\r\n<p>Para eliminar columnas o filas usaremos el m&eacute;todo <em>.drop().&nbsp;</em>Si queremos que la eliminaci&oacute;n que hagamos sea permanente, nos debemos asegurar de incluir &quot;inplace = True&quot; porque por defecto esta caracter&iacute;stica es Falsa en pandas de Python. Si queremos eliminar filas, escogemos como valor de axis = 0 (tambi&eacute;n es el valor por defecto) y para eliminar columnas, usamos axis = 1.</p>\r\n\r\n<p>Vamos a eliminar las columnas que hemos agregado recientemente seg&uacute;n estas indicaciones:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ciclo_vida.drop(labels=['Total_ciclos', 'Ciclo_1_x_2'], axis=1, inplace=True)\r\nciclo_vida</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como ver&aacute;s, la tabla que se nos muestra es la misma que ten&iacute;amos antes. Elimina, por favor, las dos filas iniciales siguiendo el mismo procedimiento.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Comandos <em>loc</em> e <em>iloc</em></h2>\r\n\r\n<p>Vamos a crear una matriz con n&uacute;meros aleatorios para aprender a seleccionar filas y columnas por medio de&nbsp;<em>loc&nbsp;</em>e&nbsp;<em>iloc:</em></p>\r\n\r\n<pre>\r\n<code class=\"language-python\">semilla=200\r\nnp.random.seed(seed)\r\nmatriz = pd.DataFrame(np.random.randn(5,5), columns= 'Ciclo_1 Ciclo_2 Ciclo_3 Ciclo_4 Ciclo_5'.split())\r\nmatriz</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/nueva_tabla.png\" style=\"height:165px; width:360px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si quisieramos seleccionar de esta matriz las filas desde la primera (0) hasta la tercera (2) con todas las columnas, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">matriz.loc[0:2, :] # Si escribimos: matriz.loc[[0,1,2],:] obtenemos el mismo resultado</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/tabla_loc.png\" style=\"height:107px; width:356px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para seleccionar todas las filas pero de las dos primeras columnas, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">matriz.loc[:, ['Ciclo_1', 'Ciclo_2']]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/05/04/ciclos_1_2.png\" style=\"height:164px; width:167px\" /></p>\r\n\r\n<p>Debemos recordar que&nbsp;<em>loc&nbsp;</em>es inclusivo en ambos lados del rango de selecci&oacute;n, es decir, cuando seleccionamos: matriz.loc[0:3, :] nos devuelve las filas 0, 1, 2 y 3. Sin embargo, cuando usamos&nbsp;<em>iloc</em>, solo se estar&aacute; incluyendo el primer n&uacute;mero, pero se excluir&aacute; el segundo. Si seleccionamos: matriz.iloc[0:3, :], nos devolver&aacute; las filas 0, 1, 2 pero no la 3. De todos modos, cuando usamos&nbsp;<em>iloc</em>, se incluir&aacute; el valor relacionado al primer n&uacute;mero pero no el relacionado al segundo. Si queremos seleccionar, por ejemplo, todas las filas de la matriz pero s&oacute;lo las de las dos primeras columnas, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">matriz.iloc[:, 0:2]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Esto devolver&aacute; la salida anterior (&quot;matriz.loc[:, [&#39;Ciclo_1&#39;, &#39;Ciclo_2&#39;]]&quot;). Si queremos un valor espec&iacute;fico de la matriz, le pasamos la posici&oacute;n al comando&nbsp;<em>iloc</em>, as&iacute;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">matriz.iloc[2,3]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Entonces, y para concluir por el momento,&nbsp;<em>loc</em>&nbsp;es usado principalmente para seleccionar usando las etiquetas (en este caso: Ciclo_1, Ciclo_2, etc.) mientras que&nbsp;<em>iloc</em>&nbsp;es usado para seleccionar usando expresiones enteras de posici&oacute;n, de alli su nombre (<strong>integer location</strong> - iloc).</p>\r\n\r\n<p>Podemos seguir en otra oportunidad descubriendo las caracter&iacute;sticas y m&aacute;s funciones de esta potente y f&aacute;cil herramienta que nos va a ayudar en nuestro camino a trav&eacute;s de la ciencia y el an&aacute;lisis de datos dentro del campo del machine learning, y espero que nos volvamos a encontrar para seguir aprendiendo. &iexcl;Hasta pronto!</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5>(Tomado y versionado de los libros: Master Data Science and Data Analysis witth Pandas, de Arun, 2020 y Machine Learning Guide for Oil and Gas using Python, de Belyadi, H. y A. Haghighat, 2021)</h5>",
		"imagen_referencial" : "imagenes/Pandas_logo.png",
		"publicado" : 1,
		"fecha_publicacion" : "2022-10-28 20:20:32.000000",
		"autor_id" : 1,
		"categoria_id" : 1
	},
	{
		"id" : 12,
		"estado" : 1,
		"fecha_creacion" : "2022-10-31",
		"fecha_modificacion" : "2023-05-07",
		"fecha_eliminacion" : "2023-05-07",
		"titulo" : "¿Cuánto de mates debemos saber para aprender machine learning?",
		"slug" : "cuanta-matematicas-debemos-saber-para-aprender-machine-learning",
		"descripcion" : "Muchas veces nos preguntamos si los conocimientos que manejamos nos serán suficientes para poder sumergirnos en las aguas, no sólo de la inteligencia artificial, sino en el mundo mucho más amplio de la informática. En esta breve lectura hablaremos un poco sobre algunas cosas que debemos tener presente si queremos abrirnos paso en el aprendizaje del machine learning.",
		"contenido" : "<h2>Las matem&aacute;ticas y el machine learning</h2>\r\n\r\n<p>Hace poco lei el muy interesante libro de Yuval N. Harari, <em>Sapiens, A brief history of humankind</em> (Vintage, 2015) y recojo aqu&iacute; las consideraciones que expone con respecto al conocimiento matem&aacute;tico:&nbsp;</p>\r\n\r\n<blockquote>\r\n<p>When most laypeople see such an equation, they usually panic and freeze, like a deer caught in the headlights of speeding vehicle. The reaction is quite natural, and does not betray a lack of intelligence or curiosity. (p. 147).</p>\r\n</blockquote>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>La ecuaci&oacute;n a la que hace referencia el autor es la que calcula la aceleraci&oacute;n de la masa i bajo la influencia de la gravedad en la Teor&iacute;a de la Relatividad, pero podemos trasnpolar esas mismas palabras a cualquier cadena de s&iacute;mbolos matem&aacute;ticos que encontremos en nuestra vida. La raz&oacute;n no es simple porque encierra principios evolutivos y sociales que no podemos tocar pero que tangencialmente podr&iacute;amos se&ntilde;alar como que nuestro cerebro aprende con esfuerzo las relaciones matem&aacute;ticas, y con menos esfuerzo a comunicarnos por medio del lenguaje.</p>\r\n\r\n<p>Para aprender s&oacute;lo necesitamos dos cosas: curiosidad y empe&ntilde;o, sin estos elementos en nuestra mochila no podremos ir muy lejos en el aprendizaje de ninguna tarea.</p>\r\n\r\n<p>De todos modos podemos a decir que para aprender machine learning no es necesario ser un profesor de matem&aacute;ticas, ni siquiera un conocedor en profundidad de los t&eacute;rminos matem&aacute;ticos que se manejan, pero en la medida en que conozcamos las herramientas que nos brinda y los conocimientos que aporta, en esa medida podemos tener una base m&aacute;s s&oacute;lida para desempe&ntilde;arnos con seguridad en el campo de la inteligencia artificial. Si no nos gustan las matem&aacute;ticas, o si no es nuestra &aacute;rea favorita de saber, veamos su aprendizaje y su puesta en pr&aacute;ctica como un excelente ejercicio intelectual que beneficia nuestro entendimiento y nuestra comprension del mundo, adem&aacute;s de brindarle a nuestro cerebro un entrenamiento cotidiano potente.&nbsp;</p>\r\n\r\n<p>La inteligencia artificial, y todas las ramas que est&aacute;n inmersas en ella, trata de matematizar la realidad para que las computadoras, por medio de algoritmos especializados, puedan ser alimentadas con dichos datos, que llegan a ser de una cantidad enorme, y por medio de esta tarea, buscar patrones en donde el ojo humano no puede encontrarlos. En el Deep Learning, por ejemplo, muchas veces las redes neuronales captan fotogramas, o im&aacute;genes de cualquier tipo, descomponi&eacute;ndolos en pedazos peque&ntilde;os y transformadas en tensores, especies de matrices matem&aacute;ticas,que van a ser digeridos por neuronas especializadas para que las computadoras puedan procesarlas, analizarlas y clasificarlas; proceso que se realiza una y otra vez hasta que la red neuronal puede construir patrones que le van a permitir clasificar y determinar, por ejempo, si una imagen representa a un perro o a un gato, u otro objeto.</p>\r\n\r\n<p>Entonces, para realizar esto, son necesarias muchas herramientas matem&aacute;ticas y algunas de ellas est&aacute;n encerradas dentro del campo del algebra lineal. Por medio de esta rama de las matem&aacute;ticas podemos representar datos en forma de vectores y matrices, datos en forma de tablas. Adem&aacute;s, la geometr&iacute;a anal&iacute;tica permite realizar calculos con estos vectores, de forma que podamos hallar similitudes entre vectores y de esa forma realizar predicciones de desempe&ntilde;o.</p>\r\n\r\n<p>Otra rama que aporta mucha informaci&oacute;n y bases de c&aacute;lculo es la estad&iacute;stica y la teor&iacute;a de la probabilidad, que van a permitir manejar y entender conceptos como rango de confianza en los c&aacute;lculos y predicciones, certezas y errores, que arrojaran la manipulaci&oacute;n de los datos. El an&aacute;lisis matem&aacute;tico tambi&eacute;n es utilizado en estos tipos de c&aacute;lculos.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><span class=\"math-tex\">\\(x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} \\)</span></p>",
		"imagen_referencial" : "imagenes/matematicas.png",
		"publicado" : 1,
		"fecha_publicacion" : "2022-10-31 20:48:41.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 13,
		"estado" : 1,
		"fecha_creacion" : "2022-11-03",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Vamos a graficar un dataset",
		"slug" : "vamos-a-graficar-un-dataset",
		"descripcion" : "Desde antigüo se sabe, lo dice ya Aristóteles en su Metafísica, que al ser humano le gusta ver las cosas para conocerlas y estudiarlas mejor y ese principio rige también el mundo del machine learning, porque al visualizar una data podemos acercarnos de manera más eficaz al problema que se nos está planteando y tomar decisiones más acertadas sobre él.",
		"contenido" : "<h2>Importar y visualizar datos</h2>\r\n\r\n<p>Vamos a realizar un ejercicio pr&aacute;ctico para adentrarnos de lleno en el mundo del machine learning y as&iacute; aprender a desenvolvernos con seguridad en &eacute;l, y de paso, aprenderemos a utilizar algunas de las librer&iacute;as que est&aacute;n dise&ntilde;adas para visualizar los datos, la energ&iacute;a de nuestro trabajo y del mundo moderno.</p>\r\n\r\n<p>Entre estas herramientas de visualizaci&oacute;n&nbsp; de datos se encuentran matplotlib, seaborn, plotpy, que usaremos junto a pandas y a numpy para analizar de manera gr&aacute;fica los datasets que alimentar&aacute;n nuestro trabajo en el machine learning.</p>\r\n\r\n<p>La visualizacion de datos es uno de los aspectos m&aacute;s importantes en la preparaci&oacute;n y an&aacute;lisis de datos. En este paso usaremos varios gr&aacute;ficos, como de distribuci&oacute;n, de caja, de puntos, etc., para mostrar los datos e identificar valores anormales. Estos valores pueden visualizarse e identificarse m&aacute;s f&aacute;cilmente con algunos gr&aacute;ficos. Al usar la visualizaci&oacute;n de datos podemos ayudarnos a entender los entresijos y es un paso fundamental antes de aplicar el algoritmo apropiado de machine learning.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Matplotlib</h2>\r\n\r\n<p>Es una de las m&aacute;s comunes e importantes librer&iacute;as de graficar en Python. Provee gran flexibilidad al momento de dibular cualquier aspecto de un gr&aacute;fico. Dise&ntilde;ada para darle al usuario una interaci&oacute;n similar a la graficadora de Matlab, una de sus principales ventajas es su versatilidad, cualquier cosa puede ser representada. De todos modos, puede ser un reto realizar gr&aacute;ficos m&aacute;s complejos, pues requerir&iacute;an m&aacute;s c&oacute;digo.</p>\r\n\r\n<p>Matplotlib crea im&aacute;genes est&aacute;ticas en archivos como JPEG o PNG, y podemos guardarlas y usarlas cuando las necesitemos.</p>\r\n\r\n<p>Para empezar vamos a crear un cuaderno en Jupyter Notebook, luego de darle un nombre adecuado, escribiremos en la primera celda la llamada a las bibliotecas que vamos a estar usando:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Importamos las bibliotecas necesarias\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\nx = np.linespace(0,10,100)\r\ny = x**3</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora graficaremos las variables x e y definidas usando &quot;plt.plot&quot;. Podemos usar cualquier color, y en este caso, usaremos el rojo, como grosor de l&iacute;nea usaremos 4 y como estilo &quot;-&quot;. Adicional a eso, &quot;plt.xlabel&quot; y &quot;plt.ylabel&quot; nos dar&aacute;n las etiquetas de ordenadas y abscisas; adem&aacute;s,&nbsp; le daremos un t&iacute;tulo con el atributo &quot;plt.title&quot;.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">plt.plot(x,y,'red',linewidth=4, linestyle=\"--\")\r\nplt.xlabel('Tiempo')\r\nplt.ylabel('Número de iteraciones')\r\nplt.title('Iteraciones vs. Tiempo') </code></pre>\r\n\r\n<pre>\r\n<code class=\"language-python\">plt.plot(x,y,'red',linewidth=4, linestyle=\"--\")\r\nplt.xlabel('Tiempo')\r\nplt.ylabel('Número de iteraciones')\r\nplt.title('Iteraciones vs. Tiempo')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<div style=\"background:#eeeeee; border:1px solid #cccccc; padding:5px 10px\">\r\n<pre>\r\n<code class=\"language-python\">plt.plot(x,y,'red',linewidth=4, linestyle=\"--\")\r\nplt.xlabel('Tiempo')\r\nplt.ylabel('Número de iteraciones')\r\nplt.title('Iteraciones vs. Tiempo')</code></pre>\r\n</div>",
		"imagen_referencial" : "imagenes/matplotlib.jpg",
		"publicado" : 0,
		"fecha_publicacion" : "2022-11-03 19:56:14.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 14,
		"estado" : 1,
		"fecha_creacion" : "2023-01-06",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Breve introducción a numpy",
		"slug" : "breve-introduccion-a-numpy",
		"descripcion" : "Numpy es una librería esencial para el manejo de datos en la inteligencia artificial al permitir realizar transformaciones matemáticas propias del algebra lineal en nuestros algoritmos",
		"contenido" : "<p>Numpy es una biblioteca del algebra lineal escrita en Python y una sobre las cuales otras bibliotecas est&aacute;n construidas. Considerada muy r&aacute;pida y muy f&aacute;cil de usar e importar como biblioteca principal para los tipos de an&aacute;lisis en machine learning, permite tambi&eacute;n transformar im&aacute;genes y otros tipos de datos en tensores precisamente para que puedan ser procesadas por las computadoras en el &aacute;rea del deep learning.</p>\r\n\r\n<p>Su uso en ML se centra m&aacute;s que todo en la manipulacic&oacute;n de vectores y matrices, pero recordemos que un vector es un arreglo (array) unidimensional, mientras que las matrices son arreglos de varias dimensiones.</p>\r\n\r\n<p>Para esta pr&aacute;ctica utilizaremos Jupiter Notebook, que viene integrado con Anaconda, una de las mejores herramientas para aprender inteligencia artificial.</p>\r\n\r\n<p>Vamos a generar una lista de n&uacute;meros a la que llamaremos &quot;A&quot;. La lista luego la transformaremos en un arreglo usando la funci&oacute;n <em>np.array</em>:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import numpy as np\r\n\r\nA = [1,2,3,4,5,6,7,8,9,10]\r\nnp.array(A)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([ 1, &nbsp;2, &nbsp;3, &nbsp;4, &nbsp;5, &nbsp;6, &nbsp;7, &nbsp;8, &nbsp;9, 10])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Un arreglo de dos dimensiones puede ser generado como sigue:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">B = [[4,5,6],[7,8,9],[10,11,12]]\r\nnp.array(B)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[ 4, &nbsp;5, &nbsp;6],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [ 7, &nbsp;8, &nbsp;9],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [10, 11, 12]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Existen varias funciones que podemos usar para generar un array. Por ejemplo, para generar un array que comience en 0, termine en 40, y tenga un incremento de 5, debemos escribir:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.arange(0,40,5)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([ 0, &nbsp;5, 10, 15, 20, 25, 30, 35])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Esta biblioteca puede ser usada tambi&eacute;n para crear arreglos de una o dos dimensiones usando ceros (0) o unos (1), as&iacute;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.zeros(5)</code></pre>\r\n\r\n<p>&nbsp;Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([0., 0., 0., 0., 0.])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.zeros((5,5))</code></pre>\r\n\r\n<p>Salida:<br />\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">array([[0., 0., 0., 0., 0.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[0., 0., 0., 0., 0.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[0., 0., 0., 0., 0.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[0., 0., 0., 0., 0.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[0., 0., 0., 0., 0.]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.ones(5)</code></pre>\r\n\r\n<p>Salida:<br />\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">array([1., 1., 1., 1., 1.])</span></p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.ones((5,5))</code></pre>\r\n\r\n<p>Salida:<br />\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">array([[1., 1., 1., 1., 1.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[1., 1., 1., 1., 1.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[1., 1., 1., 1., 1.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[1., 1., 1., 1., 1.],<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;[1., 1., 1., 1., 1.]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Otra funci&oacute;n muy &uacute;til de numpy es np.linspace, usada para crear arreglos que comiencen con cierto valor, terminen con otro y tengan un incremento entre ambos. Por ejemplo, vamos a crear un arreglo desde 0 a 20 y que tenga un incremento igual entre ambos de 10:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.linspace(0,20,10)</code></pre>\r\n\r\n<p>Salida:<br />\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">array([ 0. &nbsp; &nbsp; &nbsp; &nbsp;, &nbsp;2.22222222, &nbsp;4.44444444, &nbsp;6.66666667, &nbsp;8.88888889,<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp;11.11111111, 13.33333333, 15.55555556, 17.77777778, 20. &nbsp; &nbsp; &nbsp; &nbsp;])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&quot;np.linspace&quot; es una funci&oacute;n muy &uacute;til que puede ser usada para generar arreglos para an&aacute;lisis de sensibilidad despu&eacute;s de entrenar modelos en machine learning.</p>\r\n\r\n<p>En &aacute;lgebra lineal, la matriz identidad (llamada tambi&eacute;n matriz unitaria) es una matriz cuadrada de n por n con la diagonal principal compuesta s&oacute;lo de unos (1) y ceros (0) en el resto. La biblioteca numpy puede usarse para generar esta matriz identidad:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.eye(10)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>El resultado es una matriz identidad de 10 por 10 con 1&#39;s en la diagonal principal y 0&#39;s en el resto.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Generaci&oacute;n de n&uacute;meros aleatorios en numpy</h2>\r\n\r\n<p>Numpy tiene muchas funcionalidades cuando se trata de generar n&uacute;meros aleatorios. Por ejemplo, para generar n&uacute;meros aleatorios desde una distribuci&oacute;n uniforme, usamos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">seed=100\r\nnp.random.seed(seed)\r\nnp.random.rand(5)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([0.54340494, 0.27836939, 0.42451759, 0.84477613, 0.00471886])</span></p>\r\n\r\n<p>Para generar una matriz de 5 por 5, con n&uacute;meros aleatorios, usamos el siguiente c&oacute;digo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">seed=100\r\nnp.random.seed(seed)\r\nnp.random.rand(5,5)</code></pre>\r\n\r\n<p>Salida:&nbsp;</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[0.54340494, 0.27836939, 0.42451759, 0.84477613, 0.00471886],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0.12156912, 0.67074908, 0.82585276, 0.13670659, 0.57509333],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0.89132195, 0.20920212, 0.18532822, 0.10837689, 0.21969749],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0.97862378, 0.81168315, 0.17194101, 0.81622475, 0.27407375],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [0.43170418, 0.94002982, 0.81764938, 0.33611195, 0.17541045]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora bien, debes notar que los n&uacute;meros aleatorios que obtienes son los mismos que est&aacute;n aqu&iacute; generados, eso es debido a que la semilla (seed) indica cu&aacute;les deben ser generados. Puedes fijar la semilla (seed) o eliminarla del c&oacute;digo para obtener diferentes n&uacute;meros aleatorios cada vez que ejecutes el mismo.</p>\r\n\r\n<p>Si deseas obtener una distribuci&oacute;n normal o gaussiana de los n&uacute;meros aleatorios, s&oacute;lo a&ntilde;adimos &quot;n&quot; al final del comando generador (np.random.rand) y quedar&iacute;a as&iacute;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">seed=100\r\nnp.random.seed(seed)\r\nnp.random.randn(5)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([-1.74976547,&nbsp; 0.3426804 ,&nbsp; 1.1530358 , -0.25243604,&nbsp; 0.98132079])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">seed=100\r\nnp.random.seed(seed)\r\nnp.random.randn(5,5)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[-1.74976547,&nbsp; 0.3426804 ,&nbsp; 1.1530358 , -0.25243604,&nbsp; 0.98132079],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ 0.51421884,&nbsp; 0.22117967, -1.07004333, -0.18949583,&nbsp; 0.25500144],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [-0.45802699,&nbsp; 0.43516349, -0.58359505,&nbsp; 0.81684707,&nbsp; 0.67272081],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [-0.10441114, -0.53128038,&nbsp; 1.02973269, -0.43813562, -1.11831825],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ 1.61898166,&nbsp; 1.54160517, -0.25187914, -0.84243574,&nbsp; 0.18451869]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para generar aleatorios enteros usamos &quot;np.random.randint(). Para obtener, por ejemplo 10 enteros aleatorios entre 0 y 500, escribimos, sin usar una semilla:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">np.random.randint(1,500,10)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora vamos a crear un arreglo entre 20 y 30 (excluido &eacute;ste) y reformamos el arreglo en una matriz de 2 dimensiones de 5 por 2. La funci&oacute;n para reformar es muy &uacute;til para transformar varios arreglos a diferentes formas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">A=np.arange(20,30)\r\nA.reshape(5,2)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[20, 21],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [22, 23],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [24, 25],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [26, 27],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [28, 29]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Adicional a esto, los valores m&aacute;ximos, m&iacute;nimos, localizaci&oacute;n de m&aacute;ximo y m&iacute;nimo, media y desviaci&oacute;n estandar de los arreglos pueden hallarse usando las siguientes claves:</p>\r\n\r\n<p>A.min = se obtiene el valor m&iacute;nimo del arreglo A</p>\r\n\r\n<p>A.max = se obtiene el valor m&aacute;ximo del arreglo A</p>\r\n\r\n<p>A.argmin = se obtiene la localizaci&oacute;n del valor m&iacute;nimo del arreglo A</p>\r\n\r\n<p>A.argmax = se obtiene la localizaci&oacute;n del valor m&aacute;ximo del arreglo A</p>\r\n\r\n<p>A.std = se obtiene la desviaci&oacute;n estandar en el arreglo A</p>\r\n\r\n<p>A.mean = para obtener la media en el arreglo A</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Indexaci&oacute;n y selecci&oacute;n en numpy</h2>\r\n\r\n<p>La indexaci&oacute;n y selecci&oacute;n en numpy son herramientas muy importantes y debemos aprender su uso. Vamos a crear un array entre 30 y 40 (incluyendo a &eacute;ste) y le daremos al arreglo el nombre de X. Seleccionaremos los de &iacute;ndice entre 0 y 7 escribiendo &quot;X[0:7]&quot;. Esto es lo mismo que escribir &quot;X[:7]&quot;, dar&aacute; igual resultado, y se arroja los resultados sin incluir el de &iacute;ndice 7:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X = np.arange(30,41)\r\nX</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X[0:7]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([30, 31, 32, 33, 34, 35, 36])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si escribimos: X[:], nos retornar&aacute; todos los miembros de arreglo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X[:]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si queremos que nos arroje todos los elementos que est&aacute;n despu&eacute;s del de &iacute;ndice 2, hacemos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X[2:]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([32, 33, 34, 35, 36, 37, 38, 39, 40])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos a crear una copia del arreglo para hacr algunas modificaciones, como por ejemplo, duplicar cada uno de sus miembros:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">Y = X.copy()*2\r\nY</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora vamos a modificar los valores que tienen &iacute;ndices menores a 5, y los haremos igual a 100</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">Y[0:5] = 100\r\nY</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([1000, 1000, 1000, 1000, 1000,&nbsp;&nbsp; 70,&nbsp;&nbsp; 72,&nbsp;&nbsp; 74,&nbsp;&nbsp; 76,&nbsp;&nbsp; 78,&nbsp;&nbsp; 80])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>La indexaci&oacute;n podemos aplicarla tambi&eacute;n en arreglos de dos dimensiones. Vamos a crear una matriz con 4 filas y 3 columnas as&iacute;:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">array_2d = np.array([[50,52,54], [56,58,60], [62,64,66], [68,70,72]])\r\narray_2d</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[50, 52, 54],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [56, 58, 60],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [62, 64, 66],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [68, 70, 72]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si necesitamos escoger el 64 de la matriz, podemos seleccionarlo escribiendo y recordando que los &iacute;ndices siempre empiezan en 0:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">array_2d[2,1]</code></pre>\r\n\r\n<p>Le pedimos seleccionar de la fila con indice 2 (tercera fila) y de la columna de &iacute;ndice 1 (segunda columna) el valor.</p>\r\n\r\n<p>Si queremos obtener el 64, 66, 70 y 72, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">array_2d[2:,1:]</code></pre>\r\n\r\n<p>Aqu&iacute; indicamos que necesitamos los valores a partir de la tercera fila (2:) y segunda columna (1:)</p>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([[64, 66],<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [70, 72]])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si queremos obtener el 68 y 70, esccribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">array_2d[3,0:2]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">array([68, 70])</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Podemos concluir esta breve introducci&oacute;n a numpy recordando que su uso ser&aacute; de mucha ayuda a la hora de manejar arreglos, vectores y matrices, y los valores en las tablas; y adem&aacute;s, es recomendable seguir practicando y crear algunas matrices n-dimensionales y seleccionar conjuntos, filas y columnas.</p>\r\n\r\n<h5>(Versionado del libro: Machine Learning Guide for Oil and Gas Using Python , de Belyadi, H. 2021)</h5>",
		"imagen_referencial" : "imagenes/numpy_j8izYXk.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-01-06 15:00:39.000000",
		"autor_id" : 1,
		"categoria_id" : 1
	},
	{
		"id" : 15,
		"estado" : 1,
		"fecha_creacion" : "2023-01-08",
		"fecha_modificacion" : "2023-05-02",
		"fecha_eliminacion" : "2023-05-02",
		"titulo" : "Pasos para realizar un proyecto de machine learning (I)",
		"slug" : "pasos-para-realizar-un-proyecto-de-machine-learning",
		"descripcion" : "Antes de adentrarnos en algún proyecto de machine learning, es necesario y muy importante conocer y entender lo que implica los pasos que debemos dar. Un recorrido común para realizar  nuestro proyecto incluye, pero no está limitado: (i) Acopio e integración de los datos; (ii) limpieza de los mismos, que incluye (a) visualizaciónn de datos, (b) detección de valores anómalos e (c) imputación de datos; selección de características importantes (feature); (iv) normalización/estandardización de datos; (v) validación cruzada (cross-validation); (vi) desarrollo del modelo; (vii) búsqueda y selección de hiperparámetros para optimización y finalmente, (viii) entrenamiento e implementación del modelo. Vamos a acercarnos un poco a este recorrido.",
		"contenido" : "<h1>Ruta de trabajo en machine learning</h1>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Recogida e integraci&oacute;n de los datos</h2>\r\n\r\n<p>Las compa&ntilde;ias alrededor del mundo que utilizan el machine learning en su d&iacute;a a d&iacute;a pueden tener diferencias en su flujo de trabajo al aplicarlo, sin embargo, vamos a trazarnos como meta en este post el de presentar una manera de afrontar cualquier problema en ese campo. La recopilaci&oacute;n e integraci&oacute;n de los datos, por ejemplo, es uno de los m&aacute;s importantes pasos en cualquier proyecto de ML. Muchos proyectos simplemente fallan debido a problemas de datos. Por eso, antes de comenzar cualquier proyecto, es importante entender la disponibilidad de datos por parte de la compa&ntilde;&iacute;a. Si lis datos son insuficientes, no est&aacute;n disponibles o son muy dif&iacute;ciles de obtener, es recomendable enfocarnos entonces en proyectos con datos disponibles y que pueden arrojar respuestas tangibles sobre problemas a los que podemos responder por medio del ML. Si la compa&ntilde;&iacute;a es nueva en la aplicaci&oacute;n de de la IA y el ML, se recomienda hacer uso primeramente de casos pr&aacute;cticos, pocos, que prueben el concepto y la aplicabilidad del ML.</p>\r\n\r\n<p>Dado que los datos pueden tener distintas fuentes de origen, es imperativo para la organizaci&oacute;n tener una central de datos que pueda manejarlos; esto le va a permitir a todos en la organizaci&oacute;n hacer uso de los mismos datos en diferentes proyectos. Por ejemplo, en el campo del gas y del petr&oacute;leo, as&iacute; como en otras industrias, se hace uso de hojas de calculo con datos centralizados recopilados y guardados como SQL. Desafortunadamente, debido al ineficiente sistema de almacenamiento, en muchas compa&ntilde;&iacute;as los cient&iacute;ficos de datos pasan el 80% de su tiempo recopilando los datos y solo el 20% analiz&aacute;ndolos. Cuando los datos est&aacute;n guardados en una locaci&oacute;n centralizada, no es necesario buscar en muchas fuentes para obtener los datos requeridos y as&iacute; implementar el an&aacute;lisis para el trabajo de machine learning. Es tal vez un proceso un poco m&aacute;s largo pero que luego va a rendir sus frutos, a&uacute;n para las compa&ntilde;&iacute;as peque&ntilde;as.</p>\r\n\r\n<p>Los datos son ahora, lo dijo hace a&ntilde;os Alvin Toffler, como el oro de anta&ntilde;o y quienes desarrollen el mejor sistema de recogida, almacenamiento y an&aacute;lisis de&nbsp; ellos pueden alcanzar un mayor beneficio econ&oacute;mico.</p>\r\n\r\n<p>Antes de continuar, vamos a comprender qu&eacute; significa Centro de Datos, o data center en ingl&eacute;s. Un Centro de Datos es un edificio, o varios, o un espacio levantado con el fin de, y dedicado exclusivamente a, almacenar, resguardar y organizar, mediante equipos de almacenado y servidores, los datos de una organizaci&oacute;n o empresa. Puede ser &eacute;sta a empresas privadas, quienes crean sus propios espacios de manejo de datos; puede ser un centro de datos en la nube, que se ha vuelto muy com&uacute;n en nuestros d&iacute;as (AWS, Azure, Google Cloud Platform, etc) o puede ser un sistema h&iacute;brido que funcione como una combinaci&oacute;n de las dos anteriores.</p>\r\n\r\n<p>Estos datos generados pueden ser procesados en la nube, por estas empresas especializadas, o pueden ser procesados por cada uno de los que necesiten dichos datos, es lo que se conoce como edge computing, cuando bajamos los datasets a nuestros servidores o a los de las compa&ntilde;&iacute;as que necesiten de ello. Ambos m&eacute;todos tienen sus ventajas respectivas, como que cuando en la nube se realiza e procesamiento, al bajarlas ya tenemos un camino recorrido y un tiempo ganado con respecto al manejo de la informaci&oacute;n; pero tambi&eacute;n existe la ventaja, cuando cada compa&ntilde;&iacute;a procesa la informaci&oacute;n que necesita, y puede desechar la que no, tiene un mayor control sobre la data. En todo caso, el trabajo que debemos hacer, cuando ya tenemos los datos en nuestro poder es:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/15/flecha_abajo_s9k65PI.png\" style=\"height:133px; width:300px\" /></p>\r\n\r\n<h2>Limpiar los datos (Data cleaning)</h2>\r\n\r\n<p>Despu&eacute;s de recopilar e integrar los datos, el siguiente paso es llimpiar nuestro dataset de ruidos, datos no recogidos o datos incopletos que pudieran estar en dicho dataset. Muchos proyectos de ML fallan debido a que los datos presentan valores no asignados, entre otros errores, que impiden que los algoritmos del machine learning puedan ser aplicados. A pesar de que cada proyecto tendr&aacute; sus propias caracter&iacute;sticas, la mayor&iacute;a de ellos requiere de una cuidadosa interpretaci&oacute;n y limpieza. En esta limpieza se pueden se&ntilde;alar los siguentes pasos:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- Visualizaci&oacute;n de los datos: Este es el primer paso para la limpieza de los datos. La mayor&iacute;a de los modelos del ML fallan simplemente porque la limpieza no se ha realizado de la manera m&aacute;s apropiada para empezar su an&aacute;lisis. La revisi&oacute;n, chequeo y control en la calidad de los datos es tal vez la parte m&aacute;s importante en todo an&aacute;lisis de ML. Cuando no se realiza la limpieza del dataset de forma efectiva, pueden f&aacute;cilmente romperse las l&iacute;neas de ejecuci&oacute;n y los resultados casi siempre van a caer en el fracaso. Existen bibliotecas especializadas para la visualizaci&oacute;n de datos, como Matplotlib, Seaborn y Plotly en python, ideales para dicha tarea y que sirven para la detecci&oacute;n visual de forma r&aacute;pida, por ejemplo, de valores fuera de rango. Entender la distribuci&oacute;n de cada caracter&iacute;sticas es adem&aacute;s crucial y es recomendable pra esta tarea, por ejmplo los gr&aacute;ficos de distribuci&oacute;n, las cajas de distribuci&oacute;n (plots box)&nbsp;y los gr&aacute;ficos de puntos (plots scatter).</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- Detecci&oacute;n de valores fuera de rango (Outlier detection): Como se mencion&oacute; antes, la visualizaci&oacute;n de datos gu&iacute;a en la detecci&oacute;n de valores extra&ntilde;os. El uso de los m&eacute;todos ya se&ntilde;alados previamente (box, scatter, etc.) tambi&eacute;n podemos hacer uso de mapas de calor (heat maps), gr&aacute;ficos cruzados (cross plots) que proveen una f&aacute;cil visualizaci&oacute;n de par&aacute;metros que entran en un rango razonable de valores y aquellos que se salen de este patr&oacute;n. Dependiendo de nuestras necesidades, y es importante investigar dichas anormalidades, tambi&eacute;n debemos validar estos valores o podr&iacute;amos eliminarlos para continuar con el pr&oacute;ximo paso en nuestro proyecto. Tambi&eacute;n debemos incluir conceptos b&aacute;sicos de estad&iacute;stica, como pueden ser: frecuencia, promedio, valores m&aacute;ximos y minimos, media, desviaci&oacute;n estandar, entre otras, para obtener un rando final de par&aacute;metros satisfactorio.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- Imputaci&oacute;n de datos (Data imputation): Existen varios m&eacute;todos para manejar los valores perdidos, no asignados de nuestra data:</span>\r\n\t<ol>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Remoci&oacute;n simple de cualquier fila de datos que contengan la nomenclatura de valor N/A (Non assignament). Este es el m&eacute;todo m&aacute;s comunmente usado pero tiene la desventaja de que puede reducir dr&aacute;sticamente el n&uacute;mero de muestras recogidas debido a la eliminaci&oacute;n, en algunos casos, de muchos valores.</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Reemplazo de valores perdidos con los valores de la media y&nbsp;mediana de los valores existentes. Este m&eacute;todo es muy f&aacute;cil de hacer pero no es muy recomendable porque puede rellenar con valores no muy aproximativos los que est&aacute;n faltantes. A veces es m&aacute;s recomendable el m&eacute;todo anteriormente se&ntilde;alado.</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Hacer uso de paquetes especiales, como k vecino m&aacute;s cercano (k-nearest neighbor, KNN) o la imputaci&oacute;n multivariada por ecuaciones encadenadas (Multivariate Imputation by Chained Equations, MICE), para realizar este relleno de datos. Estos m&eacute;todos hacen un gran trabajo solo cuando se cumplen ciertas condiciones: Existe una relaci&oacute;n inherente entre los par&aacute;metros que est&aacute;n perdidos y los par&aacute;metros existentes. Para usar estos paquetes debemos asegurarnos de normalizar o estandarizar los datos.</span></li>\r\n\t</ol>\r\n\t</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/15/flecha_abajo_W80yYZt.png\" style=\"height:133px; width:300px\" /></p>\r\n\r\n<h2>Clasificaci&oacute;n y selecci&oacute;n de caracter&iacute;sticas (Featuring ranking)</h2>\r\n\r\n<p>El siguiente paso importante en el desarrollo de un an&aacute;lisis para el ML es la clasificaci&oacute;n de las caracter&iacute;sticas y su selecci&oacute;n. Aqu&iacute; es donde el dominio de los expertos juega un rol significante en el &eacute;xito del proyecto. Si un cient&iacute;fico de datos no tiene experiencia pr&aacute;ctica en la industria, ser&aacute; muy dif&iacute;cil entender la relaci&oacute;n entre las variables. Este es un caso que muestra la necesidad de combinar el dominio de los expertos y el conocedor de la IA para resolver problemas. El primer paso ser&aacute; indentificar el tipo de problema que un cient&iacute;fico de datos est&aacute; tratando de resolver. Muchas veces es muy importante poner en contacto con el cient&iacute;fico de datos (que conoce el procedimiento de ML),&nbsp; a los expertos en el &aacute;rea (como un ingeniero que conozca el trabajo y los principios de &eacute;ste), para que en conjunto puedan afrontar los retos del proyecto.</p>\r\n\r\n<p>En algunos casos, el experto y el cient&iacute;fico de datos no necesariamente conocen el impacto de cada caracter&iacute;stica y no est&aacute;n seguros de cu&aacute;l deben seleccionar para el proyecto de ML. As&iacute; y todo, es importante desarrollar la clasificaci&oacute;n de las caracter&iacute;sticas para determinar el impacto de cada una en los resultados obtenidos. Existen muchos algoritmos como el random forest (RF), extreme random forest o extra trees (XRF), gradient boost (GB), entre otros, que nos permiten realizar esta tarea de clasificaci&oacute;n.</p>\r\n\r\n<p>La colinealidad tambi&eacute;n es muy importante cuando realizamos una selecci&oacute;n de caracter&iacute;sticas. El coeficiente de correlaci&oacute;n de Pearson en el mapa de calor de las variables de salida se puede utilizar para estudiar la colinialidad de caracter&iacute;sticas. Si las caracter&iacute;sticas son colineales, proveer al modelo con la misma informaci&oacute;n puede potencialmente resultar en confusi&oacute;n. Simplemente eliminamos una de las entradas colineales. Si ambas entradas son importantes para entender, es un aviso para entrenar dos modelos separados con cada una de las caracter&iacute;sticas colineales.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/15/flecha_abajo_QvPqMZo.png\" style=\"height:133px; width:300px\" /></p>\r\n\r\n<h2>Escalamiento, normalizaci&oacute;n o estandarizaci&oacute;n</h2>\r\n\r\n<p>Para asegurarnos de que el algoritmo de aprendizaje no cometa errores con respecto a las magnitudes de los datos, &eacute;stos deben ser escalados. De esa forma la velocidad de optimizaci&oacute;n de los algoritmos aumentar&aacute; al mejorar el calculo del gradiente de descenso que usan los modelos en su desempe&ntilde;o, al tener todas las variables valores con el mismo nivel de significancia o de rango. Debemos tener presente que no todos los algoritmos de ML requerir&aacute;n un escalado de sus caracter&iacute;sticas. La regla general para el mejoramiento es que los algoritmos que exploran distancias o similitudes entre valores de datos, como la red de neuronas aritificial (Artificial neural network - ANN), KNN, vectores (support vector machine - SVM), y k-means clustering, son sensibles a la transformaci&oacute;n en las caracter&iacute;sticas. Por otro lado, algunos algoritmo basados en &aacute;rboles, como los &aacute;rboles de decisiones (decision tree - DT), RF y GB no se ven afectados por el escalamiento de las variables. Esto es porque dichos algoritmos no se basan en c&aacute;lculos de distancia y pueden manejrse variando el rango de las caracter&iacute;sticas, de all&iacute; el porqu&eacute; estos modelos se les dice de &quot;escala invariante&quot; (scale-invariant).</p>\r\n\r\n<p>Las formas principales de escalamiento son:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">* Normalizaci&oacute;n (Feature normalization): Este m&eacute;todo garantiza que el valor de las caracter&iacute;sticas va a ser escalada con un valor de entre 0 y 1. De esa forma se busca evitar que el modelo cometa errores de sesgado al tratar magnitudes de diferentes rangos, y con ello se unifican&nbsp;mediciones. Una de las mayores desventajas de este m&eacute;todo es que la desviaci&oacute;n estandar se vuelve muy peque&ntilde;a cuando tomamos valores entre 0 y 1 que podr&iacute;an resultar en sorpresivas presencias de valores fuera de rango.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">* Estandardizaci&oacute;n (Feature standardization - z-Score normalization): La Estandardizaci&oacute;n transforma cada valor de las caracter&iacute;sticas por medio de la distribuci&oacute;n gaussiana con una media de 0 y una desviaci&oacute;n estandar de 1. La estandardizaci&oacute;n no transforma la estructura de&nbsp;distribuci&oacute;n subyancente de los datos. Algunos algoritmos de aprendizaje, como el SVM, asume que los datos est&aacute;n distribuidos alrededor de 0 con igual varianza.</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora bien, &iquest;cu&aacute;ndo debemos usar la normalizaci&oacute;n o estandardizaci&oacute;n? La respuesta va a depender de nuestra aplicaci&oacute;n. En algoritmos de clustering como k-means clustering, hierarchical clustering, density-base spatial clustering o aplicaciones con ruido (DBSCAN), etc, la estandardizaci&oacute;n es importante debido a la comparaci&oacute;n de caracter&iacute;sticas similares basada sobre mediciones de distancia. Por otro lado, normalizaci&oacute;n se hace importante cuando usamos ciertos algoritmos como ANN y el procesamiento de im&aacute;genes.</p>\r\n\r\n<h2>Validaci&oacute;n cruzada (cross-validation)</h2>\r\n\r\n<p><img alt=\"\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Esquema_castell%C3%A0.jpg/350px-Esquema_castell%C3%A0.jpg\" style=\"height:190px; width:250px\" /></p>\r\n\r\n<p>Antes de aplicar los modelos de ML, debemos realizar una validaci&oacute;n cruzada cuando hacemos uso de algoritmos de aprendizaje supervisado. Esto lo debemos tener en cuanta y la validaci&oacute;n cruzada no es necesaria de hacer cuando se trata de algoritmos de aprendizaje no supervisado, simplemente porque los datos ser&aacute;n usados, en este &uacute;ltimo caso, para agrupamientos (clustering). Existen diferentes tipos de validaci&oacute;n cruzada y son:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- M&eacute;todo Holdout:&nbsp;este es uno de los m&aacute;s simples y consiste en coger una porci&oacute;n de datos del conjunto de datos de entrenamiento, justamente para hacer validaciones de los resultados. Muchas veces los datos se dividen de manera aleatoria en 70% para el entrenamiento y 30% para la validaci&oacute;n o prueba del modelo.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- Validacion por K-fold (K-fold cross-validation): Esta t&eacute;cnica es una extensi&oacute;n del anterior pero los datos van a dividirse en k subconjuntos&nbsp;(folds) y el m&eacute;todo de validaci&oacute;n anterior se realizar&aacute; k veces. Con este m&eacute;todo los datos ser&aacute;n distribuidos y seleccionados de manera aleatoria y luego se dividen en datos de entrenamiento y de prueba. En cada vuelta un subconjunto ser&aacute; tomado como datos de prueba. Ocurre algo as&iacute;: Tenemos un conjunto con 100 filas de datos, el m&eacute;todo tomar&aacute; las primeras 10 filas (si escogemos k como 10) y el resto ser&aacute; para entrenamiento. En la otra vuelta va a tomar las filas desde la d&eacute;cima hasta la vig&eacute;sima (10 - 20) y tomar&aacute; el resto de las filas para entrenamiento, las 10 primeras y desde la fila 20 hasta la 100); en la siguiente vuelta tomar&aacute; las filas desde la 20 hasta la 30 y el resto, desde la 1 a la 20 y desde la 30 hasta la 100, como datos de entrenamiento. Este procedimiento&nbsp; se repite por 10 veces, o hasta el valor de k. Finalmente, la la m&eacute;trica de evaluaci&oacute;n se promedia sobre los 10 valores resultantes. Esto adem&aacute;s hace de todos los&nbsp;valores un valor de prueba una vez y nueve (k-1) veces es un valor de entrenamiento.</span>\r\n\t<ul>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">La principal ventaja de este m&eacute;todo es que reduce el sesgado y la varianza puesto que todos los valores son usados para entrenamiento y prueba. Por otro lado, hay que tener en cuenta que mientras mayor sea el numero de k, el tiempo de c&oacute;mputo se incrementa, por eso se debe hacer un an&aacute;lisis preciso para optimizar este valor.</span></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">- Validaci&oacute;n cruzada por estratificado&nbsp;(Stratified k-fold cross-validation): Cuando existe un gran desbalance en las variables de salida (llamada tambi&eacute;n variables de respuesta) en un&nbsp; modelo de ML, podemos usar una validaci&oacute;n cruzada por estratificado. Por ejemplo, asumamos que el 70% de las respuesta de un modelo de clasificaci&oacute;n es &quot;Continuo&quot;, y 30% es definido como &quot;Evaluaci&oacute;n&quot;. Este es referido como un modelo no equilibrado con un&nbsp;n&uacute;mero de respuestas de clasificaci&oacute;n &nbsp;de 70%-30%. Entonces, una validaci&oacute;n cruzada por estratificacion asegura que cada folio (fold) es representativo de todos los estratos&nbsp;de la&nbsp;data. Esto permitir&aacute; aasegurarnos de que la distribuci&oacute;n&nbsp; de salida es similar para cada estrato de prueba. En otras palabras, la validaci&oacute;n cruzada por k-fold ha variado de manera tal que cada subconjunto contiene aproximadamente el mismo n&uacute;mero de porcentaje de ejemplo para cada valor buscado. Por ejemplo, en una validaci&oacute;n cruzada estratificada para 100K filas de datos, si 30K filas estan marcadas como &quot;Evaluar&quot; y 70K como &quot;Continua&quot; y el valor de k es 10, cada fold tendr&aacute; 3K (3 dividido entre 10) de clases &quot;Evaluar&quot; y 7K de la clase &quot;Continuar&quot;. Este m&eacute;todo no es necesario a menos que exista un desequilibrio en el modelo.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Validaci&oacute;n cruzada Leave-P-out (LPOCV): Este es otro tipo de validaci&oacute;n cruzada en donde cada combinaci&oacute;n posible de los puntos de datos de pruebas P es evaluado. En esta t&eacute;cnica, los puntos de datos P salen de los datos de entrenamiento. Si hay N puntos de datos, N-P puntos de datos son usados para entrenar el modelo y P puntos son usados para validar o probar el modelo. Esto se repite hasta que todas la combinaciones son divididas y probadas. Luego, todas las m&eacute;tricas son promediadas.&nbsp;</span></li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/15/flecha_abajo_ys2BCcC.png\" style=\"height:133px; width:300px\" /></p>\r\n\r\n<h2>Validaci&oacute;n con un conjunto nuevo</h2>\r\n\r\n<p>Adicional a la separaci&oacute;n de los datos en conjunto de entrenamiento y prueba, podemos seleccionar un conjunto de validaci&oacute;n para evaluar la certeza predictiva del modelo desarrollado. Este subconjunto es un porcentaje de los datos que no forma parte ni del conjunto de entrenamiento ni el de prueba. Este permitir&aacute; evaluar los resultados. Lo que debemos hacer es tomar de la data de la que disponemos, un 20% para evaluaci&oacute;n; luego, el restante lo dividimos en un 70%-80% para el entrenamiento y un 30%-20% para las pruebas.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Compensaci&oacute;n sesgo-varianza (Bias-variance trade-off)</h2>\r\n\r\n<p>Tomando de wikipedia el concepto de compensaci&oacute;n sesgo-varianza, se dice que es la propiedad de un modelo de que la varianza del par&aacute;metro estimado entre muestras se puede reducir aumentando el sesgo en los par&aacute;metros estimados. Vamos a definir sesgo (bias) y varianza (variance) para luego adentrarnos un poco m&aacute;s y comprender esta relaci&oacute;n en su conjunto:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\"><strong>Bias</strong> es la incapacidad de un modelo de ML para capturar la verdadera relaci&oacute;n entre los datos de entrada y los valores de salida. Los modelos con un alto valor de sesgado -bias- son incapaces de capturar esa relaci&oacute;n, y usualmente deviene en una simplificaci&oacute;n del modelo. Un ejemplo de un modelo con alto grado de sesgado es cuando una regresi&oacute;n lineal es usada con un conjunto de datos no lineales.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\"><strong>Varianza</strong> refiere a la variabilidad de la predicci&oacute;n del modelo para un deterninado valor del conjunto de datos. Un modelo con una varianza alta implica un buen entrenamiento pero un bajo trabajo de predicci&oacute;n con los datos de prueba, en otras palabras, est&aacute; sobre-entrenado (overfiting) y lo que hace es &quot;memorizar&quot; los datos de entrenamiento pero no puede predecir correctamente debido a su bajo nivel de generalizaci&oacute;n.</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si un modelo es sencillo (lo que significa que no captura mucho de la complejidad en los datos), se dice que es un modelo poco-entrenado (underfit). Un modelo underfit es otro nombre que le damos a los modelos con alto valor de bias y un bajo valor de varianza. Por otro lado, si el modelo es muy complejo y captura todas las implicaciones en los datos de entrenamiento, se le llama modelo sobre-entrenado (overfit); esto significa que tiene un valor de varianza alto y un bajo valor de sesgo (bias). El modelo ideal deber&iacute;a tener un bajo valor de bias y un bajo valor de varianza.</p>\r\n\r\n<p>En la siguiente ocasi&oacute;n continuaremos con los pasos que a&uacute;n faltan para desarrollar nuestro proyecto, asi que espero nos encontremos de nuevo. &iexcl;Saludos!</p>\r\n\r\n<p><img alt=\"laugh\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/teeth_smile.png\" style=\"height:23px; width:23px\" title=\"laugh\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/220px-Bias_and_variance_contributing_to_total_error.svg.png\" style=\"height:138px; width:220px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5><span style=\"font-family:Arial,Helvetica,sans-serif\">(Versionado del libro: Machine learning guide for Oil and Gas using Python, de Belyadi H. 2021)</span></h5>\r\n\r\n<p>&nbsp;</p>",
		"imagen_referencial" : "imagenes/numpy.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-01-08 16:36:21.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 16,
		"estado" : 1,
		"fecha_creacion" : "2023-01-12",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Anaconda y Colaboratory, herramientas para aprender I.A.",
		"slug" : "anaconda-y-colaboratory-herramientas-para-aprender-ia",
		"descripcion" : "Además del interés y la motivación que necesitamos para aprender acerca de la inteligencia artificial, el machine learning o el deep learning, también es necesario contar con un equipo para practicar y escribir nuestro código, y a pesar de que muchas veces vamos a necesitar de uno potente para el entrenamiento de nuestra red neuronal, en el caso del deep learning, es bueno saber que en la nube podemos acceder a herramientas que nos proporcionan no sólo potencia y robustez, sino también ayuda a la hora de practicar y aprender y eso es lo que proporcionan Anaconda y Google Colaboratory.",
		"contenido" : "<h2>Breve introducci&oacute;n a Anaconda</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En nuestro camino de aprendizaje es altamente recomendable bajar e instalar en nuestro equipo Anaconda, una plataforma usada para la ciencia de datos en python, aunque podemos incluir en ella un IDE para trabajar con R. Anaconda viene con las bibliotecas esenciales y necesarias en su instalaci&oacute;n para el abordar los entresijos en el &aacute;rea de la inteligencia artificial. Pandas, Seaborn, Matplotlib, entre otras bibliotecas y m&oacute;dulos ya est&aacute;n en esta plataforma, as&iacute; que no vamos a necesitar descargarlas para escribir nuestros ejercicios y programas.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/07/anaconda1.png\" style=\"height:174px; width:350px\" /></p>\r\n\r\n<h2>&nbsp;</h2>\r\n\r\n<h2>Instalaci&oacute;n</h2>\r\n\r\n<p>Para instalar Anaconda, vamos a su p&aacute;gina principal (www.anaconda.com) y seleccionamos &quot;Get Started&quot;. Luego, seleccionamos &quot;Download Anaconda Installers&quot; y descargamos la &uacute;ltima versi&oacute;n seg&uacute;n nuestro inter&eacute;s, para Windows o Mac. Anaconda viene con m&aacute;s de 250 paquetes, por eso no vamos a necesitar instalarlos al momento de necesitar usarlos, y tambi&eacute;n por eso es muy recomendable su uso, se evita estar descargando e instalando la mayor&iacute;a de las bibliotecas por separado.</p>\r\n\r\n<p>Otras bibliotecas o herramientas que se necesitar&aacute;n, como Tensorflow, Open CV, Pythorch, etc, no est&aacute;n incluidas en esta plataforma, y pueden instalarse f&aacute;cilmente abriendo el comando de Anaconda (Anaconda prompt) desde el menu &quot;start&quot;, escribir luego: <em>pip install</em>, o <em>conda install</em> (nombre de la biblioteca que se quiere instalar), y esto har&aacute; que se ejecute la instalaci&oacute;n de la biblioteca deseada.</p>\r\n\r\n<p>Una vez que Anaconda est&eacute; instalada, buscamos la opci&oacute;n de &quot;Jupyter Notebook&quot; bajo el menu de comienzo. Jupyter Notebook es un entorno interactivo de codificaci&oacute;n basada en la web que nos facilitar&aacute; la tarea en alto grado; su uso es muy amigable e intuitivo y es como un cuaderno de celdas de c&oacute;digo o comentarios con el que vamos a construir nuestros procesos.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/05/jupyternotebook.png\" style=\"height:146px; width:350px\" /></p>\r\n\r\n<p>Adem&aacute;s de Jupyter Notebook, el panel de entrada de Anaconda nos da la oportunidad de agregar otros IDE&#39;s, (como Visual Studio, PyCharm, RStudio), o utilizar Spyder o Jupyter Lab entre otras herramientas que se usan en estos d&iacute;as para el an&aacute;lisis de datos.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/07/spyder.png\" style=\"height:187px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En la pantalla de entrada de Jupyter Notebook busquemos la opci&oacute;n <em>New</em> para crear nuestro primer ejercicio y seleccionamos el lenguaje en el que vamos a estar programando, en nuestro caso <em>Python 3(ipykernel), </em>esto inmediatamente nos abre un cuaderno nuevo (kernel) en el que podemos empezar a escribir nuestro c&oacute;digo<em>:</em></p>\r\n\r\n<p><em><img alt=\"\" src=\"/mediafiles/uploads/2023/02/07/jupyter1.png\" style=\"height:162px; width:350px\" /></em></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Hacer uso de Jupyter Notebook es muy sencillo. Cada celda puede contener varios comandos y para ejecutarlos podemos teclear &quot;SHIFT + ENTER&quot;, o darle click al bot&oacute;n de &quot;RUN&quot;. Para agregar una celda usamos &quot;ALT + ENTER&quot; o seleccionamos en el men&uacute; &quot;Insert -&gt; Cell Below&quot;. Tambi&eacute;n podemos insertar celdas arriba de la que estamos usando, &quot;Insert -&gt; Cell Above&quot;. Para borrar una celda, la seleccionamos y tecleamos dos veces &quot;D&quot; (&quot;DD&quot;). Podemos interrumpir la ejecuci&oacute;n de nuestro c&oacute;digo por medio del comando &quot;Kernel -&gt; Interrupt&quot; o &quot;Kernel -&gt; Restart&quot; para reiniciar la celda. &quot;Kernel -&gt; Restart and Run All&quot; es una opci&oacute;n muy usada cuando necesitamos que todas las celdas se ejecuten de nuestro cuaderno, desde el principio hasta el final, mientras que &quot;SHIFT + ENTER&quot; ejecuta la celda en la que estamos situados. Aqu&iacute; dejamos algunos de los comandos y atajos para usar mientras programamos en Jupyter NoteBook:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Shift+Enter ==&gt; Ejecuta la celda en la que estamos y entra a la siguiente</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Crtl+Enter ==&gt;&nbsp; Ejecuta la celda seleccionada</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Alt+Enter ==&gt; Ejecuta la celda en la que estamos e inserta una nueva</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Ctrl+S ==&gt; Guarda los cambios</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Enter==&gt; Permite editar la celda. Para salir del modo editar usamos &quot;ESC&quot;.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">H ==&gt; Muestra todos los atajos</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Up==&gt; Selecciona la celda de arriba</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Down ==&gt; Selecciona la celda de abajo</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Shift+Up==&gt; Extiende la selecci&oacute;n de celdas hacia arriba</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Shift+Down==&gt; Selecciona las celdas hacia abajo a partir de la que est&aacute; en uso</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">A ==&gt; Inserta una celda arriba</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">B==&gt; Inserta una celda abajo</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">X==&gt; Corta la celda seleccionada</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">C==&gt; Copia la celda seleccionada</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">V==&gt; Pega la celda abajo</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Shift+V ==&gt; Pega la celda arriba</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">DD==&gt; Elimina la celda</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Z==&gt; Deshace la eliminaci&oacute;n</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Ctrl+A ==&gt; Selecciona todas las celdas</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Ctrl+Z==&gt; Comando deshacer</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Es recomendable que usemos un poco de nuestro tiempo en practicar estos comandos para conocerlos y habituarnos a sus usos, de esta forma iremos cogiendo confianza para realizar nuestro trabajo de manera m&aacute;s r&aacute;pida. Jupyter NoteBook ayuda de la mejor manera a que realicemos nuestra tarea con avisos, se&ntilde;ales y ayudas inmediata que obtenemos cuando estamos inmersos en la escritura de c&oacute;digo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Google Colab</h2>\r\n\r\n<p>Otra herramienta que podemos usar en nuestro recorrido por la inteligencia artificial y sus ramas, para aprender, practicar y luego construir nuestras aplicaciones nos la brinda Google, que pone a nuestra disposici&oacute;n Google Colab con solo tener una cuenta gmail. Aqu&iacute; no necesitamos instalar muchas de las bibliotecas, librerias, m&eacute;todos u otra necesidad que tengamos, pues casi todo est&aacute; preinstalado en este entorno que nos brinda Google, y lo que es mejor, el sitio pone en nuestra mano aceleradores de memoria que permiten ejecutar entrenamiento y pr&aacute;ctica de los modelos a mayor velocidad.</p>\r\n\r\n<p>Lo que debemos hacer, luego de entrar a nuestra sesi&oacute;n de Google, es colocar en el buscador la direcci&oacute;n: colab.research.google.com, y veras la siguiente pantalla:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/06/colab.png\" style=\"height:169px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Luego seleccionar la opci&oacute;n de Nuevo notebook en la esquina inferior derecha, para que se cree seguidamente un notebook en donde escribiremos nuestro c&oacute;digo, de manera parecida a como lo presenta Ananconda en Jupyter notebook que vimos anteriormente, y lo mejor es que muchos de los comandos que ya hemos visto de Jupyter, los podemos usar aqu&iacute;.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/06/notebook_colab.png\" style=\"height:158px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos ahora, luego que hemos hecho esto, podemos importar todas las librer&iacute;as que Google Colab pone a nuestra disposici&oacute;n:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/06/colab2.png\" style=\"height:157px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Lo mismo que Jupyter NoteBook, Colab nos presta mucha ayuda a la hora de nuestro trabajo&nbsp; de codificaci&oacute;n, y aunque ambos son muy parecidos, una de las ventajas de Colab es que nos permite hacer uso de aceleradores de procesamiento para cuando estamos usando conjuntos de datos muy grandes. Cuando hacemos uso de redes neuronales para, por ejemplo, procesar im&aacute;genes, Colab brinda una inmensa ayuda al poner a nuestra disposicion de estas herramientas, que hacen que el trabajo rudo se haga en menor tiempo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algunas sugerencias finales</h2>\r\n\r\n<p>Es muy recomendable, ya que vamos a trabajar con grandes cantidades de datos, que nos suscribamos a la plataforma de an&aacute;lisis y manejo de datos llamada Kaggle.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/09/kaggle.png\" style=\"height:176px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En esta p&aacute;gina podemos encontrar cursos, datas, problemas y pr&aacute;cticas que nos sirven de gran ayuda en nuestro recorrido. Otra p&aacute;gina que debemos visitar y suscribirnos es GitHub, en donde nos permiten guardar proyectos, construir portafolios y aprender m&aacute;s acerca de este mundo fascinante que es el manejo de la informaci&oacute;n.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/09/github.png\" style=\"height:176px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para terminar, te invito a que sigas practicando en estos dos entornos de desarrollo para que los conozcas mejor y as&iacute; adentrarnos m&aacute;s en este mundo fascinante que es el de la Inteligencia Artificial. Espero nos veamos en otra oportunidad.&nbsp;</p>\r\n\r\n<p><img alt=\"laugh\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/teeth_smile.png\" style=\"height:23px; width:23px\" title=\"laugh\" /></p>",
		"imagen_referencial" : "imagenes/anaconda2.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-01-12 17:04:28.000000",
		"autor_id" : 1,
		"categoria_id" : 1
	},
	{
		"id" : 17,
		"estado" : 1,
		"fecha_creacion" : "2023-02-03",
		"fecha_modificacion" : "2023-05-07",
		"fecha_eliminacion" : "2023-05-07",
		"titulo" : "Algunos algoritmos de Machine Learning",
		"slug" : "algunos-algoritmos-de-machine-learning",
		"descripcion" : "Los avances en las ciencias y la tecnología mejoran nuestra calidad de vida y el machine learning, parte integral de la inteligencia artificial, está tomando un rol de importancia en nuestra cotidianidad. Rápidamente ha cambiado el mundo moderno, con sus aplicaciones, desarrollos e investigaciones en la industria y la academia.\r\nParte fundamental de este desarrollo, que se espera continúe por un largo tiempo, son los algoritmos, el núcleo central de la tecnología del machine learning. Por medio de ellos es que los equipos pueden aprender y desempeñar las tareas que se espera sin la intervención de la mano humana.\r\nHoy vamos a conocer algunos de los algoritmos que hacen el trabajo \"sucio\" en la práctica del machine learning",
		"contenido" : "<p>Un algoritmo, como sabemos, es un conjunto de instrucciones que tienen como finalidad la resoluci&oacute;n de un problema, el desempe&ntilde;o de una actividad, el comportamiento razonado y met&oacute;dico de un sistema para alcanzar una meta. Sin saberlo hacemos uso de algoritmos para realizar una tarea o para planificar nuestro d&iacute;a a fin de que las sorpresas de la vida nos resulten menos &quot;sorpresivas&quot;. Muchas de las instrucciones que forjamos tienen esto como fin; y como una computadora no es m&aacute;s que una herramienta que programamos para realizar tareas, pues ella necesita de algoritmos que la gu&iacute;en y le muestren el recorrido que debe seguir para esto.</p>\r\n\r\n<p>As&iacute; como nosotros necesitamos de ellos a lo largo de nuestra vida para desenvolvernos, el &aacute;rea de la inteligencia artificial necesita de muchos algoritmos; si bien en estas l&iacute;neas no los vamos a barcar todos, haremos un recorrido por los principales y m&aacute;s conocidos que se usan en el campo del machine learning, tanto de los que se usan en el paradigma del aprendizaje supervisado como en el del aprendizaje no supervisado.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algoritmos para el aprendizaje supervisado</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En este m&eacute;todo, para obtener las respuestas a un conjunto de nuevos datos introducidos por el usuario, el&nbsp; modelo debe ser entrenado para predecir resultados usando datos previamente introducidos, para un proceso de entrenamiento y prueba, a manera de aprendizaje y este modelo, que ya ha aprendido, debe ser capaz de predecir con los nuevos datos. es decir, el sistema aprende a predecir por medio de la experiencia pasada. El conjunto de datos para el entrenamiento consiste, en este caso, de valores y resultado, u objetivo (en ingl&eacute;s, target).</p>\r\n\r\n<p>Existen dos t&eacute;cnicas en el aprendizaje supervisado y cada t&eacute;cnica depende del tipo de datos con que se alimenta el modelo:</p>\r\n\r\n<ol>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Regresi&oacute;n</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Clasificaci&oacute;n</span></li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Regresi&oacute;n</h2>\r\n\r\n<p>En una regresi&oacute;n, un valor num&eacute;rico o variable continua es predicha basada en la relaci&oacute;n entre predictores (variables de entrada) y variables de salida. Como ejemplo podriamos se&ntilde;alar c&oacute;mo predecir el precio de una cada basado en precios de casas de los alrededores, distrito escolar, &aacute;rea total, n&uacute;mero de cuartos, localidad y rango de la criminalidad de la zona.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Clasificaci&oacute;n</h2>\r\n\r\n<p>En la clasifiaci&oacute;n, por otro lado, una variable categ&oacute;rica es predicha, es decir, los datos de entrada pueden ser categorizados basados en etiquetas. Clasificar un correo y diferenciarlo de un spam es un problema de este tipo.</p>\r\n\r\n<p>Para resumir, la t&eacute;cnica de regresi&oacute;n es usada cuando predecimos cuantificaciones, y la t&eacute;cnica de clasificaci&oacute;n es usada cuando predecimos un objeto etiquetado.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algoritmos en el aprendizaje supervisado</h2>\r\n\r\n<p>Los principales son:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Regresi&oacute;n lineal (Linear regression)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Regresi&oacute;n log&iacute;stica (Logistic regression)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Support vector machines (SVM)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; N&auml;ive Bayes</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Arbol de decisiones (Decision trees)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Random Forest</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Artificial neural networks</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; K-nearest neighbors (KNN)</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Una muestra simple</h2>\r\n\r\n<p>Tomemos por esta vez el algoritmo de clasificaci&oacute;n Na&iuml;ve Bayes y veamos como funciona, en futuras pr&aacute;cticas aprenderemos a usar otros y a conocerlo mejor.</p>\r\n\r\n<p>Se llama na&iuml;ve (ingenuo) porque se asume que la ocurrencia de cierta propiedad es independiente de la ocurrencia de otra. As&iacute;, como si las frutas pudieran ser identificadas sobre la base del color, forma y gusto, entonces una fruta amarilla, esf&eacute;rica y dulce es reconocida como naranja. Aqu&iacute;, cada caracter&iacute;stica individual contribuye a identificar que es una naranja con indepencia de otra. Tambi&eacute;n se llama as&iacute; porque se rige por el principio del teorema de Bayes:</p>\r\n\r\n<ul>\r\n\t<li>*&nbsp; <span style=\"font-family:Arial,Helvetica,sans-serif\">&nbsp; Este teorema es tambi&eacute;n conocido como la regla de Bayes o la ley de Bayes, que es usada para determinar la probabilidad de una hip&oacute;tesis con un conocimiento a priori. Depende de la probabilidad condicional.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Su f&oacute;rmula es:</span>\r\n\t<ul>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">P(A|B) = P(B|A) P(A) / P(B)</span></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">Donde:</span></p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">P(A|B) es la probabilidad de que ocurra A observando los eventos B.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">P(B|A) es la probabilidad de que A resulte verdadera.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">P(A) es la probabilidad que ocurra A.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">P(B) es la probabilidad de que ocurra B.&nbsp;&nbsp;</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Usando el teorema de Bayes es posible construir un sistema de aprendizaje que prediga las probabilidades de que la respuesta pertenezca a alguna clase, dado un nuevo conjunto de atributos.</p>\r\n\r\n<p>Un modelo <span style=\"font-family:Arial,Helvetica,sans-serif\">N&auml;ive Bayes comprende dos tipos de probabilidades que pueden ser calculadas directamente de los datos de entrenamiento:</span></p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*</span><span style=\"font-family:Arial,Helvetica,sans-serif\">&nbsp;&nbsp;&nbsp;&nbsp;</span>La probabilidad de cada clase.</li>\r\n\t<li>*&nbsp; &nbsp; La probabilidad condicional para cada clase dado cada valor de entrada.</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En futuros escritos haremos uso de este algoritmo y lo conoceremos un poco mejor, pero por el momento vamos a ver c&oacute;mo funciona:</p>\r\n\r\n<ol>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Calcula la probabilidad previa para las etiquetas dadas.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Clacula la probabilidad condicional con cada uno de los atributos para cada clase.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Multiplica la misma probabilidad condicional.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Multiplica la probabilidad previa con la probabilidad obtenida.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Determina cual clase tiene la m&aacute;s alta probabilidad</span></li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Veamos un caso ejemplo en Python:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.naive_bayes import  GaussianNB\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n# Creamos el modelo\r\nmodelo = GaussianNB()\r\n\r\n# Entrenamos el modelo con los datos para ello\r\nmodelo.fit(train_x, train_y)\r\n\r\n# Se hacen las predicciones con los datos de prueba\r\npredict_test = modelo.predict(test_x)\r\nprint('Resultado en datos de prueba', predict_test)\r\n\r\n# Obtenemos el valor de certeza\r\ncerteza = accuracy_score(test_y, predict_test)\r\nprint('Certeza de prueba: ', certeza)</code></pre>\r\n\r\n<h2>&nbsp;</h2>\r\n\r\n<h2>Algoritmos para el aprendizaje no supervisado</h2>\r\n\r\n<p>Este m&eacute;todo no implica entrenamiento de modelos sobre la base de datos para tal fin, no existe un &quot;supervisor&quot; o &quot;datos de ense&ntilde;anza&quot; que presten su ayuda en el aprendizaje de los modelos. El modelo aprender&aacute; y predecir&aacute; los resultados por s&iacute; mismo sobre la base de sus propias observaciones, sin que tengamos que introducirle datos de entrenamiento.</p>\r\n\r\n<p>Tomemos el ejemplo de una cesta de manzanas y bananas que no est&aacute;n etiquetadas y que no tienen ninguna especificaciones. El modelo solo aprender&aacute; y organizar&aacute; la informaci&oacute;n comparando color, tama&ntilde;o y forma. Esto lo logra observando caracter&iacute;sticas espec&iacute;ficas y similitudes entre dichas caracter&iacute;sticas.</p>\r\n\r\n<p>Las t&eacute;cnicas y modelos usados en el aprendizaje no supervisado implican:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp;&nbsp;&nbsp;&nbsp;Clustering (Agrupamiento)</span>\r\n\r\n\t<ul>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Es el m&eacute;todo de dividir o agrupar los datos en porciones sobre la base de similitudes observables entre estas. Se hace una exploraci&oacute;n de la data y se separan en subconjuntos.</span></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Dimensionality reduction (Reducci&oacute;n de dimensionalidad)</span>\r\n\t<ul>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Si un dataset tiene muchas caracter&iacute;sticas o variables, se hace un proceso de segregaci&oacute;n de la data m&aacute;s compleja. Para resolver escenarios muy complejos se utiliza esta t&eacute;cnica. La idea b&aacute;sica es reducir el n&uacute;mero de variables en el dataset sin perder su clasificaci&oacute;n b&aacute;sica o capacidad de predicci&oacute;n. La clasificaci&oacute;n de im&aacute;genes puede considerarse el mejor ejemplo en donde se usa frecuentemente esta t&eacute;cnica, y donde se toman como claves focales atributos como ojos, nariz y forma de la cara.</span></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Anomaly detection (Detecci&oacute;n de anormalidades)</span>\r\n\t<ul>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">La anomal&iacute;a es un caso que se sale de la norma, es decir, un evento que no entra dentro de los patrones del resto de los casos; es el caso de los fraudes en los bancos, por ejemplo, que salen fuera de las transacciones regulares. La detecci&oacute;n de anomal&iacute;as es identificar aquellos asuntos, por medio de eventos u observaciones que despierta sospechas por diferir significativamente de la mayor&iacute;a de los datos. Otros ejemplos de uso podr&iacute;an ser la identificaci&oacute;n de defectos estructrales en la manufacturaci&oacute;n y problemas m&eacute;dicos como la detecci&oacute;n del c&aacute;ncer.</span></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><span style=\"font-family:Arial,Helvetica,sans-serif\">Algoritmos que usan el aprendizaje no supervizado en machine learning</span></h2>\r\n\r\n<p>Algunos de los m&aacute;s comunes son:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; K-means clustering</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Hierarchical clustering</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; DBSCAN</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Autoencoders</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Hebbian learning</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Deep belief nets</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Self-organizing map</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Algunas consideraciones al escoger un algoritmo</h2>\r\n\r\n<p>El machine learning es adem&aacute;s de una ciencia, un arte. Al buscar un algoritmo de machine learning no existe una sola respuesta o proximaci&oacute;n que nos ayude totalmente. Algunos problemas son muy espec&iacute;ficos y requieren de una &uacute;nica aproximaci&oacute;n, como por ejemplo, si estamos buscando un sistema de recomendaciones, es un problema muy com&uacute;n y algunas veces un algoritmo espec&iacute;fico soluciona ese tipo de problemas. En cambio, existen problemas que est&aacute;n abiertos y necesitan un tratamiento de prueba y error, en los cuales puede usarse la detecci&oacute;n de anomal&iacute;as o construir modelos predictivos generales.&nbsp;</p>\r\n\r\n<p>Con los avances del ML, existen algunos algoritmos competentes de donde escoger dependiendo de nuestras necesidades, algunos m&aacute;s pr&aacute;cticos que otros en cuanto a exito en resultados de negocios. Los factores que pueden afectar en la elecci&oacute;n podr&iacute;an ser:</p>\r\n\r\n<ol>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Tipo de problema: Es obvio que el algoritmo est&aacute; dise&ntilde;ado para resolver un tipo de problema, por eso es importante conocer a que tipo de problema nos estamos enfrentando y qu&eacute; tipo de algoritmo nos vendr&iacute;a mejor para tal problema. Al m&aacute;s alto nivel los algoritmos de ML se clasifican en aprendizaje supervisado y no supervisado, como vimos antes.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Entender la naturaleza de los datos: Para el &eacute;xito del an&aacute;lisis de grandes cantidades de datos, escoger el conjunto de datos es muy importante. El tipo de data juega un rol decisivo en la escogencia del algoritmo a usar. Algunos algoritmos pueden trabajar con conjuntos peque&ntilde;os de muestras mientras otros requieren un gran n&uacute;mero de &eacute;stas. Por ejemplo, Na&iuml;ve Bayes trabaja bien con datos de entrada categoriales pero no es sensible a datos perdidos. Por eso es importante entender la naturaleza de los datos. Diferentes algoritmos pueden tener diferentes requerimientos de variables. El tiempo que pasamos en extraer los datos y seleccionar las variables generalmente requieren una gran cantidad de tiempo para desarrollar nuestros proyectos.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Tama&ntilde;o del conjunto de datos para entrenamiento: Este es uno de las factores m&aacute;s importantes a la hora de seleccionar nuestro algoritmo. Para clasificaciones con&nbsp;peque&ntilde;os conjuntos de datos de entrenamiento, alto sesgo(bias)/baja varianza (Na&iuml;ve Bayes) tiene ventaja sobre clasificaciones con bajo sesgo/alta varianza (KNN). En tanto clasificaciones con&nbsp;bajo sesgo(bias)/alta varianza dan mejor resultados cuando el conjunto crece, porque tienen un error asint&oacute;tico m&aacute;s bajo,&nbsp;ya que los clasificadores de alto sesgo no son lo suficientemente poderosos para proporcionar modelos precisos.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Certeza: Dependiendo de la aplicaci&oacute;n los requerimientos de certeza ser&aacute;n diferente.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Tiempo de entrenamiento: Los algoritmos tienen diferentes tiempos de ejecuci&oacute;n; el tiempo de entrenamiento normalmente est&aacute; en funci&oacute;n del tama&ntilde;o de la data y las exigencias de certeza.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Linealidad: Muchos de los algoritmos de ML como regresi&oacute;n lineal, regresi&oacute;n log&iacute;stica y SVM hacen uso de la linealidad. Debemos tener esto presente.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">N&uacute;mero de par&aacute;metros: Los par&aacute;metros afectan el desenvolvimiento de los algoritmos, como el error de tolerancia o n&uacute;mero de iteraciones. T&iacute;picamente, algoritmos con gran n&uacute;mero de par&aacute;metros requiere m&eacute;todos de prueba y error para encontrar una buena combinaci&oacute;n.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">N&uacute;mero de caracter&iacute;sticas: En algunas data la cantidad de caracteristicas pueden ser muy grandes comparadas con el n&uacute;mero de valores. Esto ocurre en casos como datas de gen&eacute;tica o textuales. Algunos algoritmos como SVM son particularmente adecuados para estos casos.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Entendiendo el sistema de restricciones: Es importante conocer el sistema de almacenamiento de los datos, su capacidad y la velocidad a la que nos transmitir&aacute; las respuestas que necesitamos.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">Encontrar los algoritmos disponibles: Una vez hemos entendido mejor el problema a resolver o tratar y los datos, identificar el algoritmo m&aacute;s acorde a nuestras necesidades es lo que se nos presenta de inmediato, y por ello debemos considerar:</span>\r\n\t<ol>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Si el modelo cumple las metas?</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Cu&aacute;nto de preprocesamiento necesita el modelo?</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Cu&aacute;n acertado es?</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Cu&aacute;n r&aacute;pido es el modelo, cu&aacute;nto tiempo toma construirlo y cu&aacute;nto toma para hacer predicciones?</span></li>\r\n\t\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">&iquest;Cu&aacute;n escalable es el modelo?</span></li>\r\n\t</ol>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><span style=\"font-family:Arial,Helvetica,sans-serif\">Existe un importante criterio a la hora de la esccogencia de cualquier modelo y es su grado de complejidad. Al respecto, un modelo es m&aacute;s complejo si:</span></p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp;&nbsp;&nbsp;&nbsp;Se basa en m&aacute;s caracter&iacute;sticas para aprender y predecir (usar dos caracter&iacute;sticas no es lo mismo que usar diez para predecir un resultado).</span></li>\r\n\t<li>*&nbsp;&nbsp;&nbsp;&nbsp;Se basa en una compleja selecci&oacute;n de caracter&iacute;sticas (por ejemplo, usando t&eacute;rminos polinomiales, interacciones o componentes principales).</li>\r\n\t<li>*&nbsp; &nbsp; Tiene una mayor sobrecarga computacional.</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>De todos modos un mismo algoritmo de aprendizaje puede devenir uno m&aacute;s complejo basado en el n&uacute;mero de par&aacute;metros o en la escogencia y variabilidad de algunos de sus hiperpar&aacute;metros, o de esos par&aacute;metros que pueden variar sus valores luego de realizar el proceso de aprendizaje y que nos permiten hacerlo m&aacute;s eficiente.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En conclusi&oacute;n, podemos decir que la escogencia de nuestro modelo va a depender de muchos factores, pero nuestro aprendizaje y nuestra experiencia, poco a poco nos permitir&aacute; seleccionar el mejor o los mejores a la hora de enfrentarnos a los retos que nos presenten los datos y las tareas que queremos y debemos realizar. Como siempre, es un gusto compartir estos aprendizajes que nos llevan por el camino del machine learning y de la inteligencia artificial. Espero poder encontrarnos otra vez por ac&aacute;.&nbsp;<img alt=\"laugh\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/teeth_smile.png\" style=\"height:23px; width:23px\" title=\"laugh\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5>(Versionado del libro Introduction to machine learning in the cloud with python. Concepts and practice, Gupta, P. &amp; Naresh, Sehgal, 2021)</h5>",
		"imagen_referencial" : "imagenes/post-robot7_6SYpeJ8.jpg",
		"publicado" : 1,
		"fecha_publicacion" : "2023-02-04 01:33:29.000000",
		"autor_id" : 1,
		"categoria_id" : 3
	},
	{
		"id" : 18,
		"estado" : 1,
		"fecha_creacion" : "2023-02-05",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Vamos a cargar imágenes con la API de Keras",
		"slug" : "vamos-a-cargar-imagenes-con-la-api-de-keras",
		"descripcion" : "Hoy tenemos un post práctico y aprenderemos a cargar y visualizar imágenes en nuestro código y para ello haremos uso de una potente herramienta, como lo es Keras, que nos permitirá manipular, visualizar y procesar imágenes para nuestro trabajo de visual-computing.",
		"contenido" : "<p>En la visi&oacute;n asistida por computadora es lo m&aacute;s com&uacute;n, as&iacute; lo exige su naturaleza, trabajar con im&aacute;genes, bien sea fotograf&iacute;as o videos. Esta rama del an&aacute;lisis de im&aacute;genes ha demostrado su eficiencia y su ayuda en la detecci&oacute;n de afecciones m&eacute;dicas, clasificaci&oacute;n de objetos, manejo de veh&iacute;culos aut&oacute;matas, entre otras tareas. Keras, que ya est&aacute; incluida en TensorFlow, es una potente herramienta en el procesamiento de im&aacute;genes y hoy aprenderemos c&oacute;mo cargar, abrir, explorar y visualizar im&aacute;genes, una o varias de ellas, y para esto haremos uso de un data set muy pr&aacute;ctico y famoso con el que vamos a aprender.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Vamos a ello</h2>\r\n\r\n<p>Antes que nada, abriremos un archivo en nuestro Google Colab, y all&iacute; realizaremos la pr&aacute;ctica de hoy. Recuerda que esta es una ayuda de c&oacute;digo abierto que nos proporciona Google para aprender, practicar y hacer nuestras tares en Inteligencia Artificial.</p>\r\n\r\n<p>Como vamos a estar manipulando im&aacute;genes, es necesario que lo primero que hagamos ser&aacute; instalar en nuestro notebook de Colab la biblioteca Pillow, que nos ayudar&aacute; en esta tarea, para ello solo debemos escribir en la celda primera:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">!pip install Pillow</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Looking in indexes:&nbsp;https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/&nbsp;Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)</p>\r\n\r\n<p>Ahora s&iacute;, &iexcl;ya podemos empezar!</p>\r\n\r\n<p>Lo primero que debemos hacer es importar los m&oacute;dulos, bibliotecas (o librer&iacute;as) y funciones que nos van a permitir realizar nuestra pr&aacute;ctica:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import glob\r\nimport os\r\nimport tarfile\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.utils import load_img, img_to_array, get_file</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Los datos los vamos a bajar de la web, y para eso debemos usar algunas variables con los datos pertinentes, como la ruta de acceso, el nombre del archivo, la extensi&oacute;n (los datos est&aacute;n comprimidos y por eso utilizamos tarfile que nos permitir&aacute; descomprimirlos). El nombre del dataset es <em>cinic-10</em>, y es una alternativa al dataset <em>cifar-10</em>, que conoceremos en otra ocasi&oacute;n :</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&amp;isAllowed=y'\r\nDATA_NOMBRE = 'cinic10'\r\nEXTENSION_ARCHIVO = 'tar.gz'\r\nNOMBRE_ARCHIVO = '.'.join([DATA_NOMBRE, EXTENSION_ARCHIVO])</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En Google Colab ocuparemos un lugar en memoria para que almacene la data que estamos bajando y la descomprima:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">locacion_archivo_bajado = get_file(origin=DATASET_URL, fname=NOMBRE_ARCHIVO, extract=False)\r\n\r\n# Construimos la ruta de directorio basado en la locación del archivo bajado\r\ndirectorio_data, _ = locacion_archivo_bajado.rsplit(os.path.sep, maxsplit=1)\r\ndirectorio_data = os.path.sep.join([directorio_data, DATA_NOMBRE])\r\n\r\n# Solo extraemos los datos si no se han extraido ya\r\nif not os.path.exists(directorio_data):\r\n  tar = tarfile.open(locacion_archivo_bajado)\r\n  tar.extractall(directorio_data)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Downloading data from https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&amp;isAllowed=y.</p>\r\n\r\n<p>687544992/687544992 [==============================] - 570s 1us/step</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Dependiendo de la velocidad de nuestra conexi&oacute;n a Internet, tomar&aacute; algunos minutos la bajada del archivo.</p>\r\n\r\n<p>Ahora hacemos una lista que contiene todas las rutas de las im&aacute;genes, que tienen extensi&oacute;n .png y mostramos cu&aacute;ntas im&aacute;genes conforman el dataset:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">patron_datos = os.path.sep.join([directorio_data, '*/*/*.png'])\r\nrutas_imagenes = list(glob.glob(patron_datos))\r\nprint(f'Hay {len(rutas_imagenes):,} imágenes en el dataset')</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Hay 270,000 im&aacute;genes en el dataset</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>En la siguiente celda, escribimos el siguiente c&oacute;digo para conocer algunos aspectos de nuestros datos, tomando como ejemplo una muestra <em>[0]</em>:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Cargamos una imagen del dataset e imprimimos su metadata\r\nimagen_ejemplo = load_img(rutas_imagenes[0])\r\nprint(f'Tipo de imagen: {type(imagen_ejemplo)}')\r\nprint(f'Formato de imagen: {imagen_ejemplo.format}')\r\nprint(f'Modo de imagen: {imagen_ejemplo.mode}')\r\nprint(f'Tamaño de imagen: {imagen_ejemplo.size}')</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Tipo de imagen: &lt;class &#39;PIL.PngImagePlugin.PngImageFile&#39;&gt;</p>\r\n\r\n<p>Formato de imagen: PNG</p>\r\n\r\n<p>Modo de imagen: RGB</p>\r\n\r\n<p>Tama&ntilde;o de imagen: (32, 32)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora, con la ayuda el paquete contenido en&nbsp;<strong>utils</strong> de keras,&nbsp;img_to_array, transformamos esta imagen en un arreglo, que es el lenguaje que puede entender las computadoras para procesarla:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">imagen_ejemplo_arreglo = img_to_array(imagen_ejemplo)\r\nprint(f'Tipo de imagen: {type(imagen_ejemplo_arreglo)}')\r\nprint(f'Forma de imagen en array: {imagen_ejemplo_arreglo.shape}')</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Tipo de imagen: &lt;class &#39;numpy.ndarray&#39;&gt;</p>\r\n\r\n<p>Forma de imagen en array: (32, 32, 3)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Notemos que el arreglo es de tres dimensiones, una imagen de 32 p&iacute;xeles de alto por 32 p&iacute;xeles de ancho y 3 capas.</p>\r\n\r\n<p>Seguidamente vamos a imprimir, por medio de matplotlib, la imagen:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Mostrando la imagen seleccionada usando matplotlib:\r\nplt.imshow(imagen_ejemplo_arreglo/255.0)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/06/ejemplo.png\" style=\"height:273px; width:403px\" /></p>\r\n\r\n<p>Ahora vamos a mostrar un lote de im&aacute;genes de la data haciendo uso del m&oacute;dulo ImageDataGenerator de Keras, para ello instanciamos un generador. Cada imagen debe estar ajustada a una escala de rango entre [0, 1], por esto indicamos el valor de<em> rescale=1.0/255.0</em>:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Cargando un lote de imágenes usando ImageDataGenerator\r\ngenerador_imagen=ImageDataGenerator(horizontal_flip=True, rescale=1.0/255.0)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ya que tenemos creado el generador, lo usamos para imprimir un lote de 10 ejemplos de im&aacute;genes escogiendolas directamente desde el directorio donde est&aacute;n guardadas, creando un iterador para usarlo en nuestro bucle for:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">iterador=(generador_imagen.flow_from_directory(directory=directorio_data, batch_size=10))\r\n\r\nfor batch, _ in iterador:\r\n  plt.figure(figsize=(5,5))\r\n  for index, image in enumerate(batch, start=1):\r\n    ax=plt.subplot(5,5, index)\r\n    plt.imshow(image)\r\n    plt.axis('off')\r\n  plt.show()\r\n  break</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/06/ejemplo2.png\" style=\"height:143px; width:375px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Y as&iacute;&nbsp; hemos impreso en pantalla una muestra de 10 im&aacute;genes de nuestro dataset y hemos aprendido a utilizar la poderosa API Keras para mostrar y trabajar im&aacute;genes, cosa que nos ser&aacute; muy &uacute;til en nuestro recorrido de trabajo de visi&oacute;n por computadora.</p>\r\n\r\n<p>Como siempre, gracias por acompa&ntilde;arnos en este camino de aprendizaje y esperamos seguir en contacto. Hasta otra nueva oportunidad.<img alt=\"wink\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/wink_smile.png\" style=\"height:23px; width:23px\" title=\"wink\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5>(Versionado del libro: TensorFlow 2.0 Computer Vision Cookbook, de J. Mart&iacute;nez, 2021)</h5>\r\n\r\n<p>&nbsp;</p>",
		"imagen_referencial" : "imagenes/keras.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-02-05 19:24:03.000000",
		"autor_id" : 1,
		"categoria_id" : 2
	},
	{
		"id" : 19,
		"estado" : 1,
		"fecha_creacion" : "2023-02-09",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Aprendamos a guardar y cargar un modelo ya entrenado con TF",
		"slug" : "aprendamos-a-guardar-y-cargar-un-modelo-ya-entrenado-contf",
		"descripcion" : "Entrenar una red neuronal implica no sólo tiempo, sino un duro trabajo que consume fuerzas y decisiones, y muchas veces después de un arduo trabajo de reflexión, diseño y entrenamiento de nuestros modelos, debemos resguardar esa información para no repetir todo el procedo de nuevo. Hoy vamos a realiazar un trabajo práctico que nos permitirá aprender a guardar nuestros modelos despues de haberlos entrenado y de esa manera, al necesitarlo nuevamente, ya tenemos medio camino andado.",
		"contenido" : "<p>Dise&ntilde;ar una red neuronal no es una tarea sencilla ni mucho menos es un trabajo de un solo d&iacute;a. Toma su tiempo escoger los pasos necesarios, los factores para mejorar nuestro dise&ntilde;o, cubrir las necesidades exigidas y reducir los costos que implican los retos a los que nos enfrentamos. Por ello, reentrenar un modelo cada vez que lo necesitemos es una pr&aacute;ctica inconcebible.</p>\r\n\r\n<p>La buena noticia es que TensorFlow nos permite, por medio de Keras, guardar nuestra red neuronal una vez entrenada para tenerla lista y dispuesta a usarla cada vez que la necesitemos. En este post vamos a aprender como hacerlo, as&iacute; que... &iexcl;empecemos!</p>\r\n\r\n<p>Lo primero que vamos hacer es abrir un nuevo cuaderno en Google Colab, recuerda que solo es necesario tener una cuenta gmail.</p>\r\n\r\n<p>Esta vez vamos a dise&ntilde;ar una simple red convolucional (CNN- Convolutional Neural Network) que nos permitir&aacute; trabajar sobre la data <strong>mnist</strong>, incluida en Keras, solo para ilustrar nuestro objetivo pr&aacute;ctico.</p>\r\n\r\n<p>Ahora s&iacute;, importamos los paquetes, bibliotecas (librer&iacute;as) y m&oacute;dulos necesarios:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import json\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelBinarizer\r\nfrom keras import Model\r\nfrom keras.datasets import mnist\r\nfrom keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D, ReLU, Softmax\r\nfrom keras.models import load_model, model_from_json</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para descargar los datos y prepararlos para el proceso, definimos una funci&oacute;n que los normalice y codifique las etiquetas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">def cargar_data():\r\n  (X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n  #Normalizamos la data.\r\n  X_train = X_train.astype('float32')/255.0\r\n  X_test = X_test.astype('float32')/255.0\r\n  # Reformamos a escala de grises para incluir otra dimensión.\r\n  X_train = np.expand_dims(X_train, axis=3)\r\n  X_test = np.expand_dims(X_test, axis=3)\r\n  # Procesamos las etiquetas, codificándolas\r\n  label_binarizer = LabelBinarizer()\r\n  y_train = label_binarizer.fit_transform(y_train)\r\n  y_test = label_binarizer.fit_transform(y_test)\r\n  return X_train, X_test, y_train, y_test</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Creamos una funci&oacute;n para construir la red neuronal conformada por una simple capa convolucional y dos capas densamente conectadas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from keras.backend import dropout\r\n# Creamos una función para construir la red neuronal con una capa convolucional y dos capas completamente conectadas\r\ndef contruir_red():\r\n  capa_entrada = Input(shape=(28,28,1))\r\n  convolucional_1 = Conv2D(kernel_size=(2,2),\r\n                           padding='same',\r\n                           strides=(2,2),\r\n                           filters=32)(capa_entrada)\r\n  activacion_1 = ReLU()(convolucional_1)\r\n  normalizacion_lote_1 = BatchNormalization()(activacion_1)\r\n  pooling_1 = MaxPooling2D(pool_size=(2,2),\r\n                           strides=(1,1),\r\n                           padding='same')(normalizacion_lote_1)\r\n  dropout = Dropout(rate=0.5)(pooling_1)\r\n  aplanada = Flatten()(dropout)\r\n  densa_1 = Dense(units=128)(aplanada)\r\n  activacion_2 = ReLU()(densa_1)\r\n  densa_2 = Dense(units=10)(activacion_2)\r\n  salida = Softmax()(densa_2)\r\n  red_neuronal = Model(inputs=capa_entrada, outputs=salida)\r\n  return red_neuronal</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora definimos una funci&oacute;n que evaluar&aacute; el modelo usando los datos de prueba:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">def evaluar(modelo, X_test, y_test):\r\n  _, certeza = modelo.evaluate(X_test, y_test, verbose=0)\r\n  print (f'Certeza: {certeza}')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Preparamos los datos, creamos los subconjuntos de entrenamiento y prueba e instanciamos lo red neuronal:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X_train, X_test, y_train, y_test = cargar_data()\r\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)\r\n\r\nmodelo = contruir_red()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Compilamos y entrenamos el modelo para 50 epochs, con un tama&ntilde;o de lote de 1024. Estos valores se pueden modificar de acuerdo a la capacidad de nuestra compuatdora:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo.compile(loss='categorical_crossentropy',\r\n               optimizer='adam',\r\n               metrics=['accuracy'])\r\nmodelo.fit(X_train, y_train,\r\n           validation_data=(X_valid, y_valid),\r\n           epochs=50,\r\n           batch_size=1024,\r\n           verbose=0)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\n&lt;keras.callbacks.History at 0x7f4a78e667c0&gt;</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora guardamos nuestro modelo entrenado con todos los valores, que es la finalidad de esta pr&aacute;ctica y&nbsp;lo hacemos en formato HDF5. De es forma quedar&aacute; disponible para posteriores usos. Aunque hay otras maneras de obtener el mismo resultado, esta es la m&aacute;s comunmente usada porque podemos simplemente restaurar la red neuronal con la funci&oacute;n <strong>load_model()</strong>:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Guardando el modelo con su valores en HDF5\r\nmodelo.save('modelo_valores.hdf5')\r\n\r\n# Ahora cargamos el modelo y evaluamos su desempeño\r\nmodelo_cargado = load_model('modelo_valores.hdf5')\r\nevaluar(modelo_cargado, X_test, y_test)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Certeza: 0.9832000136375427</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como vemos, este modelo tiene un desempe&ntilde;o con 98.32% de certezas.</p>\r\n\r\n<p>Tambi&eacute;n se puede guardar el modelo separadamente de sus valores de peso usando las funciones <strong>to_json()</strong> y <strong>save_weights()</strong>, respectivamente. La ventaja de esta opci&oacute;n es que podemos copiar un modelo con la misma arquitectura desde cero usando la funci&oacute;n <strong>model_from_json()</strong>. La desventaja, sin embargo, es que necesitar&iacute;amos m&aacute;s funciones y este esfuerzo muchas veces no vale la pena.</p>\r\n\r\n<p>Y lo que es m&aacute;s importante, est&aacute; a nuestra disposici&oacute;n cada vez que lo necesitemos.</p>\r\n\r\n<p>Y eso es toda nuestra pr&aacute;ctica por ahora. Espero te haya servido y te sirva en el futuro, porque a fin de cuentas es lo importante. &iexcl;Nos vemos en otra ocasi&oacute;n!</p>\r\n\r\n<p><img alt=\"smiley\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/regular_smile.png\" style=\"height:23px; width:23px\" title=\"smiley\" /></p>\r\n\r\n<h5><span style=\"font-family:Arial,Helvetica,sans-serif\">(Versionado del libro: Tensorflow 2.0 Computer Vision Cookbook, de Martinez, J. 2021)</span></h5>",
		"imagen_referencial" : "imagenes/guacamayas.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-02-09 14:23:36.000000",
		"autor_id" : 1,
		"categoria_id" : 2
	},
	{
		"id" : 20,
		"estado" : 1,
		"fecha_creacion" : "2023-02-10",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "Construyendo una red neuronal convolucional",
		"slug" : "construyendo-una-red-neuronal-convolucional",
		"descripcion" : "El uso de la visión por compuadora ha adquirido mucha importancia, y esta importancia no hará más que crecer, con el paso de los días en los que estamos envueltos. Esta vez te invito a pasearnos por un proyecto donde crearemos una red convolucional, y de esta manera aprenderemos algunos aspectos de esta herramienta que nos ayudarán en nuestra meta.",
		"contenido" : "<h2>Convolutional Neural Networks</h2>\r\n\r\n<p>Antes de sumergirnos en el campo de las redes neuronales para la clasificaci&oacute;n de imagen, hablaremos de algunos conceptos b&aacute;sicos.</p>\r\n\r\n<p>Consideremos la siguiente imagen:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/10/cnn_1.png\" style=\"height:147px; width:220px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si a cualquiera de nosotros se nos pregunta: &iquest;Qu&eacute; representa esa fotograf&iacute;a? Inmediatamente responderemos que a un ave. Esta imagen tiene un tama&ntilde;o aproximado de 275 x 183 p&iacute;xeles y cada pixel tiene el valor de un color en un compuesto RGB (Red, Green, Blue). Si convertimos la misma imagen a una en blanco y negro, veremos:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/10/cnn_2.png\" style=\"height:143px; width:220px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Y de nuevo, si hacemos la misma pregunta de antes, igual responderemos que es un ave. Ahora, si la reescalamos a la medida de 32 x 32 p&iacute;xeles, se nos ofrecer&aacute; as&iacute;:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/10/cnn_3.png\" style=\"height:152px; width:150px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Podemos a&uacute;n visualizar que es la imagen de un ave, pero ya no estaremos tan seguros. Ahora bien, vamos a tratar de entender por qu&eacute; todas estas trasnformaciones cuando antes ten&iacute;amos una linda foto de un ave que pod&iacute;amos apreciar. Si nuestro objetivo es hacer un proyecto de machine learning para interpretar y clasificar objetos en una imagen dada, &iquest;debemos alimentar nuestra computadora con im&aacute;genes de alta resoluci&oacute;n, como la primera que vimos? Si una persona puede clasificar los objetos de una imagen, &iquest;por qu&eacute; debemos reducir su escala para que lo haga nuestro equipo? La raz&oacute;n es simple, una imgen en alta resoluci&oacute;n contiene una gran cantidad de &quot;bits&quot; de informaci&oacute;n que es redundante para nuestro prop&oacute;sito de identificaci&oacute;n. Si alimentamos a nuestra computadora con esa gran cantidad de informaci&oacute;n demandar&iacute;a una mayor demanda en memoria y poder de procesamiento, y por ende, un aumento de gastos. Entonces, para reducir el entrenamiento requerido por una red neuronal y evitar el costo del proceso, usaremos im&aacute;genes reducidas en nuestra data.</p>\r\n\r\n<p>Un gran n&uacute;mero de investigaciones se han hecho en el campo de la clasificaci&oacute;n de im&aacute;genes por computadoras y los ingenieros de software han dise&ntilde;ado exit&oacute;samente arquitecturas ANN para lograr un gran nivel de certeza. Este proceso de transformaci&oacute;n de im&aacute;genes descrito se llama convoluci&oacute;n, aunque explicar en detalles el proceso se har&iacute;a muy largo, aqu&iacute; te dejo un video por si quieres saber c&oacute;mo es el proceso:</p>\r\n\r\n<p>Los investigadores han designado estas redes con el nombre de Redes Neuronales Convolucionales (Convolutional Neural Networks - CNN) para resolver el problema de clasificaci&oacute;n y reconocimiento de objetos. Las CNN contienen capas convolucionales seguidas por otras capas de diferentes tipos. Una muestra de arquitectura de CNN t&iacute;pica contiene dos capas convolucionales:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/12/cnn_4.png\" style=\"height:157px; width:400px\" />&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Por fortuna, como hemos visto, <strong>Keras</strong> provee algunas capas ya listas para usar, entre las que incluye convolution, pooling, flatten, etc. Nuestra tarea consistir&aacute; en hacer un ensamblaje apropiado de capas y construir nuestra red, cosa que aprenderemos a continuaci&oacute;n, dise&ntilde;ando 5 diferentes CNN y evaluaremos sus desempe&ntilde;os respectivos.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Clasificando im&aacute;genes con una CNN</h2>\r\n\r\n<p>Ahora aprenderemos como clasificar una imagen dada de acuerdo a un tipo determinado. La tarea de clasificar una imagen para un ser humano es una tarea muy sencilla, pero para una m&aacute;quina no lo es tanto, a menos que podamos ense&ntilde;arle c&oacute;mo hacerlo. Eso es lo que haremos, as&iacute; que vamos a aprender no s&oacute;lo a entrenar nuestra m&aacute;quina para que lo haga, sino que en el proceso, aprenderemos a usar las herramientas que nos da <strong>Keras</strong> para lograr un mejor desempe&ntilde;o.</p>\r\n\r\n<p>Lo primero que haremos ser&aacute; abrir un nuevo cuaderno en Google Colab y le daremos un nombre. Luego importaremos y cargaremos TensorFlow; adem&aacute;s, como vamos a trabajar en una clasificaci&oacute;n de im&aacute;genes haremos uso del acelerador que nos brinda Colab, de esa forma nuestro trabajo ser&aacute; mucho m&aacute;s r&aacute;pido. Antes que nada, en el men&uacute; de nuestro cuaderno iremos a la opci&oacute;n de <strong>Entorno de ejecuci&oacute;n</strong>&nbsp;y escogemos en el desplegable <strong>Cambiar tipo de entorno de ejecuci&oacute;n,</strong>&nbsp; seguidamente, en el cuadro de di&aacute;logo seleccionar la opci&oacute;n de <strong>GPU</strong> en <strong>Acelerador de hardware.</strong>&nbsp;As&iacute; podremos activar el acelerador en nuestro cuaderno usando el siguiente c&oacute;digo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import tensorflow as tf\r\n\r\n# Así haremos uso del acelerador de google GPU\r\ndispositivo_acelerador=tf.config.experimental.list_physical_devices('GPU')\r\nprint('Num GPUs disponible: ', len(dispositivo_acelerador))\r\ntf.config.experimental.set_memory_growth(dispositivo_acelerador[0], True)</code></pre>\r\n\r\n<h5>&nbsp;</h5>\r\n\r\n<p>Una enorme cantidad de datos es la primera necesidad para cualquier proyecto de machine learning, pero para suerte nuestra, ya se encuentran disponibles colecciones de im&aacute;genes que han recopilado un gran n&uacute;mero de ellas y lo mejor, listas para su uso. De estas colecciones haremos uso, para nuestra pr&aacute;ctica, de la realizada por el Instituto Canadiense para el Avance de la Investigaci&oacute;n (Canadian Institute For Advance Research - CIFAR). Este dataset esta disponible para uso p&uacute;blico y para que las personas desarrollen t&eacute;cnicas de recocimiento de im&aacute;genes. Los investigadores del CIFAR han creado dos datasets -CIFAR10 y CIFAR100. Ambos datasets consisten en 60000 im&aacute;genes a color de tama&ntilde;o 32x32 pixeles. El CIFAR10 est&aacute; divido en 10 clases que contienen las categor&iacute;as de gatos, aves, barcos, etc (cats, birds, ships,...). Hay 6000 im&aacute;genes en cada categor&iacute;a y est&aacute;n ordenadas aleatoriamente para su uso en el machine learning. El CIFAR10 est&aacute; tambi&eacute;n dividido apropiadamente en conjuntos de entrenamientos y pruebas; el conjunto de entrenamiento consta de 50000 im&aacute;genes y el de pruebas 10000.</p>\r\n\r\n<p>Mostramos aqu&iacute; una peque&ntilde;a muestra del dataset:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/12/cnn_5.png\" style=\"height:792px; width:400px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Hay que repetir que las im&aacute;genes tienen un tama&ntilde;o de 32x32 pixeles, requerido para que nuestra m&aacute;quina aprenda. No podemos alimentar con im&aacute;genes de tama&ntilde;o variable a nuestro algoritmo. Los datos de entrada a nuestra red neuronal debe tener siempre un tama&ntilde;o predeterminado. Afortudamente, los creadores de CIFAR10 se han tomado la tarea de reducir todas las im&aacute;genes a un tama&ntilde;o fijo. Si tratamos de entrenar nuestro modelo con im&aacute;genes de 512x512 pixeles o m&aacute;s, el n&uacute;mero de pixeles de entrada requeridos ser&iacute;a enorme. Esto implicar&iacute;a un gran n&uacute;mero de &quot;pesos&quot; de entrenamiento y una gran cantidad de esfuerzo y tiempo. Al ser las im&aacute;genes de 32x32 existe obviamente una p&eacute;rdida de datos, a pesar de eso, al entrenar nuestra red con estas condiciones, se obtienen buenos resultados e indentificaciones con objetos que la red no conoce del mundo real.</p>\r\n\r\n<p>El CIFAR100 es similar al CIFAR10, solo que est&aacute;n representadas 100 clases en vez de 10. Cada categor&iacute;a, adem&aacute;s, est&aacute; dividida en subcategor&iacute;as, como por ejemplo, la superclase persona contiene las subclases bebe, chico, chica, hombre, mujer.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Bajar el dataset para el proyecto</h2>\r\n\r\n<p>Keras viene provisto de algunos conjuntos de datos para su uso en el desarrollo de modelos, incluido el CIFAR10. Est&aacute; disponible enn el m&oacute;dulo keras.dataset.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import tensorflow_datasets as tfds\r\nprint(\"Número de datasets: \", len(tfds.list_builders()))\r\n\r\n# Lista los datasets incluidos en keras\r\ntfds.list_builders()</code></pre>\r\n\r\n<h5>&nbsp;</h5>\r\n\r\n<p>Al d&iacute;a de hoy existen 1138 datasets disponibles, n&uacute;mero que seguro seguir&aacute; creciendo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Creando los datos de entrenamiento y prueba</h2>\r\n\r\n<p>Para crear los conjuntos de datos de entrenamiento y prueba, keras.datasets provee de un m&eacute;todo sencillo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar10.load_data()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Luego de cargar los datos, vamos a examinar las cuatro variables que tenemos, cu&aacute;ntas im&aacute;genes tiene cada una, sus dimensiones:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">print(\"Dimensiones de x_train: \", x_train.shape)\r\nprint(\"Dimensiones de x_test: \", x_test.shape)\r\nprint(\"Dimensiones de y_train: \", y_train.shape)\r\nprint(\"Dimensiones de y_test: \", y_test.shape)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nDimensiones de x_train:  (50000, 32, 32, 3)\r\nDimensiones de x_test:  (10000, 32, 32, 3)\r\nDimensiones de y_train:  (50000, 1)\r\nDimensiones de y_test:  (10000, 1)\r\n</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vemos que en el conjunto de entrenamiento existen 50000 im&aacute;genes de 32x32 pixeles y sus tipos (x_train e y_train), adem&aacute;s, cada imagen posee 3 valores RGB, que son las capas de color (Red, Green, Blue). Adem&aacute;s, tenemos 10000 im&aacute;genes para probar nuestros modelos. Los tipos est&aacute;n codificados desde el 0 al 9 (y_train, y_test). Vamos a imprimir un pantalla una imagen de muestra:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import matplotlib.pyplot as plt\r\nplt.imshow(x_train[40])</code></pre>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/12/cnn_6.png\" style=\"height:343px; width:350px\" /></p>\r\n\r\n<p>Esta imagen aunque borrosa de 32x32 pixeles le permite a nuestro algoritmo de machine learning entrenar para hacer predicciones con un buen rango de certeza. Vamos a prepara ahora nuestros datos de entrenamiento para el modelo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Preparando los datos para entrenar el modelo</h2>\r\n\r\n<p>Como hemos dicho ante, usaremos estas im&aacute;genes para entrenar, pero necesitamos reservar una parte de esta data para validaci&oacute;n durante la fase de entrenamiento. Reservaremos el 5% de estos datos para dicha evaluaci&oacute;n.</p>\r\n\r\n<p>Para eso utilizaremos el m&eacute;todo de sklearn que nos permite separar los datos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.model_selection import train_test_split\r\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.05, random_state=0)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Diversificando la data</h2>\r\n\r\n<p>Ahora realizaremos un proceso sobre nuestra data que har&aacute; de cada imagen una nueva versi&oacute;n de ella. Este proceso se conoce en ingl&eacute;s como <strong>Augmentation</strong>, si lo traducimos al espa&ntilde;ol ser&iacute;a aumento, pero esto no implica que aumentar&aacute; el numero de im&aacute;genes, sino que algunas de sus caracter&iacute;sticas cambiar&aacute;n.&nbsp;El &quot;augmento&quot; de la data es una estrategia que utilizan los cient&iacute;ficos de datos para incrmentar la diversidad de data disponible durante el entrenamiento del modelo. Los datos recolectados en el campo suelen presentar una uniformidad que no permite un perfecto entrenamiento. Este proceso de &quot;augmento&quot; de data evita la necesidad de recolectar nuevos datos que brinden diversidad. Algunas de las t&eacute;cnicas usadas para esto incluyen recortes, relleno y voltear horizontalmente las im&aacute;genes a fin de brindar esa diversidad que se necesita. Keras, para hacer todo esto, tiene un m&oacute;dulo de preprocesamiento, que utilizamos de la siguiente manera:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from keras.preprocessing.image import ImageDataGenerator\r\ndatagen = ImageDataGenerator(\r\n    rotation_range=15,\r\n    width_shift_range=0.1,\r\n    height_shift_range=0.1,\r\n    horizontal_flip=True,\r\n)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Aqu&iacute;, lo que acabamos de hacer es llamar el m&oacute;dulo ImageDataGenerator de keras, lo instanciamos y a esta instancia le damos algunos valores para que manipule nuestras im&aacute;genes, aumentando el ancho, altura, rot&aacute;ndolas o gir&aacute;ndolas horizontalmente.</p>\r\n\r\n<p>Ahora vamos a definir una funci&oacute;n que normalice nuestras im&aacute;genes, es decir, las reescale en un rango que est&eacute; entre 0 y 1:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">def normalizar(data):\r\n  data=data.astype(\"float32\")\r\n  data=data/255.0\r\n  return data</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora hagamos la diversificaci&oacute;n y el reescalamiento de nuestros datos de entrenamiento y prueba:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">x_train = normalizar(x_train)\r\ndatagen.fit(x_train)\r\nx_val = normalizar(x_val)\r\ndatagen.fit(x_val)\r\nx_test = normalizar(x_test)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Date cuenta que las im&aacute;genes de prueba s&oacute;lo se reescalan, no se diversifican; esto es porque las pruebas deben acercarse a im&aacute;genes del mundo real.</p>\r\n\r\n<p>Una &uacute;ltima cosa que necesitamos hacer como procesamiento de los datos es llamar al m&eacute;todo <strong>to_categorical</strong> para convertir los valores num&eacute;ricos enteros en una matriz. Notemos que nuestra columna de etiquetas tiene 10 categor&iacute;as perteneciente a los diferentes tipos de im&aacute;genes, como aves, perros, gatos, etc.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">y_train = keras.utils.to_categorical(y_train, 10)\r\ny_test = keras.utils.to_categorical(y_test, 10)\r\ny_val = keras.utils.to_categorical(y_val, 10)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Vamos a imprimir las dimensiones de nuestra data modificada para entender lo que hemos hecho hasta ahora:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">print(\"Dimensiones de x_train: \", x_train.shape)\r\nprint(\"Dimensiones de y_train: \", y_train.shape)\r\nprint(\"Dimensiones de x_test: \", x_test.shape)\r\nprint(\"Dimensiones de y_test: \",y_test.shape)\r\nprint(\"Dimensiones de x_val: \", x_val.shape)\r\nprint(\"Dimensiones de y_val: \", y_val.shape)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nDimensiones de x_train: (47500, 32, 32, 3)\r\nDimensiones de y_train: (47500, 10)\r\nDimensiones de x_test: (10000, 32, 32, 3)\r\nDimensiones de y_test: (10000, 10)\r\nDimensiones de x_val: (2500, 32, 32, 3)\r\nDimensiones de y_val: (2500, 10)</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Podemos notar que las dimensiones de los conjuntos de caracter&iacute;sticas son las mismas, mientras que las de los vectores de respuestas, (y_train, y_test e y_val) ahora cambiaron a 10, debido a que tenemos 10 clases de objetos en nuestra data.</p>\r\n\r\n<p>Ahora vamos a definir el modelo. Primero creamos una funci&oacute;n que entrenar&aacute;, evaluar&aacute; y graficar&aacute; las m&eacute;tricas de error del modelo espec&iacute;fico. La raz&oacute;n para hacer esto es que as&iacute; evaluaremos varias arquitecturas para comparar resultados, de esa manera, podremos descartar y aprender c&oacute;mo var&iacute;an las m&eacute;tricas de acuerdo a nuestros modelos.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Desarrollo del modelo</h2>\r\n\r\n<p>&nbsp;Vamos a construir varios modelos, de arquitecturas distintas, luego veremos c&oacute;mo son sus desempe&ntilde;os y predicciones; el de mejor desempe&ntilde;o lo guardaremos para usarlo con im&aacute;genes desconocidas por &eacute;l.</p>\r\n\r\n<p>Para comparar desempe&ntilde;os, crearemos una funci&oacute;n que usaremos con cada modelo. Dicha funci&oacute;n entrenar&aacute;, evaluar&aacute; y graficar&aacute; resultados de m&eacute;trica y certeza.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Funci&oacute;n de entrenamiento, evaluaci&oacute;n y graficado</p>\r\n\r\n<p>Llamaremos a esta funci&oacute;n <strong>resultados</strong>:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">def resultados(modelo):\r\n  epoch = 20\r\n  r = modelo.fit(x_train, y_train, batch_size=32,\r\n                 epochs=epoch, validation_data=(x_val, y_val), verbose=1)\r\n  acc = modelo.evaluate(x_test, y_test)\r\n  print(\"Pérdida con conjunto de prueba: \", acc[0])\r\n  print(\"Certeza con conjunto de prueba: \", acc[1]*100)\r\n\r\n  # Gráfico de certeza con datos de entrenamiento y validación\r\n  epoch_range = range(1, epoch+1)\r\n  plt.plot(epoch_range, r.history['accuracy'])\r\n  plt.plot(epoch_range, r.history['val_accuracy'])\r\n  plt.title('Certeza de clasificación')\r\n  plt.ylabel('Certeza')\r\n  plt.xlabel('Epoch')\r\n  plt.legend(['Entrenamiento', 'Val'], loc='lower right')\r\n  plt.show()\r\n\r\n  # Grafico de valores de pérdida de entrenamiento y validación\r\n  plt.plot(epoch_range, r.history['loss'])\r\n  plt.plot(epoch_range, r.history['val_loss'])\r\n  plt.title('Pérdida del modelo')\r\n  plt.ylabel('Pérdida')\r\n  plt.xlabel('Epoch')\r\n  plt.legend(['Train', 'Val'], loc='lower right')\r\n  plt.show()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>La funci&oacute;n toma el nombre del modelo por par&aacute;metro, lo entrena, con un n&uacute;mero de iteraciones de 20 (epoch=20), lo eval&uacute;a y grafica su certeza y valores de p&eacute;rdida. Podemos cambiar el n&uacute;mero de epoch de acuerdo a los resultados.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Funci&oacute;n de predicci&oacute;n</h2>\r\n\r\n<p>Como nuestra data est&aacute; compuesta de 10 clases (tipos), primero definiremos los nombres de estas clases. Luego crearemos la funci&oacute;n que har&aacute; la predicci&oacute;n y &eacute;sta tomar&aacute; dos par&aacute;metros, el nombre del archivo de la imagen y el modelo usado para clasificar dicha imagen. En esta funci&oacute;n cargamos la imagen en memoria con el m&eacute;todo<strong> load_img</strong> de <strong>keras</strong>, luego se imprime dicha imagen. A continuaci&oacute;n a la imagen debemos hacerle todas las transformaciones que le hicimos antes a las im&aacute;genes utilizadas, recordemos que esta funci&oacute;n tomar&aacute; una imagen no usada previamente.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from keras.utils import img_to_array, load_img\r\n\r\ndef predecir_clase(nombrearchivo, modelo):\r\n  img = load_img(nombrearchivo, target_size=(32, 32))\r\n  plt.imshow(img)\r\n\r\n  # Convertimos la imagen en un arreglo\r\n  # y la reformamos a una con 3 canales\r\n  img = img_to_array(img)\r\n  img = img.reshape(1, 32, 32, 3)\r\n  # normalizamos la imagen\r\n  img = img.astype('float32')\r\n  img = img/255.0\r\n\r\n  # Se hace la predicción\r\n  resultado = modelo.predict(img)\r\n\r\n\r\n  \"\"\"\r\n  La salida que da la función predict tiene forma de matriz.\r\n  Copiaremos esta salida en un diccionario. La predicción es el key, la clase será el value.\r\n  \"\"\"\r\n\r\n  dicc2 = {}\r\n  for i in range(10):\r\n    dicc2[resultado[0][i]]=clases[i]\r\n\r\n  res = resultado[0]\r\n  res.sort()\r\n  res = res[::-1]\r\n  resultados = res[:3]\r\n\r\n  print(\"Las 3 predicciones más acertadas para la imagen son:\")\r\n  for i in range(3):\r\n    print(\"{} : {}\".format(dicc2[resultados[i]],\r\n                           (resultados[i]*100).round(2)))\r\n    \r\n  print(\"La imagen dada es\")</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Definiendo los modelos</h2>\r\n\r\n<p>Ahora crearemos varios modelos con un nivel de complejidad que ir&aacute; en aumento. Luego de definir cada modelo, lo entrenamos pasando a la funci&oacute;n y evaluaremos su desempe&ntilde;o. Empecemos con el primero:</p>\r\n\r\n<p>PRIMER MODELO: CON DOS CAPAS CONVOLUCIONALES</p>\r\n\r\n<p>Usaremos la API de Keras, Sequential, para armar la arquitectura del modelo. Este primer modelo constar&aacute; de 6 capas, la primera de las cuales ser&aacute; de tipo Conv2D (nuestra capa convolucional), y est&aacute; formada por 32 filtros de tama&ntilde;o 3x3, que recorre sobre las im&aacute;genes de 32x32,3. La entrada a la capa, recordemos, tiene las medidas de cada imagen (32,32,3). La segunda capa tambi&eacute;n es convolucional. La tercera es una MaxPooling2D seguida de una capa plana (Flatten) y luego dos densamente conectadas (Dense), de 128 y 10 nodos respectivamente. La primera de &eacute;stas tiene como funci&oacute;n de activaci&oacute;n ReLU, mientras que la otra tiene como funci&oacute;n de activaci&oacute;n softmax. Esta capa Dense tiene 10 nodos precisamente porque es la &uacute;ltima y son 10 las clases de nuestra data:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\r\n\r\nmodelo_1 = Sequential([\r\n    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\r\n    Conv2D(32, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10, activation='softmax')\r\n])</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Despu&eacute;s de definir el modelo, lo compilamos y seguidamente lo entrenamos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">opt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\r\nmodelo_1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>\r\n\r\n<pre>\r\n<code class=\"language-python\">resultados(modelo_1)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Luego de varios minutos que dura el entrenamiento se dan los valores resultantes y los gr&aacute;ficos de certeza y p&eacute;rdida:</p>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nP&eacute;rdida con conjunto de prueba: 1.6913135051727295\r\nCerteza con conjunto de prueba: 64.56000208854675</pre>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/13/cnn_7.png\" style=\"height:465px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como vemos, la certeza es aproximadamente de 65% (este valor puede variar en tu pr&aacute;ctica pero es normal)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>SEGUNDO MODELO: CON 4 CAPAS CONVOLUCIONALES</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_2 = Sequential([\r\n    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\r\n    Conv2D(32, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10, activation='softmax')\r\n])\r\n\r\nopt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\r\nmodelo_2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>\r\n\r\n<pre>\r\n<code class=\"language-python\">resultados(modelo_2)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nP&eacute;rdida con conjunto de prueba: 1.6504671573638916\r\nCerteza con conjunto de prueba: 68.15000176429749</pre>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/13/cnn_8.png\" style=\"height:486px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>TERCER MODELO: CON 6 CAPAS COVOLUCIONALES</p>\r\n\r\n<p>Ahora vamos a agregar dos capas convolucionales m&aacute;s para ver si obtenemos un incremente de la certeza:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_3 = Sequential([\r\n    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\r\n    Conv2D(32, (3,3), activation = 'relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Conv2D(128, (3,3), activation='relu', padding='same'),\r\n    Conv2D(128, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10, activation='softmax')\r\n])\r\n\r\nopt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\r\nmodelo_3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>\r\n\r\n<pre>\r\n<code class=\"language-python\">resultados(modelo_3)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nP&eacute;rdida con conjunto de prueba: 1.0908386707305908\r\nCerteza con conjunto de prueba: 71.20000123977661\r\n</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>CUARTO MODELO</p>\r\n\r\n<p>La idea principal de la capa Dropout, con el que se quiere regularizar el desempe&ntilde;o y evitar el sobreajuste (overfiting) es que aleatoriamente se eliminan peque&ntilde;as unidades con sus conexiones de la red durante el entrenamiento.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_4 = Sequential([\r\n    Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32,32,3)),\r\n    Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Dropout(0.2),\r\n    Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'),\r\n    Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Dropout(0.2),\r\n    Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'),\r\n    Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'),\r\n    MaxPooling2D((2,2)),\r\n    Dropout(0.3),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10, activation='softmax')\r\n])\r\n\r\nopt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\r\nmodelo_4.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nP&eacute;rdida con conjunto de prueba: 0.6663143038749695\r\nCerteza con conjunto de prueba: 76.85999870300293</pre>\r\n\r\n<p>Podemos ver que la certeza se ha incrementado a casi un 77% de aciertos, algo mayor con respecto a los anteriores.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>QUINTO MODELO</p>\r\n\r\n<p>La capa BatchNormalization trabaja de la misma forma que una normalizaci&oacute;n normal excepto que lo hace por lotes de datos. La activaci&oacute;n de la capa previa ocurre por cada lote mantiendo la activaci&oacute;n media cerca de 0 y la desviaci&oacute;n estandar cerca de 1; esto reduce el n&uacute;mero de pasos de entrenamiento y de este modo acelera el proceso de entrenamiento. En algunos casos resulta en la no necesidad de una capa Dropout.</p>\r\n\r\n<p>La regularizaci&oacute;n (Regularization) es una t&eacute;cnica usada para reducir el error por sobre-entrenamiento.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">peso_decaimiento=1e-4\r\n\r\nmodelo_5 = Sequential([\r\n    Conv2D(32, (3,3), activation = 'relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(peso_decaimiento), input_shape=(32,32,3)),\r\n    BatchNormalization(),\r\n    Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(peso_decaimiento), padding='same'),\r\n    BatchNormalization(),\r\n    Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(peso_decaimiento), padding='same'),\r\n    BatchNormalization(),\r\n    MaxPooling2D((2,2)),\r\n    Dropout(0.3),\r\n    Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(peso_decaimiento)),\r\n    BatchNormalization(),\r\n    MaxPooling2D((2,2)),\r\n    Dropout(0.3),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10, activation='softmax')\r\n])\r\n\r\nopt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\r\nmodelo_5.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>\r\n\r\n<pre>\r\n<code class=\"language-python\">resultados(modelo_5)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\nP&eacute;rdida con conjunto de prueba: 0.8145386576652527\r\nCerteza con conjunto de prueba: 78.35000157356262</pre>\r\n\r\n<h5><img alt=\"\" src=\"/mediafiles/uploads/2023/02/13/cnn_8_pxEEgnW.png\" style=\"height:486px; width:350px\" /></h5>\r\n\r\n<p>Como vemos, hay una leve mejor&iacute;a con respecto al anterior, pues da un 78,35% de aciertos.</p>\r\n\r\n<p>Vamos a guardar este modelo, y de esa forma lo podremos usar cuando lo necesitemos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_5.save('modelo_5.h5')\r\n\r\n# Para cargar dicho modelo solo escribimos: m=load_model('modelo_5.h5')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Bueno, y hemos aprendido a construir modelos de redes convolucionales para la clasificaci&oacute;n de im&aacute;genes, adem&aacute;s, conocimos un poco sobre la data CIFAR10 y sobre otras cosas m&aacute;s. Espero podamos estar en contacto de nuevo pronto. &iexcl;Hasta la vista!</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5><span style=\"font-family:Arial,Helvetica,sans-serif\">(Versionado de libro: Artificial Neural Networks with TensorFlow 2. ANN Architecture Machine Learning Projects, de P. Sarang, 2021)</span></h5>",
		"imagen_referencial" : "imagenes/post-robot10_5dfBciq.jpg",
		"publicado" : 1,
		"fecha_publicacion" : "2023-02-10 15:19:46.000000",
		"autor_id" : 1,
		"categoria_id" : 2
	},
	{
		"id" : 21,
		"estado" : 1,
		"fecha_creacion" : "2023-02-24",
		"fecha_modificacion" : "2023-05-06",
		"fecha_eliminacion" : "2023-05-06",
		"titulo" : "La transferencia de aprendizaje con Tensorflow",
		"slug" : "la-transferencia-de-aprendizaje-con-tensorflow",
		"descripcion" : "Así como nosotros podemos transmitir nuestras enseñanzas, ya sea en nuestro rol de padres, maestros o simplemente amigos, existen algoritmos de la inteligencia artificial que también pueden transferir lo que han aprendido durante su entrenamiento, para acortar el proceso de aprendizaje y ganar tiempo y precisión en nuestras tareas de machine learning y deep learning. Vamos a aprender cómo...",
		"contenido" : "<p>Cuando buscamos que nuestro modelo de clasificaci&oacute;n de im&aacute;genes alcance una precisi&oacute;n grande, tal vez mayor al 95%, necesitamos disponer de un conjunto de datos tambi&eacute;n grande, a mayor cantidad de datos disponibles para entrenamiento, la precisi&oacute;n se incrementa y este es un principio a tener en cuenta en la construcci&oacute;n de redes neuronales. Existen en nuestros d&iacute;as p&aacute;ginas que cada vez se hacen con m&aacute;s datos (sean estos de la naturaleza que sea) y en materia de im&aacute;genes, por ejemplo, la primera en su tipo en t&eacute;rminos de escala es ImageNet (https://devopedia.org/imagenet), que consiste en m&aacute;s de 14 millones de im&aacute;genes organizadas en m&aacute;s de 20 mil subcategor&iacute;as que est&aacute;n clasificadas en m&aacute;s de 27 subconjuntos. Para clasificar dichas im&aacute;genes muchos modelos de machine learning fueron desarrollados, principalmente en investigaciones&nbsp; y competiciones.</p>\r\n\r\n<p>La cuesti&oacute;n ahora es poder reusar esos recursos de redes neuronales que han aprendido y de esta forma obtener los beneficios que esto trae consigo. La tecnolog&iacute;a que nos permite disfrutar de estas facilidades es llamada transferencia de aprendizaje (transfer learning) y hoy vamos a aprender como realizar dicha tarea.</p>\r\n\r\n<p>Aprenderermos:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; &iquest;Qu&eacute; significa transferencia de conocimientos?</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; &iquest;Qu&eacute; es TensorFlow Hub?</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; C&oacute;mo hacer uso de modelos pre-entrenado</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Transferencia de conocimientos</h2>\r\n\r\n<p>En las &uacute;ltimas d&eacute;cadas los desarrolladores de software han estado usando recursos para reusar su propio c&oacute;digo en sus programas y tambi&eacute;n recursos que otros han creado. En machine learning podemos hacer uso de bibliotecas pero solo con las limitaciones que nos da la codificaci&oacute;n, pues solo podemos disponer de lo que los ingenieros de software nos proveen. Los cuatro ingredientes de una modelo entrenado son:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Algoritmo</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Data</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Entrenamiento</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Pericias</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Primeramente, el algoritmo es desarrollado por alguien para entrenar una red neuronal, esta es la parte del c&oacute;digo para la transferencia de conocimientos. Por lo general un modelo aprende mediante la ayuda de una gran cantidad de datos. Tercero, para el entrenamiento se precisan de grandes requerimientos de energ&iacute;a y tiempo; mientras se entrena una red neuronal se consumen recursos, y esto aumenta a medida que los datos sean mayores. Por &uacute;ltimo, el conocimiento y la pericia de un experto que sepa entrenar los modelos. As&iacute; que, &iquest;c&oacute;mo transferimos estos aspectos a un nuevo modelo? Para facilitar esta tarea los investigadores de TensorFlow desarrollaron TensorFlow Hub.</p>\r\n\r\n<p>TensorFlow Hub es una plataforma incluida en TensorFlow en donde modelos ya creados, entrenados y probados son hechos p&uacute;blico a fin de que cualquier usuario pueda descubrir y reusar partes de los m&oacute;dulos de machine learning de estos modelos. Podemos re-entrenar dichos m&oacute;dulos teniendo en cuenta que debemos usar una tasa de aprendizaje baja, dado que los valores predefinidos podr&iacute;an arrojar resultados inesperados. Adem&aacute;s, cuando creamos nuestros propios modelos, y si creemos que podr&iacute;a ser usado por la comunidad de usuarios, podemos incluirlo en esta plataforma.</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/24/tensorhub.png\" style=\"height:178px; width:350px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Clasificador de perros de acuerdo a su raza</h2>\r\n\r\n<p>Queremos crear un proyecto que tome la imagen de un perro y nos diga a qu&eacute; raza pertenece, pero para ello ahorraremos una ingente cantidad de trabajo y recursos haciendo uso de lo que ya otros han hecho, y de manera m&aacute;s eficiente. La transferencia de aprendizaje es, en pocas palabras, tomar una parte de un modelo ya entrenado y agregarle nuestra capa clasificadora para que nos d&eacute; los resultados que queremos.</p>\r\n\r\n<p>En este proyecto aprenderemos c&oacute;mo construir un modelo en Keras para clasificar im&aacute;genes y usaremos el modelo pre-entrenado con TF2 de TensorFlow Hub MobileNet V2, que toma im&aacute;genes de tama&ntilde;o 224x224. Ser&aacute; un clasificador multiclase; en contraposici&oacute;n al clasificador binario que arroja solo dos clasificaciones, &eacute;ste nos clasifica un perro de acuerdo a una lista de razas.</p>\r\n\r\n<p>Usaremos el dataset de la p&aacute;gina Kaggle (www.kaggle.com/c/dog-breed-identification/overview), que consiste en una colecci&oacute;n de m&aacute;s de 10 mil im&aacute;genes de perros de 120 razas diferentes. Necesitamos procesar las im&aacute;genes y transformarlas en tensores. La data est&aacute; compuesta por un conjunto de entrenamiento, uno de prueba (sin etiquetas) y un archivo con los nombres de las razas. Puedes bajar la data a tu computadora desde la p&aacute;gina, pero yo lo har&eacute; descargando los permisos de Kaggle. Para obtener la data debemos tener una cuenta en Kaggle, as&iacute; que si no la tienes a&uacute;n, ya es tiempo.</p>\r\n\r\n<p>Lo primero es abrir una secci&oacute;n en Google Colab, y como en otras pr&aacute;cticas, vamos a utilizar el acelerador que nos provee esta plataforma, como ya hemos hecho en otra pr&aacute;ctica, para que el trabajo pesado se torne un poco m&aacute;s liviano y r&aacute;pido. Recuerda que antes de activar el acelerador debes cambiar el entorno de ejecuci&oacute;n y seleccionar GPU.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\n# Así haremos uso del acelerador de google GPU\r\ndispositivo_acelerador=tf.config.experimental.list_physical_devices('GPU')\r\nprint('Num GPUs disponible: ', len(dispositivo_acelerador))\r\ntf.config.experimental.set_memory_growth(dispositivo_acelerador[0], True)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Cargando la data</h2>\r\n\r\n<p>Como ya dije antes, el conjunto de datos se encuentra en la p&aacute;gina de Kaggle y podemos descargarla desde all&iacute; (www.kaggle.com/c/dog-breed-identification/data) a nuestra compu, pero hoy vamos a aprender como descargard directamente nuestra data al entorno de Google Colab desde Kaggle (debemos tener nuestra cuenta). Primero instalamos kaggle que contiene los codigos de transferencia y permisos, entre otras cosas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">!pip install kaggle</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora que se instal&oacute;, abrimos nuestra cuenta en Kaggle y hacemos click en nuestro perfil; se desplegar&aacute; un submen&uacute;, all&iacute; seleccionamos <strong>Account</strong>. En la p&aacute;gina que se abre, vamos a la sub-secci&oacute;n de <strong>API</strong>, y hacemos click en <strong>Create New API Token</strong>; esto va a crear un archivo json con el token necesario para permitir la conexi&oacute;n entre Colab y Kaggle que se descarga a nuestra computadora. Ahora escribimos en una celda:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">#Descarga permiso de kaggle\r\nfrom google.colab import files\r\nfiles.upload()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Esto va a iniciar un ciclo de espera con un bot&oacute;n de selecci&oacute;n para subir a Colab nuestro archivo token que acabamos de bajar. Damos click al boton y buscamos en nuestra compu dicho archivo, lo seleccionamos y este archivo subir&aacute; a Colab.</p>\r\n\r\n<p>Ahora, en una nueva celda y ya con nuestro token cargado, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">#Antes de importar la dataset debemos usar este codigo\r\n!mkdir -p ~/.kaggle\r\n!cp kaggle.json ~/.kaggle/\r\n\r\n#Este permiso cambia\r\n!chmod 600 ~/.kaggle/kaggle.json</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Nuestro token, como otro cualquiera, tiene una duraci&oacute;n, por ello cuando necesitamos abrir otra secci&oacute;n o modificar nuestro proyecto otro d&iacute;a, es seguro que ya debemos renovar el token, pero que esto no te intimide, hemos ganado mucho!&nbsp;<img alt=\"yes\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/thumbs_up.png\" style=\"height:23px; width:23px\" title=\"yes\" /></p>\r\n\r\n<p>Ahora podemos bajar nuestra data sin problemas:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Importamos el dataset que queremoos usar un proyecto!\r\n!kaggle competitions download -c dog-breed-identification</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Lo descomprimimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">!unzip dog-breed-identification.zip</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Y presentamos una muestra:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from IPython.display import display, Image\r\nImage('/content/train/83bc62b0fffa99a9c94ba0b67a5f7395.jpg')</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/24/perro_raza_1.png\" style=\"height:305px; width:400px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Nuestra data consta de varias carpetas en donde se almacenan fotos para entrenamiento (train), para pruebas (test) y un archivo en donde se guardan los distintos nombres de las razas (en ingl&eacute;s). Este &uacute;ltimo archivo podemos revisarlo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import pandas as pd\r\netiquetas_csv = pd.read_csv('/content/labels.csv')\r\netiquetas_csv.head()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Aqu&iacute; se nos mostrar&aacute;n las 5 primeras filas del archivo, con su id y la raza a la pertenece la imagen. Si queremos saber un poco m&aacute;s sobre la data, vemos lo que nos arroja su descripci&oacute;n:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">etiquetas_csv.describe()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Podemos ver que hay m&aacute;s de 10 mil im&aacute;genes, la cantidad de razas (120), la primera imagen y la frecuencia en que la raza de este perro est&aacute; en la data. Vamos a ver gr&aacute;ficamente algunos datos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># ¿Cuantas imágenes de cada raza hay en la data?\r\netiquetas_csv['breed'].value_counts().plot.bar(figsize=(20,10))</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como podemos ver hay mas de 60 im&aacute;genes de cada categor&iacute;a (raza). Google recomienda un m&iacute;nimo de diez im&aacute;genes por clases para una clasificaci&oacute;n. Mientras m&aacute;s im&aacute;genes, existe una m&aacute;s alta probabilidad de encontrar patrones entre ellas.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Preprocesando im&aacute;genes y etiquetas</h2>\r\n\r\n<p>Vamos a preparar la data para entrenamiento, y lo haremos de la siguiente manera:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Definimos nuestra ruta de archivos de entrenamiento\r\nruta_entrenamiento = '/content/train'\r\n\r\n# Creamos ruta de nombres de los ID's\r\nn_archivo=[ruta_entrenamiento + '/' + nombreA + '.jpg' for nombreA in etiquetas_csv['id']]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Podemos verificar que funciona correctamente imprimiendo una de las im&aacute;genes:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">Image(n_archivo[9000])</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/24/perro_raza_2.png\" style=\"height:359px; width:400px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora que tenemos las im&aacute;genes casi listas para procesarlas, vamos a convertir las etiquetas en arreglos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import numpy as np\r\netiquetas = etiquetas_csv['breed'].to_numpy()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como ya sabemos, el numero de etiquetas (la raza de cada perro) es de 10.222. Necesitamos extraer los valores &uacute;nicos de dichas etiquetas (las razas):</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">razas = np.unique(etiquetas)\r\nlen(razas)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Esto arroja un valor de 120, que es el n&uacute;mero de razas (clases) que hay en la data. Si quieres imprimir esta lista de razas, escribe:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">list(razas)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Esto imprimir&aacute; una lista con las distintas razas que componen el arreglo. Ahora , debemos codificar las etiquetas (target) con valores entre 0 y 119; esto lo hacemos porque la red no trabaja sino con n&uacute;meros:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Codificamos los nombres de cada una de las razas con valores entre 0 y 120\r\nfrom sklearn.preprocessing import LabelEncoder\r\netiquetas = LabelEncoder().fit_transform(etiquetas).reshape(-1,1)\r\netiquetas</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\narray([[19], [37], [85], ..., [ 3], [75], [28]])</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como vemos, el primer perro es clasificado como #19, el segundo como #37, etc. Ahora usamos el OneHotEncoder para transformar estos valores categoriales en columnas para el entrenamiento del modelo</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Usamos one-hot encoder para convertir los valores categoriales\r\nfrom sklearn.preprocessing import OneHotEncoder\r\n\r\netiquetas_boolean= OneHotEncoder().fit_transform(etiquetas).toarray()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora veamos que hicimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">etiquetas_boolean[2]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Se contamos los valores que hay en el arreglo, ser&aacute;n 120, por cada campo, el valor es 0, excepto para uno que tendra el valor 1 y que ser&aacute; la clase para la imagen dada.</p>\r\n\r\n<p>Ya que tenemos preprocesadas las etiquetas y listas, vamos a trabajar con las im&aacute;genes y ponerlas a punto para alimentar nuestro modelo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Preprocesando las im&aacute;genes</p>\r\n\r\n<p>Hemos convertido las etiquetas en formato num&eacute;rico, ahora debemos transformar las im&aacute;genes en tensores para alimentar el modelo y para ello vamos a crear una funci&oacute;n que:</p>\r\n\r\n<ul>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Tome los archivos de las im&aacute;genes de entrada.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Cargue la imagen (*.jpeg) en formato binario.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Vuelva esa imagen en tensor.</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Redimensione la imagen a 224x224 (recuerda que es la medida que acepta el modelo MobileNet)</span></li>\r\n\t<li><span style=\"font-family:Arial,Helvetica,sans-serif\">*&nbsp; &nbsp; Devuelva el tensor a imagen.</span></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Asignamos las variables:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">X = n_archivo\r\ny = etiquetas_boolean</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora creamos los conjuntos de entrenamiento y de evaluaci&oacute;n del modelo.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">from sklearn.model_selection import train_test_split\r\n\r\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>De las 10.222 im&aacute;genes en la carpeta de entrenamiento, separamos 2.045 para validar el modelo.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Funci&oacute;n que transforma imagen</h2>\r\n\r\n<p>Ahora creamos una funci&oacute;n para preprocesar una imagen desde una ruta dada, para que la transforme en un tensor:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">IMG_SIZE = 224\r\n\r\ndef procesar_imagen(ruta_imagen):\r\n  \"\"\"\r\n  Toma la ruta de la imagen y la vuelve un tensor\r\n  \"\"\"\r\n  # Lee el archivo de la imagen\r\n  imagen=tf.io.read_file(ruta_imagen)\r\n  # Transforma la imagen jepg en un tensor numérico\r\n  imagen=tf.image.decode_jpeg(imagen, channels=3)\r\n  # Convierte el canal de colores en valores de 0-255 a valores entre 0 y 1\r\n  imagen=tf.image.convert_image_dtype(imagen, tf.float32)\r\n  # Redimensiona la imagen al tamaño deseado de 244x244\r\n  imagen=tf.image.resize(imagen, size=[IMG_SIZE, IMG_SIZE])\r\n  return imagen</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Asociando etiquetas e im&aacute;genes</p>\r\n\r\n<p>Crearemos una funci&oacute;n que tome la imagen, transforme la imagen y la ligue a su etiqueta:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Creamos una función que retorna una tupla (imagen, etiqueta)\r\ndef obtener_imagen_etiqueta(ruta_imagen, etiqueta):\r\n  \"\"\"\r\n  Toma una ruta de imagen y la asocia a su etiqueta respectiva, procesa la imagen y devuelve la tupla imagen - etiqueta\r\n  \"\"\"\r\n  imagen = procesar_imagen(ruta_imagen)\r\n  return imagen, etiqueta</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Creando lotes de datos</h2>\r\n\r\n<p>Un lote (en ingl&eacute;s batch) es una peque&ntilde;a porci&oacute;n de la data (las im&aacute;genes y sus etiquetas, en este caso). Normalmente, se ha mostrado su eficacia, el tama&ntilde;o del lote es de 32 (puede variar) y esto significa que la data se divide en lotes de 32 im&aacute;genes con sus respectivas etiquetas. En deep learning, en vez de encontrar patrones en una data completa al mismo tiempo, a menudo se divide en lotes para trabajar cada uno y mejorar el desempe&ntilde;o de los modelos.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">BATCH_SIZE = 32\r\n\r\ndef crear_data_lotes(x, y = None, batch_size=BATCH_SIZE, tipo_dato=1):\r\n  \"\"\"\r\n  Se crean lotes de datos de pares imagen (x) y etiqueta (y).\r\n  Se remueven los datos si son de entrenamiento, pero no se hace si son de validación\r\n  También se aceptan datos de prueba como entradas(no etiquetas)\r\n  \"\"\"\r\n  # Si la data es un conjunto de prueba, no tenemos etiquetas\r\n  if tipo_dato == 3:\r\n    print('Creando lotes de datos de prueba...')\r\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x)))\r\n\r\n    data_lote=data.map(procesar_imagen).batch(BATCH_SIZE)\r\n    return data_lote\r\n\r\n    # Si la data es un conjunto de validación, no necesita removerlo\r\n  elif tipo_dato == 2:\r\n    print('Creando lotes de datos de validación...')\r\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))\r\n    data_lote=data.map(obtener_imagen_etiqueta).batch(BATCH_SIZE)\r\n    return data_lote\r\n\r\n  else:\r\n    # Si la data es de entrenamiento, lo removemos\r\n    print('Creando lotes de datos de entrenamiento...')\r\n    # Devuelve rutas y etiquetas en tensores\r\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))\r\n    # Removiendo las rutas y etiquetas antes de relacionar imágenes\r\n    # Es más rápido que remover imágenes\r\n    data = data.shuffle(buffer_size=len(x))\r\n\r\n    # Creamos tuplas imagen - etiquetas\r\n    data = data.map(obtener_imagen_etiqueta)\r\n    # Pone la data en lotes\r\n    data_lote = data.batch(BATCH_SIZE)\r\n\r\n  return data_lote</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Dependiendo del tipo de dato que le enviamos (1, 2 o 3) la funci&oacute;n va a crear los lotes de entrenamiento, validaci&oacute;n y prueba en los conjuntos de datos. En los datos de entrenamiento se realiza un proceso de &quot;barajar&quot; (shuffle, en ingl&eacute;s) que implica intercambiar aleatoriamente las im&aacute;genes de su lugar en el grupo. La funci&oacute;n&nbsp;<strong>from_tensor_slices</strong> convierte la data en <strong>pipelines</strong> de entrada al modelo, mientras que&nbsp; la <strong>map</strong> convierte la tupla imagen -etiqueta tambi&eacute;n en <strong>pipelines</strong> para alimentar el modelo. La funci&oacute;n <strong>batch</strong> divide la data en lotes.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Creando funci&oacute;n para mostras im&aacute;genes</h2>\r\n\r\n<p>Ahora crearemos una funci&oacute;n para mostrar un mosaico de 25 im&aacute;genes del conjunto de datos y que nos ayudar&aacute; en la prueba.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import matplotlib.pyplot as plt\r\n\r\ndef mostrar_25_imagenes(imagenes, etiquetas):\r\n  plt.figure(figsize=(10,10))\r\n  for i in range(25):\r\n    ax = plt.subplot(5,5, i+1)\r\n    plt.imshow(imagenes[i])\r\n    plt.title(razas[etiquetas[i].argmax()])\r\n    plt.axis('off')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Seleccionando el modelo pre-entrenado</h2>\r\n\r\n<p>Habiendo definido las funciones para el preprocesado de la data, vamos ahora a crear nuestro modelo de aprendizaje.</p>\r\n\r\n<p>Como dijimos antes, usaremos para la transferencia de aprendizaje un modelo tomado de TensorFLow Hub (www.tfhub/dev). Aqu&iacute; existen varios modelos para clasificar diferentes categor&iacute;as. Si seleccionamos en en la categoria de clasificaci&oacute;n de im&aacute;genes, se listar&aacute;n varios modelos. Algunos son espec&iacute;ficamente para versi&oacute;n 1.x. Como estamos usando TF2, seleccionamos un modelo que soporte esta versi&oacute;n; uno de los cuales es mobilenet_v2_130_224 en su versi&oacute;n 5, que es la que usaremos. Si miramos en su documentaci&oacute;n descubrimos que el modelo tiene como forma de entrada (224, 244, 3).</p>\r\n\r\n<p>Para definir nuestro modelo, necesitamos:</p>\r\n\r\n<p>*&nbsp; &nbsp; La forma de datos de entrada (im&aacute;genes en forma de tensores)</p>\r\n\r\n<p>*&nbsp; &nbsp; El n&uacute;mero de clases deseadas (n&uacute;mero de razas de perros)</p>\r\n\r\n<p>*&nbsp; &nbsp; La URL del modelo pre-entrenado que queremos usar</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Entonces, declaramos las variables necesarias:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ENTRADA = [None, IMG_SIZE, IMG_SIZE, 3] # Lote, altura, ancho y canales de colores\r\n\r\nSALIDA = len(razas) # El número de las razas\r\n\r\nURL_MODELO = 'https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5'\r\n\r\ndef crear_modelo(input_shape = ENTRADA, output_shape=SALIDA, model_url=URL_MODELO):\r\n  print('Creando modelo con: ', URL_MODELO)\r\n\r\n  # Las capas del modelo\r\n  modelo = tf.keras.Sequential([\r\n      hub.KerasLayer(URL_MODELO), # Capa de TensorFlow Hub\r\n      tf.keras.layers.Dense(units=SALIDA, activation='softmax') # Capa de salida\r\n  ])\r\n\r\n  # Compilando el modelo\r\n  modelo.compile(\r\n      loss=tf.keras.losses.CategoricalCrossentropy(),\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      metrics=['accuracy']\r\n  )\r\n\r\n  # Creando modelo\r\n  modelo.build(ENTRADA)\r\n  return modelo</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Usamos como funci&oacute;n de p&eacute;rdida <strong>CategoricalCrossentropy&nbsp;</strong>y la <strong>Adam</strong> como funci&oacute;n de optimizaci&oacute;n. Tomamos como m&eacute;trica&nbsp;<strong>accuracy</strong>&nbsp;(certeza) para evaluar el desempe&ntilde;o del modelo.</p>\r\n\r\n<p>Podemos probar esta funci&oacute;n creando un modelo e imprimiendo su resumen:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo = crear_modelo()\r\nmodelo.summary()</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Creando los conjuntos de datos</h2>\r\n\r\n<p>Ahora que tenemos nuestro modelo listo, vamos a preprocesar los datos de entrenamiento y validaci&oacute;n usando nuestra funci&oacute;n&nbsp;<strong>crear_data_lotes</strong>.</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">datos_entrenamiento = crear_data_lotes(X_train, y_train)\r\ndatos_validacion = crear_data_lotes(X_val, y_val, tipo_dato=2)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si queremos chequear algunas im&aacute;genes de los datos de entrenamiento, usemos la funci&oacute;n que hemos creado:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Creamos un iterador\r\nimg_entrenamiento, eti_entrenamiento = next(datos_entrenamiento.as_numpy_iterator())\r\n\r\n# Pasamos los pares creados\r\nmostrar_25_imagenes(img_entrenamiento, eti_entrenamiento)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/25/perro_raza_3.png\" style=\"height:373px; width:400px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si no se muestran las mismas im&aacute;genes, no te preocupes, recuerda que las removimos aleatoriamente en el dataset.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Usando TensorBoard</h2>\r\n\r\n<p>Ahora vamos a cargar TensorBoard y limpiamos el registro por si hay datos previos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">%load_ext tensorboard\r\n!rm -rf ./logs/</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Crearemos una funci&oacute;n de llamada. En esta funci&oacute;n creamos un directorio de registro para almacenar los registros de TensorBoard con sus datos cronol&oacute;gicos.:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">import datetime\r\nimport os\r\n\r\ndef crear_llamada_tensorboard():\r\n  logdir = os.path.join('logs',\r\n                        datetime.datetime.now().strftime('%Y%m/%d-%H%M%S'))\r\n  return tf.keras.callbacks.TensorBoard(logdir)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Usaremos la funci&oacute;n&nbsp;<strong>EarlyStopping</strong>&nbsp;de Keras, para monitorear la certeza en la validaci&oacute;n de datos. Tambi&eacute;n crearemos dos variables para registrar y monitorear. Debemos notar que el entrenamiento de nuestro modelo se detendr&aacute; cuando vea que los valores de certeza no mejoran despu&eacute;s del tercer ciclo repetido (patience=3):</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo_tensorboard = crear_llamada_tensorboard()\r\n\r\nmodelo_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy',\r\n                                                         patience=3) # Se detiene después de 3 ciclos sin mejoras</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Entrenando el modelo</h2>\r\n\r\n<p>Primero construimos el modelo llamando la funci&oacute;n que hemos definido antes. Definimos una variable para el n&uacute;mero de epochs durante el entrenamiento, &eacute;sta es un n&uacute;mero grande pero para eso tenemos la funci&oacute;n definida antes, que detendr&aacute; el entrenamiento cuando no haya una mejor&iacute;a en los resultados obtenidos; veremos que dicho entrenamiento se detiene mucho antes de llegar a la epoch N&deg; 100 :</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo = crear_modelo()\r\n\r\nNUM_EPOCHS = 100\r\n\r\nmodelo.fit(x = datos_entrenamiento,\r\n           epochs = NUM_EPOCHS,\r\n           validation_data= datos_validacion,\r\n           callbacks=[modelo_tensorboard,\r\n                      modelo_early_stopping])</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Examinando los registros</h2>\r\n\r\n<p>Luego del entrenamiento, que tomar&aacute; algunos minutos, abrimos TensorBoard en nuestro cuaderno de Colab:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">%tensorboard --logdir logs</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Se despliega una pantalla como esta, que grafica la evoluci&oacute;n de la certeza y la funci&oacute;n de p&eacute;rdida durante el entrenamiento del modelo:</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/27/epoch_certeza.png\" style=\"height:240px; width:300px\" /></p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/27/epoch_loss.png\" style=\"height:265px; width:300px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Evaluando el desempe&ntilde;o</h2>\r\n\r\n<p>Vamos a ver c&oacute;mo fue el desempe&ntilde;o de nuestro modelo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">modelo.evaluate(datos_validacion)</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\n64/64 [==============================] - 69s 1s/step - loss: 0.7940 - accuracy: 0.8127\r\n</pre>\r\n\r\n<pre>\r\n[0.794028103351593, 0.8127139210700989]</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Como podemos ver, la certeza conseguida es de un 81%.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Haciendo predicciones</h2>\r\n\r\n<p>Ahora podemos hacer predicciones y comprobar si nuestro modelo se desempe&ntilde;a de manera satisfactoria. En la data que bajamos de Kaggel hay un directorio con im&aacute;genes para hacer pruebas (test). Vamos a indicar a nuestro proyecto que tome dichas im&aacute;genes y las prepare para probar el modelo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">ruta_prueba = '/content/test'\r\narch_prueba = [ruta_prueba + '/' + nombreA for nombreA in os.listdir(ruta_prueba)]\r\ndata_prueba = crear_data_lotes(arch_prueba, tipo_dato=3)\r\n</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para hacer predicciones solo tenemos que llamar al m&eacute;todo&nbsp;<strong>predict</strong>&nbsp;en el modelo entrenado:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Haciendo predicciones\r\ntest_predicciones = modelo.predict(data_prueba, verbose=1)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Con esta l&iacute;nea de c&oacute;digo obtenemos un arreglo de predicciones. Podemos chequear el tama&ntilde;o de este arreglo imprimiendo sus dimensiones:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">predicciones_test.shape</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Hemos obtenido una salida de (10357, 120) que indica que 10.357 im&aacute;genes fueron analizadas. Cada &iacute;tem en un arreglo es otro arreglo de 120 &iacute;tems. Cada &iacute;tem indica la probabilidad del tipo de perro con su respectivo valor &iacute;ndice. Por ejemplo, si queremos imprimir el arreglo para las predicciones de la primera imagen, escribimos en una celda:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">test_predicciones[0]</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/27/perro_raza_6.png\" style=\"height:348px; width:400px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Este arreglo contiene 120 valores, cada una de las probabilidades de raza para la imagen.&nbsp;Podemos hallar la m&aacute;xima probabilidad con el valor del &iacute;ndice, usando la funci&oacute;n&nbsp;<strong>argmax</strong>&nbsp;de numpy. Por ejemplo, para esta primera imgen tenemos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\"># Valor de probabilidad máxima predicha por el modelo\r\nprint(f'Valor máximo: {np.max(predicciones_test[0])}')\r\n\r\n# El índice donde el valor máximo de las predicciones de la primera imagen ocurre\r\nprint(f'Indice máximo: {np.argmax(predicciones_test[0])}')\r\n\r\n# La etiqueta predicha\r\nprint(f'Raza del perro en la predicción: {razas[np.argmax(predicciones_test[0])]}</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<pre>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">Valor maximo: 0.9377518892288208</span>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">Indice max: 44</span>\r\n<span style=\"font-family:Arial,Helvetica,sans-serif\">Raza predicha: flat-coated_retriever</span></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Estos resultados no tienen que ser necesariamente iguales al que obtienes.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>Visualizando resultados.</h2>\r\n\r\n<p>Ahora, vamos a imprimir las im&aacute;genes de los perros, la raza predicha y un gr&aacute;fico de barra con los 10 valores m&aacute;ximos en la predicci&oacute;n.&nbsp;La primera funci&oacute;n muestra la imagen del perro con su clase predicha. La segunda muestra un gr&aacute;fico de barras con los primeros 10 valores del arreglo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">def plot_pred(prob_prediccion, imagenes):\r\n  imagen = procesar_imagen(imagenes)\r\n  pred_etiqueta = razas[np.argmax(prob_prediccion)]\r\n  plt.imshow(imagen)\r\n  plt.axis('off')\r\n  plt.title(pred_etiqueta)\r\n\r\n\r\ndef plot_pred_conf(prob_prediccion):\r\n  max_10_ind_pred = prob_prediccion.argsort()[-10:][::-1]\r\n  max_10_val_pred = prob_prediccion[max_10_ind_pred]\r\n  max_10_eti_pred = razas[max_10_ind_pred]\r\n  img_top = plt.bar(np.arange(len(max_10_eti_pred)), max_10_val_pred, color='grey')\r\n  plt.xticks(np.arange(len(max_10_eti_pred)), labels=max_10_eti_pred, rotation='vertical')\r\n  img_top[0].set_color('green')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Ahora vamos a imprimir las 3 primeras im&aacute;genes (s&oacute;lo voy a poner aqu&iacute; las dos primeras, pero si todo est&aacute; correcto, a ti se te presentar&aacute;n 3):</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">num_filas = 3\r\n\r\nplt.figure(figsize=(5*2, 5*num_filas))\r\n\r\nfor i in range(num_filas):\r\n  plt.subplot(num_filas, 2, 2*i+1)\r\n  plot_pred(prob_prediccion=test_predicciones[i],\r\n            imagenes=arch_prueba[i])\r\n  plt.subplot(num_filas, 2, 2*i+2)\r\n  plot_pred_conf(prob_prediccion=test_predicciones[i])\r\n\r\nplt.tight_layout(h_pad=1.0)\r\nplt.show()</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p><img alt=\"\" src=\"/mediafiles/uploads/2023/02/27/perro_raza_5.png\" style=\"height:256px; width:300px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>El siguiente paso es hacer una predicci&oacute;n con una foto no ya del conjunto de im&aacute;genes que tenemos, sino que vamos a buscar una cualquiera de Google, por ejemplo. Primero instalamos la librer&iacute;a para obtener archvios, buscamos la url de la imagen que queremos y alimentamos el modelo con dicha imagen:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">!pip install wget\r\n\r\nurl = 'https://t2.ea.ltmcdn.com/es/posts/4/9/9/curiosidades_del_tigre_25994_orig.jpg'\r\nimport wget\r\nwget.download(url,'curiosidades_del_tigre_25994_orig.jpg')</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Preprocesamos la imagen y la pasamos al modelo:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">data = crear_data_lotes(['/content/curiosidades_del_tigre_25994_orig.jpg'], batch_size=1, tipo_dato=3)\r\n\r\nresultado = modelo.predict(data)</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Si queremos ver el arreglo de los datos de la imagen que acabamos de pasar, escribimos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">resultado[0]</code></pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Para&nbsp; chequear el valor m&aacute;ximo de las predicciones, lo seleccionamos y si es menor que un l&iacute;mite de certeza, lo descartamos:</p>\r\n\r\n<pre>\r\n<code class=\"language-python\">if np.max(resultado[0]) &gt; 0.7:\r\n  print(nombre_clase_predicha)\r\nelse:\r\n  print('Hay un {}% de probabilidades de que no sea un perro.'.format(100-(round(np.max(resultado[0]*100)))))</code></pre>\r\n\r\n<p>Salida:</p>\r\n\r\n<p>Hay un 69% de probabilidades de que no sea un perro.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Y bien, eso es todo por hoy. Creo que podemos felicitarnos por lo que hemos aprendido, pero tambi&eacute;n, no debemos descuidar nuestro inter&eacute;s, y por ello vamos a seguir aprendiendo porque hay muchas m&aacute;s cosas, interesantes y fascinantes en este mundo de la inteligencia artificial. Espero nos veamos en otra oportunidad.</p>\r\n\r\n<p><img alt=\"laugh\" src=\"http://127.0.0.1:8000/staticfiles/ckeditor/ckeditor/plugins/smiley/images/teeth_smile.png\" style=\"height:23px; width:23px\" title=\"laugh\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h5>(Versionado y corregido del libro: Artificial Neural Network with TensorFlow 2.x, de P. Sarang, 2021)</h5>",
		"imagen_referencial" : "imagenes/perro_raza.png",
		"publicado" : 1,
		"fecha_publicacion" : "2023-02-24 02:20:10.000000",
		"autor_id" : 1,
		"categoria_id" : 2
	}
]
